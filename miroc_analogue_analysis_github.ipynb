{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Notes**\n",
    "### Purpose: Analogue analysis between two periods for MIROC6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Preparation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # for storing vector and matrix data\n",
    "import matplotlib.pyplot as plt # to plot figures\n",
    "import netCDF4 as nc #to read netCDF files\n",
    "import cartopy.crs as ccrs # to plot maps\n",
    "# (ergens in test ook: import cartopy as cart)\n",
    "import cartopy.feature as cf\n",
    "# from matplotlib import ticker\n",
    "import scipy.io\n",
    "from scipy.stats import pearsonr # voor persistence\n",
    "import scipy.stats as stats\n",
    "# from cartopy.util import add_cyclic_point\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "from matplotlib.colors import CenteredNorm\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path,*variables_to_add):\n",
    "  \"\"\"\n",
    "Provide the path to a file and the variables you want to extract\n",
    "  \"\"\"\n",
    "  data = nc.Dataset(path, mode='r')\n",
    "  variable_list = []\n",
    "  for variable in variables_to_add:\n",
    "    var =data.variables[variable][:]\n",
    "    variable_list.append(var)\n",
    "  return variable_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_area(S, N, W, E, lat, lon, variable,event = False):\n",
    "    \"\"\"\n",
    "    This function slices the data given the S, N, W, E bounds. Use event = True if there are only two dimensions (since then there is no time dimension), this means after using this\n",
    "    function you need to use event[0] to get the data\n",
    "    \"\"\"\n",
    "    # Change longitude data to go from -180 to 180\n",
    "    for i in range(len(lon)):\n",
    "        if lon[i] > 180:\n",
    "          lon[i] = lon[i] - 360\n",
    "        else:\n",
    "          lon[i] = lon[i]\n",
    "\n",
    "    # Calculate the index of the bounds\n",
    "    sIndex = np.argmin(np.abs(lat - S))\n",
    "    nIndex = np.argmin(np.abs(lat - N))\n",
    "    wIndex = np.argmin(np.abs(lon - W))\n",
    "    eIndex = np.argmin(np.abs(lon - E))\n",
    "\n",
    "    if event:\n",
    "        variable = np.expand_dims(variable, axis = 0)\n",
    "\n",
    "    if wIndex > eIndex: # If the west index is higher than the east index, think of the right side of the world map as left boundary and vice versa\n",
    "        latSlice = lat[sIndex: nIndex + 1]\n",
    "        lonSlice = np.concatenate((lon[wIndex:], lon[:eIndex + 1]))\n",
    "        variableSlice = np.concatenate((variable[:, sIndex: nIndex + 1, wIndex:], variable[:, sIndex: nIndex + 1, :eIndex + 1]), axis = 2)\n",
    "\n",
    "    else:\n",
    "        latSlice = lat[sIndex: nIndex + 1]\n",
    "        lonSlice = lon[wIndex: eIndex + 1]\n",
    "        variableSlice = variable[:, sIndex: nIndex + 1, wIndex: eIndex + 1]\n",
    "\n",
    "    return latSlice, lonSlice, variableSlice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_variablet(lat, lon, variable,folder,name):\n",
    "    plt.figure(figsize = (10,10))\n",
    "    ax = plt.axes(projection = ccrs.PlateCarree())\n",
    "    plot = plt.contourf(lon, lat, variable, cmap = \"RdBu_r\", transform = ccrs.PlateCarree(), levels = 15) #levels=np.linspace(-8.2e7, 1e7, 10), extend='both\n",
    "    ax.coastlines()\n",
    "    ax.add_feature(cf.BORDERS)\n",
    "    plt.colorbar(plot, ax=ax, orientation = \"horizontal\", label = \"Pressure (Pa)\", pad = 0.05)\n",
    "    #plt.savefig(f\"{folder}/{name}.png\",dpi=300)\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lists_for_era5_dates(final_year,final_month,final_day):\n",
    "    \"\"\"\n",
    "    Creates lists with all months and years in the complete ERA5 data, taking into account leap days (schrikkeldagen)\n",
    "    \"\"\"\n",
    "    start_date_all_era5_data = datetime(1950, 1, 1) # Is included\n",
    "    end_date_all_era5_data = datetime(final_year,final_month,final_day) # Is included\n",
    "    delta_time = timedelta(days=1)\n",
    "\n",
    "    date_list_basic = []\n",
    "    current_date = start_date_all_era5_data\n",
    "    while current_date <= end_date_all_era5_data:\n",
    "        date_list_basic.append(current_date)\n",
    "        current_date += delta_time\n",
    "    date_strings = [date.strftime('%Y-%m-%d') for date in date_list_basic]\n",
    "    month_list = [date.month for date in date_list_basic]\n",
    "    year_list = [date.year for date in date_list_basic]\n",
    "\n",
    "    return month_list, year_list, date_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_years_and_months_era5(era5_data,desired_start_month,desired_end_month,desired_start_year,desired_end_year,list_with_all_months,list_with_all_years):\n",
    "    \"\"\"\n",
    "    Slice era5 data based on the months and years, months and years that are used as input variable will be included as well\n",
    "    \"\"\"\n",
    "    if era5_data.shape[0] != len(list_with_all_months):\n",
    "        print (\"Error: Amount of days in the data and list with all dates are not the same\")\n",
    "    \n",
    "    list_for_filtered_era5_data = []\n",
    "    list_for_filtered_years = []\n",
    "    list_for_filtered_months = []\n",
    "    for i in range(era5_data.shape[0]):\n",
    "        month_at_index = list_with_all_months[i]\n",
    "        year_at_index = list_with_all_years[i]\n",
    "        if month_at_index >= desired_start_month and month_at_index <= desired_end_month and year_at_index >= desired_start_year and year_at_index <= desired_end_year:\n",
    "            data_to_select = era5_data[i,:,:]\n",
    "            list_for_filtered_era5_data.append(data_to_select)\n",
    "            list_for_filtered_years.append(year_at_index)\n",
    "            list_for_filtered_months.append(month_at_index)\n",
    "    array_selected_era5_data = np.array(list_for_filtered_era5_data)\n",
    "\n",
    "    return array_selected_era5_data, list_for_filtered_years, list_for_filtered_months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidian_distance(data, event):\n",
    "  \"\"\"\n",
    "Calculates the euclidian distance for each day in the data compared to a given single event, gives an array (~list) of all the distances in chronological order\n",
    "  \"\"\"\n",
    "  return np.sqrt(np.sum((data - event)**2, axis = (1, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_analogues_era5(euclidian_distances, target_number_of_analogues,analogue_seperation_range,list_months,list_years,desired_start_month,desired_end_month,event_is_in_data = True):\n",
    "  \"\"\"\n",
    "  Determine the best analogues. Give an array of all unsorted euclidan distances. The analogue_seperation_range determines how many days have to seperate\n",
    "  the different analogues (if analogue_seperation_range = 5, the fifth day after an analogue can also still not be a new analogue(dus die dag mag ook niet)).\n",
    "  Target_number_of_analogues is how many analogues are selected. Returns the indexes of the best analogues in the original data, and the euclidian distances corresponding to the analogues.\n",
    "  Give semi-filtered data to euclidian_distances: filtered on years but all months, so the seperation range doesn't continue to count in the next season\n",
    "  Should give semi-filtered lists of months and years as well??\n",
    "  \"\"\"\n",
    "  distance_index_dictionary = {value: index for index, value in enumerate(euclidian_distances)} # Gives the index in the original euclidian distances list for a value in the sorted list\n",
    "  sorted_distances = np.sort(euclidian_distances) # sort the distances from low to high euclidian distances\n",
    "  analogues_index_list = [] # create a list to save the indexes of the selected analogues\n",
    "  euclidian_distance_list = [] # create a list to save the euclidian distances of the selected analogues\n",
    "  selected_analogue_years = []\n",
    "  if event_is_in_data == False:\n",
    "    i = 0\n",
    "    if sorted_distances[0] == 0:\n",
    "      print (\"WARNING: event does seem to be in data, while event_is_in_data == False (zelf)\")\n",
    "    while len(analogues_index_list) < target_number_of_analogues and (i < len(euclidian_distances)):\n",
    "      differences= []\n",
    "      index = distance_index_dictionary[sorted_distances[i]]\n",
    "      month = list_months[index]\n",
    "      year = list_years[index]\n",
    "      if len(analogues_index_list) == 0:\n",
    "        if month >= desired_start_month and month <= desired_end_month:\n",
    "          analogues_index_list.append(index)\n",
    "          euclidian_distance_list.append(sorted_distances[i])\n",
    "          selected_analogue_years.append(year)\n",
    "        i = i + 1\n",
    "      else:\n",
    "        if month >= desired_start_month and month <= desired_end_month:\n",
    "          for item in analogues_index_list:\n",
    "            difference = (index-item)\n",
    "            if difference < (-1*analogue_seperation_range) or difference > analogue_seperation_range:\n",
    "              differences.append(2) #goed\n",
    "            elif difference >= (-1*analogue_seperation_range) and difference <= (analogue_seperation_range):\n",
    "              differences.append(1) #niet goed\n",
    "          if min(differences) == 2:\n",
    "            analogues_index_list.append(index)\n",
    "            euclidian_distance_list.append(sorted_distances[i])\n",
    "            selected_analogue_years.append(year)\n",
    "            i = i + 1\n",
    "          elif min(differences) == 1:\n",
    "            i = i + 1\n",
    "        else:\n",
    "          i = i + 1\n",
    "\n",
    "  elif event_is_in_data == True: #Need to make sure the selected analogues are also 5 days away from the event, eventhough it is not a selecgted analogue \n",
    "    event_index = distance_index_dictionary[sorted_distances[0]]\n",
    "    analogues_index_list.append(event_index)\n",
    "    i=1 #index counter\n",
    "    if sorted_distances[0] != 0:\n",
    "      print (\"WARNING: event does not seem to be in data, while event_is_in_data == True (zelf)\")\n",
    "    while len(analogues_index_list) < (target_number_of_analogues+1) and (i < len(euclidian_distances)): #+1 omdat event er nu ook in en die later weghalen\n",
    "      differences= []\n",
    "      index = distance_index_dictionary[sorted_distances[i]]\n",
    "      month = list_months[index]\n",
    "      year = list_years[index]\n",
    "      if len(analogues_index_list) == 0:\n",
    "        if month >= desired_start_month and month <= desired_end_month:\n",
    "          analogues_index_list.append(index)\n",
    "          euclidian_distance_list.append(sorted_distances[i])\n",
    "          selected_analogue_years.append(year)\n",
    "        i = i + 1\n",
    "      else:\n",
    "        if month >= desired_start_month and month <= desired_end_month:\n",
    "          for item in analogues_index_list:\n",
    "            difference = (index-item)\n",
    "            if difference < (-1*analogue_seperation_range) or difference > analogue_seperation_range:\n",
    "              differences.append(2) #goed\n",
    "            elif difference >= (-1*analogue_seperation_range) and difference <= (analogue_seperation_range):\n",
    "              differences.append(1) #niet goed\n",
    "          if min(differences) == 2:\n",
    "            analogues_index_list.append(index)\n",
    "            euclidian_distance_list.append(sorted_distances[i])\n",
    "            selected_analogue_years.append(year)\n",
    "            i = i + 1\n",
    "          elif min(differences) == 1:\n",
    "            i = i + 1\n",
    "        else:\n",
    "          i = i + 1\n",
    "    analogues_index_list.pop(0)\n",
    "\n",
    "  if len(analogues_index_list) < target_number_of_analogues:\n",
    "    print (\"WARNING: not enough data to find the required amount of analogues (zelf)\")\n",
    "  if len(analogues_index_list) != target_number_of_analogues:\n",
    "    print (\"WARNING: not the right amount of analogues has been found\")\n",
    "  return analogues_index_list,euclidian_distance_list,selected_analogue_years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_analogues_era5_old(euclidian_distances, target_number_of_analogues,analogue_seperation_range,list_months,list_years,desired_start_month,desired_end_month,event_is_in_data = True):\n",
    "  \"\"\"\n",
    "  Determine the best analogues. Give an array of all unsorted euclidan distances. The analogue_seperation_range determines how many days have to seperate\n",
    "  the different analogues (if analogue_seperation_range = 5, the fifth day after an analogue can also still not be a new analogue(dus die dag mag ook niet)).\n",
    "  Target_number_of_analogues is how many analogues are selected. Returns the indexes of the best analogues in the original data, and the euclidian distances corresponding to the analogues.\n",
    "  Give semi-filtered data to euclidian_distances: filtered on years but all months, so the seperation range doesn't continue to count in the next season\n",
    "  Should give semi-filtered lists of months and years as well??\n",
    "  \"\"\"\n",
    "  distance_index_dictionary = {value: index for index, value in enumerate(euclidian_distances)} # Gives the index in the original euclidian distances list for a value in the sorted list\n",
    "  sorted_distances = np.sort(euclidian_distances) # sort the distances from low to high euclidian distances\n",
    "  analogues_index_list = [] # create a list to save the indexes of the selected analogues\n",
    "  euclidian_distance_list = [] # create a list to save the euclidian distances of the selected analogues\n",
    "  selected_analogue_years = []\n",
    "  if event_is_in_data == True:\n",
    "    i = 1\n",
    "  elif event_is_in_data == False:\n",
    "    i=0 #index counter\n",
    "  while len(analogues_index_list) < target_number_of_analogues and (i < len(euclidian_distances)):\n",
    "    differences= []\n",
    "    index = distance_index_dictionary[sorted_distances[i]]\n",
    "    month = list_months[index]\n",
    "    year = list_years[index]\n",
    "    if len(analogues_index_list) == 0:\n",
    "      if month >= desired_start_month and month <= desired_end_month:\n",
    "        analogues_index_list.append(index)\n",
    "        euclidian_distance_list.append(sorted_distances[i])\n",
    "        selected_analogue_years.append(year)\n",
    "      i = i + 1\n",
    "    else:\n",
    "      if month >= desired_start_month and month <= desired_end_month:\n",
    "        for item in analogues_index_list:\n",
    "          difference = (index-item)\n",
    "          if difference < (-1*analogue_seperation_range) or difference > analogue_seperation_range:\n",
    "            differences.append(2) #goed\n",
    "          elif difference >= (-1*analogue_seperation_range) and difference <= (analogue_seperation_range):\n",
    "            differences.append(1) #niet goed\n",
    "        if min(differences) == 2:\n",
    "          analogues_index_list.append(index)\n",
    "          euclidian_distance_list.append(sorted_distances[i])\n",
    "          selected_analogue_years.append(year)\n",
    "          i = i + 1\n",
    "        elif min(differences) == 1:\n",
    "          i = i + 1\n",
    "      else:\n",
    "        i = i + 1\n",
    "  if len(analogues_index_list) < target_number_of_analogues:\n",
    "    print (\"WARNING: not enough data to find the required amount of analogues (zelf)\")\n",
    "  return analogues_index_list,euclidian_distance_list,selected_analogue_years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def persistence(analogue_indexes,data,c_threshold):\n",
    "  \"\"\"\n",
    "Defines the persistence of each analogue based on a correlation threshold (>=). It returns a list with a list for each analogue, -1 is a day before the event, 1 is a day after the event.\n",
    "So if you want the total lenght of the event you still need to add the event itself? (nu wel in lenghts gedaan :) Also returns the correlations in the correct order.\n",
    "(als je dus de analogues bepaald heb met data gesliced op seizoen die ook hier gebruiken?)\n",
    "  \"\"\"\n",
    "  list_for_lists = [] #\n",
    "  list_for_lists_values = []\n",
    "  # nu 500 ook laatste dag dus kan niet vooruit\n",
    "  for index in analogue_indexes:\n",
    "    list_for_length = []\n",
    "    list_for_values = []\n",
    "    analogue_map = data[index]\n",
    "    Ab,Ac = np.shape(analogue_map)\n",
    "    analogue_af = analogue_map.reshape(Ab*Ac)\n",
    "\n",
    "    index_dag1 = index + 1\n",
    "    index_dag2 = index - 1\n",
    "    correlation = 1\n",
    "\n",
    "    while correlation >= c_threshold and index_dag2 >= 0:\n",
    "      day_data = data[index_dag2]\n",
    "      Bb,Bc = np.shape(day_data)\n",
    "      day_map_af = day_data.reshape(Bb*Bc)\n",
    "      correlation = pearsonr(analogue_af, day_map_af)[0]\n",
    "      if correlation >= c_threshold:\n",
    "        list_for_length.insert(0,-1)\n",
    "        list_for_values.insert(0,correlation) # dit is met insert dat is anders, dan doet ie het aan de voorkant dus dan staan de values echt op volgorde van de dag.\n",
    "      index_dag2 = index_dag2 - 1\n",
    "\n",
    "    correlation = 1\n",
    "\n",
    "    while correlation >= c_threshold and index_dag1 <= (len(data)-1):\n",
    "      day_data = data[index_dag1]\n",
    "      Bb,Bc = np.shape(day_data)\n",
    "      day_map_af = day_data.reshape(Bb*Bc)\n",
    "      correlation = pearsonr(analogue_af, day_map_af)[0]\n",
    "      if correlation >= c_threshold:\n",
    "        list_for_length.append(1)\n",
    "        list_for_values.append(correlation)\n",
    "      index_dag1 = index_dag1 + 1\n",
    "\n",
    "\n",
    "    list_for_lists.append(list_for_length) # nu dus niet anaologue zelf meegenomen in lengte dus voor lengte nog lengte lijst +1\n",
    "    list_for_lists_values.append(list_for_values)\n",
    "\n",
    "  list_for_lengths = []\n",
    "  for lijst in list_for_lists:\n",
    "    lengte = len(lijst) + 1\n",
    "    list_for_lengths.append(lengte)\n",
    "\n",
    "  return list_for_lists, list_for_lists_values,list_for_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_and_mean_analogue_array(analogue_indexes_p1,data_p1,analogue_indexes_p2,data_p2):\n",
    "  \"\"\"\n",
    "    Gives the array with the data for the analogue and the mean of all the analogues and the difference between the two periods\n",
    "    the analogue array is sorted op analogue quality (not on time)\n",
    "  \"\"\"\n",
    "  list_for_p1 = []\n",
    "  list_for_p2 = []\n",
    "\n",
    "  for index in analogue_indexes_p1:\n",
    "    data_analogue = data_p1[index]\n",
    "    list_for_p1.append(data_analogue)\n",
    "\n",
    "  for index in analogue_indexes_p2:\n",
    "    data_analogue = data_p2[index]\n",
    "    list_for_p2.append(data_analogue)\n",
    "\n",
    "  array_p1 = np.array(list_for_p1) #maak van de list weer een array\n",
    "  mean_array_p1 = np.mean(array_p1, axis = 0)\n",
    "\n",
    "  array_p2 = np.array(list_for_p2) #maak van de list weer een array\n",
    "  mean_array_p2 = np.mean(array_p2, axis = 0)\n",
    "\n",
    "  difference_between_periods = mean_array_p2 - mean_array_p1\n",
    "  return difference_between_periods, mean_array_p1, mean_array_p2, array_p1, array_p2, list_for_p1, list_for_p2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def typicality(analogue_distances, analogue_indexes,data,target_number_of_analogues,analogue_seperation_range,list_months,list_years,desired_start_month,desired_end_month,is_event_in_data):\n",
    "  \"\"\"\n",
    "Calculates the Tevent en Tanalogue. Distances need to be from the selected analogues only. Data has to be the one used to calculate the original analogues.\n",
    "  \"\"\"\n",
    "  Tevent = 1/(sum(analogue_distances))\n",
    "  list_for_Tanalogue = []\n",
    "  for i in range(len(analogue_indexes)):\n",
    "    index_in_used_data = analogue_indexes[i]\n",
    "    euclidian_distance_for_analogue = euclidian_distance(data,data[index_in_used_data])\n",
    "    index_for_analogues_for_analogue,distance_for_analogues_for_analogue, years_of_analogues_for_analogues = determine_analogues_era5(euclidian_distance_for_analogue, target_number_of_analogues,analogue_seperation_range,list_months,list_years,desired_start_month,desired_end_month,event_is_in_data = True )\n",
    "    Tanalogue = 1/(sum(distance_for_analogues_for_analogue))\n",
    "    list_for_Tanalogue.append(Tanalogue)\n",
    "  return Tevent, list_for_Tanalogue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def violin_plot(Tevent_past,Tevent_present,Tanalogue_past,Tanalogue_present,persistence_past,persistence_present,persistence_event,folder,name):\n",
    "  \"\"\"\n",
    "Doet het? maar stipjes van persistence standaard op 1 want hebt nog niet van event?\n",
    "  \"\"\"\n",
    "  u, p = stats.ttest_ind(Tanalogue_past, Tanalogue_present) #calculate p value\n",
    "  text_to_plot = f\"p = {p:.3f}\"\n",
    "\n",
    "  fig, (ax1,ax2) = plt.subplots(1,2,figsize = (8, 5))\n",
    "  violins = ax1.violinplot([Tanalogue_past, Tanalogue_present], [1, 1.6], showmeans=False, showextrema=False, showmedians=False)\n",
    "  colors = [\"magenta\", \"green\"]\n",
    "  for pc, color in zip(violins[\"bodies\"], colors): #pc zijn de violins dus moet zo als verschillende kleuren wil\n",
    "    pc.set_facecolor(color)\n",
    "  ax1.axhline(np.mean(Tanalogue_past), color = colors[0], linewidth = 3)\n",
    "  ax1.axhline(np.mean(Tanalogue_present), color = colors[1], linewidth = 3)\n",
    "  ax1.plot(1,Tevent_past, marker = \"o\", color = \"r\") #plot de events\n",
    "  ax1.plot(1.6,Tevent_present, marker = \"o\", color = \"r\")  #plot de events\n",
    "  ax1.set_xticks([1, 1.6])\n",
    "  ax1.set_xticklabels([\"Past\", \"Present\"])\n",
    "  ax1.set_ylim(top = (1.035*(max(max(Tanalogue_past),max(Tanalogue_present))))) # to ensure that the plotted p-value and graph do not overlap\n",
    "  ax1.text(0.035,0.95,text_to_plot,transform=ax1.transAxes)\n",
    "  ax1.set_ylabel(\"Typicality (x$10^{-10}$)\")\n",
    "  ax1.set_title(\"(A)\",loc= \"center\",fontsize = 11)\n",
    "  u, p = stats.ttest_ind(persistence_past, persistence_present) #calculate p value\n",
    "  text_to_plot = f\"p = {p:.3f}\"\n",
    "\n",
    "\n",
    "  violins = ax2.violinplot([persistence_past, persistence_present], [1, 1.6], showmeans=False, showextrema=False, showmedians=False)\n",
    "  colors = [\"magenta\", \"green\"]\n",
    "  for pc, color in zip(violins[\"bodies\"], colors): #pc zijn de violins dus moet zo als verschillende kleuren wil\n",
    "    pc.set_facecolor(color)\n",
    "  ax2.axhline(np.mean(persistence_past), color = colors[0], linewidth = 3)\n",
    "  ax2.axhline(np.mean(persistence_present), color = colors[1], linewidth = 3)\n",
    "  ax2.plot(1,persistence_event, marker = \"o\", color = \"r\") #plot de events\n",
    "  ax2.plot(1.6,persistence_event, marker = \"o\", color = \"r\")  #plot de events\n",
    "  ax2.set_xticks([1, 1.6])\n",
    "  ax2.set_xticklabels([\"Past\", \"Present\"])\n",
    "  ax2.set_ylim(top = (1.07*(max(max(persistence_past),max(persistence_present))))) # to ensure that the plotted p-value and graph do not overlap\n",
    "  ax2.text(0.035,0.95,text_to_plot,transform=ax2.transAxes)\n",
    "  ax2.set_ylabel(\"Persistence (days)\")\n",
    "  ax2.set_title(\"(B)\",loc= \"center\",fontsize = 11)\n",
    "\n",
    "  plt.subplots_adjust(wspace=0.25)  # Change the value as needed\n",
    "  #plt.savefig(\"/usr/people/noest/stage_folders/outputs/figures_net/violin2023.png\",dpi=600)\n",
    "  plt.show()\n",
    "  plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def t_test(data1850,data1950,p_value):\n",
    "  \"\"\"\n",
    "Performs a two sides t-test, for a analogue difference map data1850 and data1950 should be lists with arrays for each analogue (so not already the mean!), p_value should be for example 0.05\n",
    "  \"\"\"\n",
    "  number_of_analogues = len(data1850)\n",
    "  significance_mask = data1850[0].copy()\n",
    "  a, b = np.shape(data1850[0])\n",
    "  for i in range(a): # for a x\n",
    "    #print(i)\n",
    "    for j in range(b): # check every y on that x\n",
    "      lijst1850 = []\n",
    "      lijst1950 = []\n",
    "      for R in range(number_of_analogues): # for each combination of x and y (so each cell) check each analogue\n",
    "        lijst1850.append(data1850[R][i,j])\n",
    "        lijst1950.append(data1950[R][i,j])\n",
    "      #u, p = stats.mannwhitneyu(loc_list1,loc_list2)\n",
    "      u, p = stats.ttest_ind(lijst1850, lijst1950)\n",
    "      if p < p_value:\n",
    "        significance_mask[i,j] = 1\n",
    "      else:\n",
    "        significance_mask[i,j] = 0\n",
    "  return significance_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def t_test_trend(data1850,data1950):\n",
    "  \"\"\"\n",
    "Performs a two sides t-test, for a analogue difference map data1850 and data1950 should be lists with arrays for each analogue (so not already the mean!), p_value should be for example 0.05\n",
    "  \"\"\"\n",
    "  u, p = stats.ttest_ind(data1850, data1950)\n",
    "  print (p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_differencet(lat, lon, variable,significance_mask,folder,name):\n",
    "    plt.figure(figsize = (10,10))\n",
    "    ax = plt.axes(projection = ccrs.PlateCarree())\n",
    "    plot = plt.contourf(lon, lat, variable, cmap = \"RdBu_r\", transform = ccrs.PlateCarree(), levels = 15) #levels=np.linspace(-8.2e7, 1e7, 10), extend='both\n",
    "    ax.contourf( lon, lat,significance_mask, levels=[-2,0,2], hatches=[None, '////'], colors='none', transform=ccrs.PlateCarree())\n",
    "    ax.coastlines()\n",
    "    ax.add_feature(cf.BORDERS)\n",
    "    plt.colorbar(plot, ax=ax, orientation = \"horizontal\", label = \"Pressure (Pa)\", pad = 0.05)\n",
    "    #plt.savefig(f\"{folder}/{name}.png\",dpi=300)\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_differencet_centeredt(lat, lon, variable,significance_mask,folder,name):\n",
    "    plt.figure(figsize = (10,10))\n",
    "    ax = plt.axes(projection = ccrs.PlateCarree())\n",
    "    plot = plt.contourf(lon, lat, variable, cmap = \"RdBu_r\", transform = ccrs.PlateCarree(), levels = 15, norm = CenteredNorm()) #levels=np.linspace(-8.2e7, 1e7, 10), extend='both\n",
    "    ax.contourf( lon, lat,significance_mask, levels=[-2,0,2], hatches=[None, '////'], colors='none', transform=ccrs.PlateCarree())\n",
    "    ax.coastlines()\n",
    "    ax.add_feature(cf.BORDERS)\n",
    "    plt.colorbar(plot, ax=ax, orientation = \"horizontal\", label = \"Streamfunction (mÂ²/s)\", pad = 0.05)\n",
    "    #plt.savefig(f\"{folder}/{name}.png\",dpi=300)\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yearly_minimum_euclidian_distance(euclidian_distances, yearlength = 360):\n",
    "  \"\"\"\n",
    "Finds the best analogue (lowest ecleudian distance) for each year to do trend analysis. Returns an array with the distances.\n",
    "  \"\"\"\n",
    "  distances_per_year = euclidian_distances.reshape((euclidian_distances.shape[0] // yearlength, yearlength))\n",
    "  return np.min(distances_per_year, axis = 1) #loopt blijkbaar al automatisch door alle jaren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yearly_minimum_euclidian_distance2(euclidian_distances,yearlength,year_of_event,start_year, event_is_in_data):\n",
    "  \"\"\"\n",
    "Finds the best analogue (lowest ecleudian distance) for each year to do trend analysis. Returns an array with the distances.\n",
    "  \"\"\"\n",
    "  distances_per_year = euclidian_distances.reshape((euclidian_distances.shape[0] // yearlength, yearlength))\n",
    "  if event_is_in_data == False:\n",
    "    return np.min(distances_per_year, axis = 1) #loopt blijkbaar al automatisch door alle jaren\n",
    "  elif event_is_in_data == True:\n",
    "    list_for_minimums = []\n",
    "    for i in range(distances_per_year.shape[0]):\n",
    "      yearly_data = distances_per_year[i,:]\n",
    "      year = start_year + i\n",
    "      if year == year_of_event:\n",
    "        sorted_data = np.sort(yearly_data)\n",
    "        min_distance = sorted_data[1]\n",
    "        list_for_minimums.append(min_distance)\n",
    "      else:\n",
    "        min_distance = np.min(yearly_data)\n",
    "        list_for_minimums.append(min_distance)\n",
    "    array_min_distances = np.array(list_for_minimums)\n",
    "    return array_min_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_trend(yearly_min_distance,distances, start_year, end_year, folder, name,running_mean_window = 1):\n",
    "  \"\"\"\n",
    "Moet wel goed checken dat length x_ticks and y_data hetzelfde is (dus 100 years, is 1850 tot en met 1949! dus ook end_year = 1949)\n",
    "  \"\"\"\n",
    "  plt.figure(figsize = (10,8))\n",
    "  max_distance = max(distances)\n",
    "  y_data = 1 - (yearly_min_distance/max_distance) # loopt ook somehow al door alle distances\n",
    "  running_mean = np.convolve(y_data, np.ones(running_mean_window) / running_mean_window, mode = \"valid\") #valid is dat rand niet doet (kan wel met 'same' maar dan voegt die nullen toe dus dan average fout,). Kernel (2e input) is weight voor verschillende plekken in window (hier overal gelijk?)\n",
    "  x_ticks = range(start_year, end_year + 1)\n",
    "  plt.ylabel(\"1 - (ED/EDmax)\")\n",
    "  plt.xlabel(\"Years\")\n",
    "\n",
    "  plt.plot(x_ticks,y_data)\n",
    "  if running_mean_window != 1:\n",
    "    plt.plot(range(start_year + (running_mean_window // 2 - 1), end_year - (running_mean_window // 2 - 1)),running_mean) #begin van bram, omdat eerste paar jaar niet window kan doen\n",
    "  #plt.savefig(f\"{folder}/{name}.png\",dpi=300)\n",
    "  plt.show()\n",
    "  plt.close()\n",
    "\n",
    "  return y_data, running_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_trend2(yearly_min_distance,distances, start_year, end_year,max_euclidian_distance, folder, name,running_mean_window = 1):\n",
    "  \"\"\"\n",
    "Moet wel goed checken dat length x_ticks and y_data hetzelfde is (dus 100 years, is 1850 tot en met 1949! dus ook end_year = 1949)\n",
    "  \"\"\"\n",
    "  plt.figure(figsize = (10,8))\n",
    "  max_distance = max_euclidian_distance\n",
    "  y_data = 1 - (yearly_min_distance/max_distance) # loopt ook somehow al door alle distances\n",
    "  running_mean = np.convolve(y_data, np.ones(running_mean_window) / running_mean_window, mode = \"valid\") #valid is dat rand niet doet (kan wel met 'same' maar dan voegt die nullen toe dus dan average fout,). Kernel (2e input) is weight voor verschillende plekken in window (hier overal gelijk?)\n",
    "  x_ticks = range(start_year, end_year + 1)\n",
    "  plt.ylabel(\"1 - (ED/EDmax)\")\n",
    "  plt.xlabel(\"Years\")\n",
    "\n",
    "  plt.plot(x_ticks,y_data)\n",
    "  if running_mean_window != 1:\n",
    "    plt.plot(range(start_year + (running_mean_window // 2 - 1), end_year - (running_mean_window // 2 - 1)),running_mean) #begin van bram, omdat eerste paar jaar niet window kan doen\n",
    "  #plt.savefig(f\"{folder}/{name}.png\",dpi=300)\n",
    "  plt.show()\n",
    "  plt.close()\n",
    "\n",
    "  return y_data, running_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_trend_combined_subplots(y_data_p1,y_data_p2,running_mean_data_p1,running_mean_data_p2, start_year1850,start_year1950, end_year1850,end_year1950, folder, name,running_mean_window = 1):\n",
    "  \"\"\"\n",
    "  Moet wel goed checken dat length x_ticks and y_data hetzelfde is (dus 100 years, is 1850 tot en met 1949! dus ook end_year = 1949)\n",
    "  \"\"\"\n",
    "  fig, (ax1,ax2) = plt.subplots(nrows = 1, ncols = 2,figsize = (10,4), sharey = True)\n",
    "  x_ticks1 = range(start_year1850, end_year1850 + 1)\n",
    "  x_ticks2 = range(start_year1950, end_year1950 + 1)\n",
    "  ax1.set_ylabel(\"1 - (ED/EDmax)\")\n",
    "  ax1.set_xlabel(\"Years\")\n",
    "  ax2.set_xlabel(\"Years\")\n",
    "  \n",
    "  ax1.plot(x_ticks1,y_data_p1, c = \"darkturquoise\") #linewidth = 2\n",
    "  if running_mean_window != 1:\n",
    "    ax1.plot(range(start_year1850 + (running_mean_window // 2 - 1), end_year1850 - (running_mean_window // 2 - 1)),running_mean_data_p1, c = \"black\") #begin van bram, omdat eerste paar jaar niet window kan doen\n",
    "  ax2.plot(x_ticks2,y_data_p2, c = \"darkturquoise\") #linewidth = 2\n",
    "  if running_mean_window != 1:\n",
    "    ax2.plot(range(start_year1950 + (running_mean_window // 2 - 1), end_year1950 - (running_mean_window // 2 - 1)),running_mean_data_p2, c = \"black\") #begin van bram, omdat eerste paar jaar niet window kan doen\n",
    "  plt.subplots_adjust(wspace=0.1) # Adjust the horizontal space between the two plot (in percentage compared to the average axis-length?)\n",
    "  #plt.savefig(f\"{folder}/{name}.png\",dpi=300)\n",
    "  plt.show()\n",
    "  plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_trend_combined_subplots2(y_data_p1,y_data_p2,running_mean_data_p1,running_mean_data_p2, start_year1850,start_year1950, end_year1850,end_year1950, folder, name,running_mean_window = 1):\n",
    "  \"\"\"\n",
    "  Moet wel goed checken dat length x_ticks and y_data hetzelfde is (dus 100 years, is 1850 tot en met 1949! dus ook end_year = 1949)\n",
    "  \"\"\"\n",
    "  fig, (ax1,ax2) = plt.subplots(nrows = 1, ncols = 2,figsize = (10,4), sharey = True)\n",
    "  x_ticks1 = range(start_year1850, end_year1850 + 1)\n",
    "  x_ticks2 = range(start_year1950, end_year1950 + 1)\n",
    "  ax1.set_ylabel(\"Similarity\")\n",
    "  ax1.set_xlabel(\"Years\")\n",
    "  ax2.set_xlabel(\"Years\")\n",
    "  \n",
    "  ax1.plot(x_ticks1,y_data_p1, c = \"darkturquoise\") #linewidth = 2\n",
    "  mean1 = np.mean(y_data_p1)\n",
    "  ax1.axhline(y = mean1, c = \"grey\", linestyle = \"--\") #begin van bram, omdat eerste paar jaar niet window kan doen\n",
    "  ax2.plot(x_ticks2,y_data_p2, c = \"darkturquoise\") #linewidth = 2\n",
    "  mean2 = np.mean(y_data_p2)\n",
    "  ax2.axhline(y = mean2, c = \"grey\", linestyle = \"--\")\n",
    "  plt.subplots_adjust(wspace=0.1) # Adjust the horizontal space between the two plot (in percentage compared to the average axis-length?)\n",
    "  #plt.savefig(\"/usr/people/noest/stage_folders/outputs/figures_net/trend2023_p_0.09508_label.png\",dpi=600)\n",
    "  plt.show()\n",
    "  plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_streamfunction(data):\n",
    "    \"\"\"\n",
    "    doen nadat bijgesneden\n",
    "    \"\"\"\n",
    "    list_for_new_data = []\n",
    "    for i in range(data.shape[0]):\n",
    "        data_day = data[i,:,:]\n",
    "        day_mean = np.mean(data_day)\n",
    "        new_data = data_day - day_mean\n",
    "        list_for_new_data.append(new_data)\n",
    "    new_array = np.array(list_for_new_data)\n",
    "    \n",
    "    return new_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_max_for_trend(distances_p1,distances_p2):\n",
    "    max1 = np.max(distances_p1)\n",
    "    max2 = np.max(distances_p2)\n",
    "    if max1 > max2:\n",
    "        return max1\n",
    "    elif max2 > max1:\n",
    "        return max2\n",
    "    else:\n",
    "        print (\"WARNING, something went wrong in determining the max distance for the trend plot (zelf)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def violin_plot2014(Tevent_past,Tevent_present,Tanalogue_past,Tanalogue_present,persistence_past,persistence_present,persistence_event,e):\n",
    "  \"\"\"\n",
    "Doet het? maar stipjes van persistence standaard op 1 want hebt nog niet van event?\n",
    "  \"\"\"\n",
    "  u, p = stats.ttest_ind(Tanalogue_past, Tanalogue_present) #calculate p value\n",
    "  text_to_plot = f\"p = {p:.3f}\"\n",
    "\n",
    "  fig, (ax1,ax2) = plt.subplots(1,2,figsize = (7, 4))\n",
    "  violins = ax1.violinplot([Tanalogue_past, Tanalogue_present], [1, 1.6], showmeans=False, showextrema=False, showmedians=False)\n",
    "  colors = [\"magenta\", \"green\"]\n",
    "  for pc, color in zip(violins[\"bodies\"], colors): #pc zijn de violins dus moet zo als verschillende kleuren wil\n",
    "    pc.set_facecolor(color)\n",
    "  ax1.axhline(np.mean(Tanalogue_past), color = colors[0], linewidth = 3)\n",
    "  ax1.axhline(np.mean(Tanalogue_present), color = colors[1], linewidth = 3)\n",
    "  ax1.plot(1,Tevent_past, marker = \"o\", color = \"r\") #plot de events\n",
    "  ax1.plot(1.6,Tevent_present, marker = \"o\", color = \"r\")  #plot de events\n",
    "  ax1.set_xticks([1, 1.6])\n",
    "  ax1.set_xticklabels([\"Past\", \"Present\"])\n",
    "  ax1.set_ylim(bottom=1.5e-10,top = 3.0e-10) # to ensure that the plotted p-value and graph do not overlap\n",
    "  ax1.text(0.035,0.95,text_to_plot,transform=ax1.transAxes)\n",
    "  ax1.set_ylabel(\"Typicality (x$10^{-10}$)\")\n",
    "  ax1.set_title(\"(A)\",loc= \"center\",fontsize = 11)\n",
    "  u, p = stats.ttest_ind(persistence_past, persistence_present) #calculate p value\n",
    "  text_to_plot = f\"p = {p:.3f}\"\n",
    "\n",
    "\n",
    "  violins = ax2.violinplot([persistence_past, persistence_present], [1, 1.6], showmeans=False, showextrema=False, showmedians=False)\n",
    "  colors = [\"magenta\", \"green\"]\n",
    "  for pc, color in zip(violins[\"bodies\"], colors): #pc zijn de violins dus moet zo als verschillende kleuren wil\n",
    "    pc.set_facecolor(color)\n",
    "  ax2.axhline(np.mean(persistence_past), color = colors[0], linewidth = 3)\n",
    "  ax2.axhline(np.mean(persistence_present), color = colors[1], linewidth = 3)\n",
    "  ax2.plot(1,persistence_event, marker = \"o\", color = \"r\") #plot de events\n",
    "  ax2.plot(1.6,persistence_event, marker = \"o\", color = \"r\")  #plot de events\n",
    "  ax2.set_xticks([1, 1.6])\n",
    "  ax2.set_xticklabels([\"Past\", \"Present\"])\n",
    "  ax2.set_ylim(top = 11) # to ensure that the plotted p-value and graph do not overlap\n",
    "  ax2.text(0.035,0.95,text_to_plot,transform=ax2.transAxes)\n",
    "  ax2.set_ylabel(\"Persistence (days)\")\n",
    "  ax2.set_title(\"(B)\",loc= \"center\",fontsize = 11)\n",
    "\n",
    "  fig.suptitle(f\"MIROC6 (r{e})\")\n",
    "  plt.subplots_adjust(wspace=0.25)  # Change the value as needed\n",
    "  #plt.savefig(f\"/usr/people/noest/stage_folders/outputs/net/serious_run2/violin/violin_mir{e}.png\",dpi=300)\n",
    "  plt.show()\n",
    "  plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Set-up**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select: \"regridded\" or \"original\", Als original data doet hebt kans dat kleiner gebied moet doen want doet EU data en dat data geupdate is dus dat lijst langer moet zijn met dates\n",
    "data_to_use = \"regridded\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the path where the data is located\n",
    "if data_to_use == \"regridded\":\n",
    "    #data_path_msl = \"/net/pc200246/nobackup/users/noest/ERA5_regridded/era5_psi500_daily_regridded.nc\"\n",
    "    data_path_msl = \"/net/pc200265/nobackup/users/pinto/phi500/MIROC6/phi500_day_MIROC6_historical_r1i1p1f1_gn_19510101-20141231_regrid.nc\"\n",
    "    data_path_msl = \"/net/pc200265/nobackup/users/pinto/phi500/MIROC6/phi500_day_MIROC6_historical_r6i1p1f1_gn_19510101-20141231_regrid.nc\"\n",
    "    data_path_msl = \"/net/pc200265/nobackup/users/pinto/phi500/MIROC6/phi500_day_MIROC6_historical_r9i1p1f1_gn_19510101-20141231_regrid.nc\"\n",
    "    data_path_pressure = \"/net/pc200246/nobackup/users/noest/ERA5_regridded/era5_msl_daily_regridded.nc\"\n",
    "    ensemblenr = 9\n",
    "elif data_to_use == \"original\":\n",
    "    data_path_msl = \"/net/pc200265/nobackup/users/pinto/phi500/era5_phi500.fixed.1degE.nc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the path where the event is located (path to actual file), for the Vautard event: its downloaded using the global data so also use the global data to make sure the resolution is the same\n",
    "if data_to_use == \"regridded\":\n",
    "    event_path = \"/usr/people/noest/stage_folders/event_data/Vautard_southerlyflow_2019-06-29_regridded_streamfunction_data_at_index_25381.npy\"\n",
    "elif data_to_use == \"original\":\n",
    "    event_path = '/usr/people/noest/stage_folders/event_data/Vautard_southerlyflow_2019-06-29_original_streamfunction_data_at_index_25381.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the area to be analysed (for original is lat omgedraaid, iig extent eu data (gebruikt niet hier): lat (29.875-75.125) en lon(-30.125-40.125))\n",
    "bbox = [30,60,-30,20] #Pick borders like: [S,N,W,E]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The latest date in the ERA5 dataset, for the regridded data its 2024, 2, 29\n",
    "final_yearV = 2024\n",
    "final_monthV = 2\n",
    "final_dayV = 29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the months and years that will be analysed for period 1 (V1) and period 2 (V2)\n",
    "desired_start_monthV1 = 3 # 6 voor JJA en 3 voor MAM\n",
    "desired_end_monthV1 = 5 # 8 voor JJA en 5 voor MAM\n",
    "desired_start_yearV1 = 1950 # 1950 for all data\n",
    "desired_end_yearV1 = 1979 # 2014 for model comparison, 2023 is the last complete year\n",
    "desired_start_monthV2 = 3 # 6 voor JJA en 3 voor MAM\n",
    "desired_end_monthV2 = 5 # 8 voor JJA en 5 voor MAM\n",
    "desired_start_yearV2 = 1985 # 1950 for all data\n",
    "desired_end_yearV2 = 2014 # 2014 for model comparison, 2023 is the last complete year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determine how to look for analogues\n",
    "target_number_of_analoguesV = 30 # How many analogues to find\n",
    "analogue_seperation_rangeV = 6 # How many days should the analogues be seperated by\n",
    "event_is_in_dataV1 = False # Is the event in the filtered data? (For example, if event is in 2019, and period 1 is 1950-1979, than = False)(doet hierbij wel semi-filtered data? dus denk ook als kijkt naar spring en event in summer toch true? nee want voegt alleen analogue toe als ook goede maand?)\n",
    "event_is_in_dataV2 = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determine how to calculate the persistence (the minimum correlation coefficient a day needs in order to be taken into account as the same event)\n",
    "correlation_thresholdV = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the year in which the event takes place to calculate the persistence of the event\n",
    "event_year = 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the p value for the significance in the difference plots\n",
    "p_value_difference_mapsV = 0.05 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the filtered yearlenght and running mean window for the trend plot\n",
    "filterd_yearlengthV = 92 # 92 voor JJA en MAM\n",
    "running_mean_windowV = 10 # If this = 1, no running mean window is plotted\n",
    "event_is_in_data_trendV1 = False # Is the event in the filtered data? (For example, if event is in 2019, and period 1 is 1950-1979, than = False)(Bij deze wel op seizoen letten! dus als kijkt naar spring en event in summer = False want gebruikt season data for euclidian distance)\n",
    "event_is_in_data_trendV2 = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determine the amount of analogue plots to save for each period\n",
    "amount_of_analogues_to_plot = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determine which arrays to save"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Uitvoeren**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load data\n",
    "lat_global,lon_global,msl_levels = load_data(data_path_msl,\"lat\",\"lon\",\"stream\")\n",
    "msl_global = np.squeeze(msl_levels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load event\n",
    "event_msl_global = np.load(event_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determine the boundaries to use\n",
    "if data_to_use == \"regridded\":\n",
    "    S1 = bbox[0] # for serious_run1 = 30 en 70 als data_to_use = \"original\"\n",
    "    N1 = bbox[1] # for serious_run1 = 70 en 30 als data_to_use = \"original\"\n",
    "    W1 = bbox[2] # for serious_run1 = -30\n",
    "    E1 = bbox[3] # for serious_run1 = 30\n",
    "elif data_to_use == \"original\":\n",
    "    S1 = bbox[1] # for serious_run1 = 30 en 70 als data_to_use = \"original\"\n",
    "    N1 = bbox[0] # for serious_run1 = 70 en 30 als data_to_use = \"original\"\n",
    "    W1 = bbox[2] # for serious_run1 = -30\n",
    "    E1 = bbox[3] # for serious_run1 = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract area\n",
    "lat_box, lon_box, msl_box_og = extract_area(S1,N1,W1,E1,lat_global,lon_global,msl_global, event = False)\n",
    "msl_box = prep_streamfunction(msl_box_og)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract areaa\n",
    "lat_box2, lon_box2, event_box_og = extract_area(S1,N1,W1,E1,lat_global,lon_global,event_msl_global, event = True)\n",
    "event_box = prep_streamfunction(event_box_og)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create lists with the months, years and complete dates of the era5 data\n",
    "list_with_months, list_with_years, list_with_dates = lists_for_era5_dates(final_yearV,final_monthV,final_dayV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract the periods and the seasons, semi_filtered is all months so the index for the analogue_seperation_range doesn't continue between years\n",
    "msl_box_semi_filtered_p1,filtered_years_list_p1,filtered_months_list_p1 = extract_years_and_months_era5(msl_box,1,12,desired_start_yearV1,desired_end_yearV1,list_with_months, list_with_years)\n",
    "msl_box_semi_filtered_p2,filtered_years_list_p2,filtered_months_list_p2 = extract_years_and_months_era5(msl_box,1,12,desired_start_yearV2,desired_end_yearV2,list_with_months, list_with_years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate euclidian distances\n",
    "euclidian_distance_p1 = euclidian_distance(msl_box_semi_filtered_p1,event_box)\n",
    "euclidian_distance_p2 = euclidian_distance(msl_box_semi_filtered_p2,event_box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find analogues\n",
    "analogues_index_list_p1,euclidian_distance_list_p1,selected_analogue_years_p1 = determine_analogues_era5(euclidian_distance_p1, target_number_of_analoguesV,analogue_seperation_rangeV,filtered_months_list_p1,filtered_years_list_p1,desired_start_monthV1,desired_end_monthV1,event_is_in_data = event_is_in_dataV1)\n",
    "analogues_index_list_p2,euclidian_distance_list_p2,selected_analogue_years_p2 = determine_analogues_era5(euclidian_distance_p2, target_number_of_analoguesV,analogue_seperation_rangeV,filtered_months_list_p2,filtered_years_list_p2,desired_start_monthV2,desired_end_monthV2,event_is_in_data = event_is_in_dataV2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (min(euclidian_distance_list_p1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find persistence (use the same data that was used as for the euclidian distances)\n",
    "persistence_days_p1, persistence_correlations_p1, lengths_p1 = persistence(analogues_index_list_p1,msl_box_semi_filtered_p1,correlation_thresholdV)\n",
    "persistence_days_p2, persistence_correlations_p2, lengths_p2 = persistence(analogues_index_list_p2,msl_box_semi_filtered_p2,correlation_thresholdV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the difference between periods, the mean analogue arrays and the arrays with all analogues\n",
    "pressure_difference_between_periods, mean_analogue_array_p1, mean_analogue_array_p2, analogues_array_p1, analogues_array_p2, analogues_list_p1, analogues_list_p2 = complete_and_mean_analogue_array(analogues_index_list_p1,msl_box_semi_filtered_p1,analogues_index_list_p2,msl_box_semi_filtered_p2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the analogue maps\n",
    "# for i in range(amount_of_analogues_to_plot):\n",
    "#     analogue_to_plot_p1 = analogues_array_p1[i,:,:]\n",
    "#     analogue_to_plot_p2 = analogues_array_p2[i,:,:]\n",
    "#     plot_variablet(lat_box,lon_box,analogue_to_plot_p1,\"test\",\"test\")\n",
    "#     plot_variablet(lat_box,lon_box,analogue_to_plot_p2,\"test\",\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate typicalities\n",
    "Tevent_p1, Tanalogue_p1 = typicality(euclidian_distance_list_p1,analogues_index_list_p1,msl_box_semi_filtered_p1,target_number_of_analoguesV,analogue_seperation_rangeV,filtered_months_list_p1,filtered_years_list_p1,desired_start_monthV1,desired_end_monthV1, event_is_in_dataV1)\n",
    "Tevent_p2, Tanalogue_p2 = typicality(euclidian_distance_list_p2,analogues_index_list_p2,msl_box_semi_filtered_p2,target_number_of_analoguesV,analogue_seperation_rangeV,filtered_months_list_p2,filtered_years_list_p2,desired_start_monthV2,desired_end_monthV2, event_is_in_dataV2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the persistence of the event\n",
    "# year_event_min = event_year - 1\n",
    "# year_event_max = event_year + 1\n",
    "# msl_box_semi_filtered_event,filtered_years_list_event,filtered_months_list_event = extract_years_and_months_era5(msl_box,1,12,year_event_min,year_event_max,list_with_months, list_with_years)\n",
    "# euclidian_distance_event = euclidian_distance(msl_box_semi_filtered_event,event_box)\n",
    "# analogues_index_list_event,euclidian_distance_list_event,selected_analogue_years_event = determine_analogues_era5(euclidian_distance_event, 1,analogue_seperation_rangeV,filtered_months_list_event,filtered_years_list_event,1,12,event_is_in_data = False)\n",
    "# persistence_days_event, persistence_correlations_event, lengths_event = persistence(analogues_index_list_event,msl_box_semi_filtered_event,correlation_thresholdV)\n",
    "lengths_event  = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "violin_plot(Tevent_p1,Tevent_p2,Tanalogue_p1,Tanalogue_p2,lengths_p1,lengths_p2,lengths_event,\"test\",\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "violin_plot2014(Tevent_p1,Tevent_p2,Tanalogue_p1,Tanalogue_p2,lengths_p1,lengths_p2,lengths_event,ensemblenr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if save_output == True:\n",
    "#     filenametxx = f\"{save_path}/{season_name}_model_meanTxxlist.pkl\"\n",
    "#     with open(filenametxx, 'wb') as f:\n",
    "#         pickle.dump(list_for_TXx_area_average, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ks_2samp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract the actual season for the trend plot and create trendplot (trend 2)\n",
    "msl_box_season_filtered_p1,season_filtered_years_list_p1,season_filtered_months_list_p1 = extract_years_and_months_era5(msl_box,desired_start_monthV1,desired_end_monthV1,desired_start_yearV1,desired_end_yearV1,list_with_months, list_with_years)\n",
    "msl_box_season_filtered_p2,season_filtered_years_list_p2,season_filtered_months_list_p2 = extract_years_and_months_era5(msl_box,desired_start_monthV2,desired_end_monthV2,desired_start_yearV2,desired_end_yearV2,list_with_months, list_with_years)\n",
    "euclidian_distance_p1_season = euclidian_distance(msl_box_season_filtered_p1,event_box)\n",
    "euclidian_distance_p2_season = euclidian_distance(msl_box_season_filtered_p2,event_box)\n",
    "max_ed = find_max_for_trend(euclidian_distance_p1_season,euclidian_distance_p2_season)\n",
    "min_euclidian_distances_p1 = yearly_minimum_euclidian_distance2(euclidian_distance_p1_season,filterd_yearlengthV,event_year,desired_start_yearV1,event_is_in_data=event_is_in_data_trendV1)\n",
    "min_euclidian_distances_p2 = yearly_minimum_euclidian_distance2(euclidian_distance_p2_season,filterd_yearlengthV,event_year,desired_start_yearV2,event_is_in_data=event_is_in_data_trendV2)\n",
    "trend_y_data_p1,trend_running_mean_data_p1 = plot_trend2(min_euclidian_distances_p1,euclidian_distance_p1_season,desired_start_yearV1,desired_end_yearV1,max_ed,\"test\",\"test\",running_mean_window = running_mean_windowV)\n",
    "trend_y_data_p2,trend_running_mean_data_p2 = plot_trend2(min_euclidian_distances_p2,euclidian_distance_p2_season,desired_start_yearV2,desired_end_yearV2,max_ed,\"test\",\"test\",running_mean_window = running_mean_windowV)\n",
    "plot_trend_combined_subplots(trend_y_data_p1,trend_y_data_p2,trend_running_mean_data_p1,trend_running_mean_data_p2,desired_start_yearV1,desired_start_yearV2,desired_end_yearV1,desired_end_yearV2,\"test\",\"test\",running_mean_window = running_mean_windowV)\n",
    "plot_trend_combined_subplots2(trend_y_data_p1,trend_y_data_p2,trend_running_mean_data_p1,trend_running_mean_data_p2,desired_start_yearV1,desired_start_yearV2,desired_end_yearV1,desired_end_yearV2,\"test\",\"test\",running_mean_window = running_mean_windowV)\n",
    "t_test_trend(trend_y_data_p1,trend_y_data_p2)\n",
    "_, p_value_ks = ks_2samp(trend_y_data_p1, trend_y_data_p2)\n",
    "print (p_value_ks)\n",
    "_, p2 = scipy.stats.kstest(trend_y_data_p1, trend_y_data_p2)\n",
    "print (p2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract the actual season for the trend plot and create trendplot\n",
    "msl_box_season_filtered_p1,season_filtered_years_list_p1,season_filtered_months_list_p1 = extract_years_and_months_era5(msl_box,desired_start_monthV1,desired_end_monthV1,desired_start_yearV1,desired_end_yearV1,list_with_months, list_with_years)\n",
    "msl_box_season_filtered_p2,season_filtered_years_list_p2,season_filtered_months_list_p2 = extract_years_and_months_era5(msl_box,desired_start_monthV2,desired_end_monthV2,desired_start_yearV2,desired_end_yearV2,list_with_months, list_with_years)\n",
    "euclidian_distance_p1_season = euclidian_distance(msl_box_season_filtered_p1,event_box)\n",
    "euclidian_distance_p2_season = euclidian_distance(msl_box_season_filtered_p2,event_box)\n",
    "min_euclidian_distances_p1 = yearly_minimum_euclidian_distance2(euclidian_distance_p1_season,filterd_yearlengthV,event_year,desired_start_yearV1,event_is_in_data=event_is_in_data_trendV1)\n",
    "min_euclidian_distances_p2 = yearly_minimum_euclidian_distance2(euclidian_distance_p2_season,filterd_yearlengthV,event_year,desired_start_yearV2,event_is_in_data=event_is_in_data_trendV2)\n",
    "trend_y_data_p1,trend_running_mean_data_p1 = plot_trend(min_euclidian_distances_p1,euclidian_distance_p1_season,desired_start_yearV1,desired_end_yearV1,\"test\",\"test\",running_mean_window = running_mean_windowV)\n",
    "trend_y_data_p2,trend_running_mean_data_p2 = plot_trend(min_euclidian_distances_p2,euclidian_distance_p2_season,desired_start_yearV2,desired_end_yearV2,\"test\",\"test\",running_mean_window = running_mean_windowV)\n",
    "plot_trend_combined_subplots(trend_y_data_p1,trend_y_data_p2,trend_running_mean_data_p1,trend_running_mean_data_p2,desired_start_yearV1,desired_start_yearV2,desired_end_yearV1,desired_end_yearV2,\"test\",\"test\",running_mean_window = running_mean_windowV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "significance_map_difference = t_test(analogues_list_p1, analogues_list_p2,p_value_difference_mapsV)\n",
    "plot_differencet_centeredt(lat_box,lon_box,pressure_difference_between_periods,significance_map_difference,\"test\",\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ook difference voor pressure\n",
    "lat_global_pressure,lon_global_pressure,pressure_global = load_data(data_path_pressure,\"lat\",\"lon\",\"msl\")\n",
    "lat_pressure_box, lon_pressure_box, pressure_box = extract_area(S1,N1,W1,E1,lat_global_pressure,lon_global_pressure,pressure_global, event = False)\n",
    "pressure_box_semi_filtered_p1,filtered_years_list_pressure_p1,filtered_months_list_pressure_p1 = extract_years_and_months_era5(pressure_box,1,12,desired_start_yearV1,desired_end_yearV1,list_with_months, list_with_years)\n",
    "pressure_box_semi_filtered_p2,filtered_years_list_pressure_p2,filtered_months_list_pressure_p2 = extract_years_and_months_era5(pressure_box,1,12,desired_start_yearV2,desired_end_yearV2,list_with_months, list_with_years)\n",
    "actual_pressure_difference_between_periods, mean_pressure_analogue_array_p1, mean_pressure_analogue_array_p2, pressure_analogues_array_p1, pressure_analogues_array_p2, pressure_analogues_list_p1, pressure_analogues_list_p2 = complete_and_mean_analogue_array(analogues_index_list_p1,pressure_box_semi_filtered_p1,analogues_index_list_p2,pressure_box_semi_filtered_p2)\n",
    "significance_map_difference_pressure = t_test(pressure_analogues_list_p1, pressure_analogues_list_p2,p_value_difference_mapsV)\n",
    "plot_differencet_centeredt(lat_pressure_box, lon_pressure_box,actual_pressure_difference_between_periods,significance_map_difference_pressure,\"test\",\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Proberen**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from matplotlib.ticker import ScalarFormatter\n",
    "\n",
    "fig = plt.figure(figsize = (14,7))\n",
    "gs = GridSpec(3, 6, figure=fig, height_ratios=[1, 1, 0.1], width_ratios=[1, 1,1,1,1,1])\n",
    "#ax = plt.axes(projection = ccrs.PlateCarree())\n",
    "\n",
    "levels1 = np.linspace(-1.75*1e7,1.75*1e7,15)\n",
    "levels2 = np.linspace(-3*1e6,3*1e6,13)\n",
    "ax1 = fig.add_subplot(gs[0, 1:3],projection=ccrs.PlateCarree())\n",
    "contour1 = ax1.contourf(lon_box, lat_box, event_box[0], cmap = \"RdBu_r\", transform = ccrs.PlateCarree(), levels = levels1,extend = \"both\") #levels=np.linspace(-8.2e7, 1e7, 10), extend='both\n",
    "ax1.coastlines()\n",
    "ax1.add_feature(cf.BORDERS)\n",
    "ax1.set_title(\"A) Event\",fontsize = 10)\n",
    "#plt.colorbar(ax1, ax=ax1, orientation = \"horizontal\", label = \"Pressure (Pa)\", pad = 0.05)\n",
    "\n",
    "ax2 = fig.add_subplot(gs[1, 0:2],projection=ccrs.PlateCarree())\n",
    "contour1 = ax2.contourf(lon_box, lat_box, mean_analogue_array_p1, cmap = \"RdBu_r\", transform = ccrs.PlateCarree(), levels = levels1,extend = \"both\") #levels=np.linspace(-8.2e7, 1e7, 10), extend='both\n",
    "ax2.coastlines()\n",
    "ax2.add_feature(cf.BORDERS)\n",
    "ax2.set_title(\"B) Past\",fontsize = 10)\n",
    "\n",
    "ax3 = fig.add_subplot(gs[1, 2:4],projection=ccrs.PlateCarree())\n",
    "contour1 = ax3.contourf(lon_box, lat_box, mean_analogue_array_p2, cmap = \"RdBu_r\", transform = ccrs.PlateCarree(), levels = levels1,extend = \"both\") #levels=np.linspace(-8.2e7, 1e7, 10), extend='both\n",
    "ax3.coastlines()\n",
    "ax3.add_feature(cf.BORDERS)\n",
    "ax3.set_title(\"C) Present\",fontsize = 10)\n",
    "\n",
    "ax4 = fig.add_subplot(gs[1, 4:],projection=ccrs.PlateCarree())\n",
    "contour2 = ax4.contourf(lon_box, lat_box, pressure_difference_between_periods, cmap = \"PiYG\", transform = ccrs.PlateCarree(), levels = levels2, extend = \"both\") #levels=np.linspace(-8.2e7, 1e7, 10), extend='both\n",
    "ax4.contourf( lon_box, lat_box,significance_map_difference, levels=[-2,0,2], hatches=[None, '////'], colors='none', transform=ccrs.PlateCarree())\n",
    "ax4.coastlines()\n",
    "ax4.add_feature(cf.BORDERS)\n",
    "ax4.set_title(\"D) Difference\",fontsize = 10)\n",
    "\n",
    "\n",
    "cax1 = fig.add_subplot(gs[2,1:3])\n",
    "plt.colorbar(contour1, cax=cax1, orientation=\"horizontal\", label=\"500 hPa Streamfunction\\n(x$10^7$ m$^2$/s)\")\n",
    "formatter = ScalarFormatter(useMathText=True)\n",
    "formatter.set_powerlimits((0, 0))  # Set the exponent limits\n",
    "cax1.yaxis.set_major_formatter(formatter)\n",
    "\n",
    "cax2 = fig.add_subplot(gs[2,4:])\n",
    "plt.colorbar(contour2, cax=cax2, orientation=\"horizontal\", label=\"Difference in 500 hPa streamfunction\\n(x$10^6$ m$^2$/s)\")\n",
    "#plt.savefig(\"/usr/people/noest/stage_folders/outputs/figures_net/difference_test3.png\",dpi=600)\n",
    "#plt.subplots_adjust(hspace=0.9)  # Change the value as needed\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from matplotlib.ticker import ScalarFormatter\n",
    "\n",
    "fig = plt.figure(figsize = (14,3))\n",
    "gs = GridSpec(1, 6, figure=fig, height_ratios=[1,], width_ratios=[1, 1,1,1,1,1])\n",
    "#ax = plt.axes(projection = ccrs.PlateCarree())\n",
    "\n",
    "levels1 = np.linspace(-1.75*1e7,1.75*1e7,15)\n",
    "levels2 = np.linspace(-3*1e6,3*1e6,13)\n",
    "# ax1 = fig.add_subplot(gs[0, 1:3],projection=ccrs.PlateCarree())\n",
    "# contour1 = ax1.contourf(lon_box, lat_box, event_box[0], cmap = \"RdBu_r\", transform = ccrs.PlateCarree(), levels = levels1,extend = \"both\") #levels=np.linspace(-8.2e7, 1e7, 10), extend='both\n",
    "# ax1.coastlines()\n",
    "# ax1.add_feature(cf.BORDERS)\n",
    "# ax1.set_title(\"A) Event\",fontsize = 10)\n",
    "#plt.colorbar(ax1, ax=ax1, orientation = \"horizontal\", label = \"Pressure (Pa)\", pad = 0.05)\n",
    "\n",
    "ax2 = fig.add_subplot(gs[0, 0:2],projection=ccrs.PlateCarree())\n",
    "contour1 = ax2.contourf(lon_box, lat_box, mean_analogue_array_p1, cmap = \"RdBu_r\", transform = ccrs.PlateCarree(), levels = levels1,extend = \"both\") #levels=np.linspace(-8.2e7, 1e7, 10), extend='both\n",
    "ax2.coastlines()\n",
    "ax2.add_feature(cf.BORDERS)\n",
    "ax2.set_title(\"B) Past\",fontsize = 10)\n",
    "\n",
    "ax3 = fig.add_subplot(gs[0, 2:4],projection=ccrs.PlateCarree())\n",
    "contour1 = ax3.contourf(lon_box, lat_box, mean_analogue_array_p2, cmap = \"RdBu_r\", transform = ccrs.PlateCarree(), levels = levels1,extend = \"both\") #levels=np.linspace(-8.2e7, 1e7, 10), extend='both\n",
    "ax3.coastlines()\n",
    "ax3.add_feature(cf.BORDERS)\n",
    "ax3.set_title(\"C) Present\",fontsize = 10)\n",
    "\n",
    "ax4 = fig.add_subplot(gs[0, 4:],projection=ccrs.PlateCarree())\n",
    "contour2 = ax4.contourf(lon_box, lat_box, pressure_difference_between_periods, cmap = \"PiYG\", transform = ccrs.PlateCarree(), levels = levels2, extend = \"both\") #levels=np.linspace(-8.2e7, 1e7, 10), extend='both\n",
    "ax4.contourf( lon_box, lat_box,significance_map_difference, levels=[-2,0,2], hatches=[None, '////'], colors='none', transform=ccrs.PlateCarree())\n",
    "ax4.coastlines()\n",
    "ax4.add_feature(cf.BORDERS)\n",
    "ax4.set_title(\"D) Difference\",fontsize = 10)\n",
    "\n",
    "title = f\"MIROC6 (r{ensemblenr})\"\n",
    "fig.text(0.1, 0.5, title, va='center', ha='center', rotation='vertical', fontsize=12)\n",
    "\n",
    "# cax1 = fig.add_subplot(gs[2,1:3])\n",
    "# plt.colorbar(contour1, cax=cax1, orientation=\"horizontal\", label=\"500 hPa Streamfunction\\n(x$10^7$ m$^2$/s)\")\n",
    "# formatter = ScalarFormatter(useMathText=True)\n",
    "# formatter.set_powerlimits((0, 0))  # Set the exponent limits\n",
    "# cax1.yaxis.set_major_formatter(formatter)\n",
    "\n",
    "# cax2 = fig.add_subplot(gs[2,4:])\n",
    "# plt.colorbar(contour2, cax=cax2, orientation=\"horizontal\", label=\"Difference in 500 hPa streamfunction\\n(x$10^6$ m$^2$/s)\")\n",
    "#plt.savefig(f\"/usr/people/noest/stage_folders/outputs/net/serious_run2/intensity/int_mir{ensemblenr}.png\",dpi=300)\n",
    "#plt.subplots_adjust(hspace=0.9)  # Change the value as needed\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from matplotlib.ticker import ScalarFormatter\n",
    "\n",
    "fig = plt.figure(figsize = (15,5.5))\n",
    "gs = GridSpec(2, 6, figure=fig, height_ratios=[1,0.1], width_ratios=[1, 1,1,1,1,1])\n",
    "#ax = plt.axes(projection = ccrs.PlateCarree())\n",
    "\n",
    "levels1 = np.linspace(-1.75*1e7,1.75*1e7,15)\n",
    "levels2 = np.linspace(-3*1e6,3*1e6,13)\n",
    "# ax1 = fig.add_subplot(gs[0, 1:3],projection=ccrs.PlateCarree())\n",
    "# contour1 = ax1.contourf(lon_box, lat_box, event_box[0], cmap = \"RdBu_r\", transform = ccrs.PlateCarree(), levels = levels1,extend = \"both\") #levels=np.linspace(-8.2e7, 1e7, 10), extend='both\n",
    "# ax1.coastlines()\n",
    "# ax1.add_feature(cf.BORDERS)\n",
    "# ax1.set_title(\"A) Event\",fontsize = 10)\n",
    "#plt.colorbar(ax1, ax=ax1, orientation = \"horizontal\", label = \"Pressure (Pa)\", pad = 0.05)\n",
    "\n",
    "ax2 = fig.add_subplot(gs[0, 0:2],projection=ccrs.PlateCarree())\n",
    "contour1 = ax2.contourf(lon_box, lat_box, mean_analogue_array_p1, cmap = \"RdBu_r\", transform = ccrs.PlateCarree(), levels = levels1,extend = \"both\") #levels=np.linspace(-8.2e7, 1e7, 10), extend='both\n",
    "ax2.coastlines()\n",
    "ax2.add_feature(cf.BORDERS)\n",
    "ax2.set_title(\"B) Past\",fontsize = 10)\n",
    "\n",
    "ax3 = fig.add_subplot(gs[0, 2:4],projection=ccrs.PlateCarree())\n",
    "contour1 = ax3.contourf(lon_box, lat_box, mean_analogue_array_p2, cmap = \"RdBu_r\", transform = ccrs.PlateCarree(), levels = levels1,extend = \"both\") #levels=np.linspace(-8.2e7, 1e7, 10), extend='both\n",
    "ax3.coastlines()\n",
    "ax3.add_feature(cf.BORDERS)\n",
    "ax3.set_title(\"C) Present\",fontsize = 10)\n",
    "\n",
    "ax4 = fig.add_subplot(gs[0, 4:],projection=ccrs.PlateCarree())\n",
    "contour2 = ax4.contourf(lon_box, lat_box, pressure_difference_between_periods, cmap = \"PiYG\", transform = ccrs.PlateCarree(), levels = levels2, extend = \"both\") #levels=np.linspace(-8.2e7, 1e7, 10), extend='both\n",
    "ax4.contourf( lon_box, lat_box,significance_map_difference, levels=[-2,0,2], hatches=[None, '////'], colors='none', transform=ccrs.PlateCarree())\n",
    "ax4.coastlines()\n",
    "ax4.add_feature(cf.BORDERS)\n",
    "ax4.set_title(\"D) Difference\",fontsize = 10)\n",
    "\n",
    "title = f\"MIROC6 (r{ensemblenr})\"\n",
    "\n",
    "cax1 = fig.add_subplot(gs[1,1:3])\n",
    "plt.colorbar(contour1, cax=cax1, orientation=\"horizontal\", label=\"500 hPa Streamfunction\\n(x$10^7$ m$^2$/s)\")\n",
    "formatter = ScalarFormatter(useMathText=True)\n",
    "formatter.set_powerlimits((0, 0))  # Set the exponent limits\n",
    "cax1.yaxis.set_major_formatter(formatter)\n",
    "\n",
    "cax2 = fig.add_subplot(gs[1,4:])\n",
    "plt.colorbar(contour2, cax=cax2, orientation=\"horizontal\", label=\"Difference in 500 hPa streamfunction\\n(x$10^6$ m$^2$/s)\")\n",
    "plt.tight_layout()\n",
    "fig.text(-0.02, 0.57, title, va='center', ha='center', rotation='vertical', fontsize=12)\n",
    "plt.savefig(f\"/usr/people/noest/stage_folders/outputs/net/serious_run2/intensity/int_mir{ensemblenr}2_q.png\",dpi=600)\n",
    "#plt.subplots_adjust(hspace=0.9)  # Change the value as needed\n",
    "plt.show()\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "knmi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
