{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gP6DHEg44uwt"
      },
      "source": [
        "# **Notes**\n",
        "### Analogue analysis for HadGEM3 model streamfunction data\n",
        "#### Also makes the violin and intensity plots\n",
        "\n",
        "* (Folder should only have files containing data from 1950-2014?)\n",
        "* The AMO functions are not propely tested. When using anything other than the violin plots and intensity plots (the plots that made it into the actual report), I would recommend to double-check the code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NV6IBrz4aDYi"
      },
      "source": [
        "# **Preparation**\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "201aaMXNZy1G",
        "outputId": "dcea4114-5357-48a3-f267-929ea95fe6f6"
      },
      "outputs": [],
      "source": [
        "import numpy as np # for storing vector and matrix data\n",
        "import matplotlib.pyplot as plt # to plot figures\n",
        "import netCDF4 as nc #to read netCDF files\n",
        "import cartopy.crs as ccrs # to plot maps\n",
        "# (ergens in test ook: import cartopy as cart)\n",
        "import cartopy.feature as cf\n",
        "# from matplotlib import ticker\n",
        "import scipy.io\n",
        "from scipy.stats import pearsonr # voor persistence\n",
        "import scipy.stats as stats\n",
        "# from cartopy.util import add_cyclic_point\n",
        "import os\n",
        "from matplotlib.gridspec import GridSpec\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ika_HiUOaF8J"
      },
      "source": [
        "# **Functions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Dd8XaiSzaNeJ"
      },
      "outputs": [],
      "source": [
        "def load_data(path,*variables_to_add):\n",
        "  \"\"\"\n",
        "Provide the path to a file and the variables you want to extract\n",
        "  \"\"\"\n",
        "  data = nc.Dataset(path, mode='r')\n",
        "  variable_list = []\n",
        "  for variable in variables_to_add:\n",
        "    var =data.variables[variable][:]\n",
        "    variable_list.append(var)\n",
        "  return variable_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "KJSU9UGufHWs"
      },
      "outputs": [],
      "source": [
        "def extract_area(S, N, W, E, lat, lon, variable,event = False):\n",
        "    \"\"\"\n",
        "    This function slices the data given the S, N, W, E bounds. Use event = True if there are only two dimensions (since then there is no time dimension), this means after using this\n",
        "    function you need to use event[0] to get the data\n",
        "    \"\"\"\n",
        "    # Change longitude data to go from -180 to 180\n",
        "    for i in range(len(lon)):\n",
        "        if lon[i] > 180:\n",
        "          lon[i] = lon[i] - 360\n",
        "        else:\n",
        "          lon[i] = lon[i]\n",
        "\n",
        "    # Calculate the index of the bounds\n",
        "    sIndex = np.argmin(np.abs(lat - S))\n",
        "    nIndex = np.argmin(np.abs(lat - N))\n",
        "    wIndex = np.argmin(np.abs(lon - W))\n",
        "    eIndex = np.argmin(np.abs(lon - E))\n",
        "\n",
        "    if event:\n",
        "        variable = np.expand_dims(variable, axis = 0)\n",
        "\n",
        "    if wIndex > eIndex: # If the west index is higher than the east index, think of the right side of the world map as left boundary and vice versa\n",
        "        latSlice = lat[sIndex: nIndex + 1]\n",
        "        lonSlice = np.concatenate((lon[wIndex:], lon[:eIndex + 1]))\n",
        "        variableSlice = np.concatenate((variable[:, sIndex: nIndex + 1, wIndex:], variable[:, sIndex: nIndex + 1, :eIndex + 1]), axis = 2)\n",
        "\n",
        "    else:\n",
        "        latSlice = lat[sIndex: nIndex + 1]\n",
        "        lonSlice = lon[wIndex: eIndex + 1]\n",
        "        variableSlice = variable[:, sIndex: nIndex + 1, wIndex: eIndex + 1]\n",
        "\n",
        "    return latSlice, lonSlice, variableSlice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "JTMdyGiD_VM8"
      },
      "outputs": [],
      "source": [
        "def extract_season_year(variable,yearlength,start_day,end_day, start_year = False, end_year = False):\n",
        "  \"\"\"\n",
        "  Start day and end day should be the actuall day, so if you want the second day, third and fourth day, do 2,4 (151,240 would be JJA?) jaren ook: 1,5 is 1 tot en met 5\n",
        "  nadenken dat als gaat checken met al gesneden data dat yearlenght 90 is als op 3 maanden gesneden\n",
        "  \"\"\"\n",
        "  start_index = start_day-1\n",
        "  end_index = end_day-1\n",
        "  if start_year == False and end_year == False:\n",
        "    years = variable.shape[0]//yearlength\n",
        "    for year in range(years):\n",
        "      if year == 0:\n",
        "        selected_data = variable[(year*360)+start_index:(end_index+1),:,:] # +1 omdat tot is ipv tot en met voor de laatste\n",
        "      elif year != 0:\n",
        "        add_data = variable[(year*360)+start_index:(year*360)+(end_index+1),:,:] # stel is 10, na 1 jaar dan 370 is TOT 370 dus index 369 en dan is dag 10\n",
        "        selected_data = np.concatenate((selected_data, add_data), axis = 0)\n",
        "    return selected_data\n",
        "  else:\n",
        "    years = (end_year-start_year) + 1\n",
        "    for year in range(years):\n",
        "      year_multiplier = (year + start_year) - 1\n",
        "      if year == 0:\n",
        "        selected_data = variable[(year_multiplier*360)+start_index:(year_multiplier*360)+(end_index+1),:,:] # +1 omdat tot is ipv tot en met voor de laatste\n",
        "      elif year != 0:\n",
        "        add_data = variable[(year_multiplier*360)+start_index:(year_multiplier*360)+(end_index+1),:,:] # stel is 10, na 1 jaar dan 370 is TOT 370 dus index 369 en dan is dag 10\n",
        "        selected_data = np.concatenate((selected_data, add_data), axis = 0)\n",
        "    return selected_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "SY3gCzDP1mY2"
      },
      "outputs": [],
      "source": [
        "def euclidian_distance(data, event):\n",
        "  \"\"\"\n",
        "Calculates the euclidian distance for each day in the data compared to a given single event, gives an array (~list) of all the distances in chronological order\n",
        "  \"\"\"\n",
        "  return np.sqrt(np.sum((data - event)**2, axis = (1, 2)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ZNv97tVK4FLi"
      },
      "outputs": [],
      "source": [
        "def determine_analogues(euclidian_distances, target_number_of_analogues,analogue_seperation_range):\n",
        "  \"\"\"\n",
        "  Determine the best analogues. Give an array of all unsorted euclidan distances. The analogue_seperation_range determines how many days have to seperate\n",
        "  the different analogues (if analogue_seperation_range = 5, the fifth day after an analogue can also still not be a new analogue(dus die dag mag ook niet)).\n",
        "  Target_number_of_analogues is how many analogues are selected. Returns the indexes of the best analogues in the original data, and the euclidian distances corresponding to the analogues.\n",
        "  \"\"\"\n",
        "  distance_index_dictionary = {value: index for index, value in enumerate(euclidian_distances)} # Gives the index in the original euclidian distances list for a value in the sorted list\n",
        "  sorted_distances = np.sort(euclidian_distances) # sort the distances from low to high euclidian distances\n",
        "  analogues_index_list = [] # create a list to save the indexes of the selected analogues\n",
        "  euclidian_distance_list = [] # create a list to save the euclidian distances of the selected analogues\n",
        "  i=0 #index counter\n",
        "  while len(analogues_index_list) < target_number_of_analogues and (i < len(euclidian_distances)):\n",
        "    differences= []\n",
        "    index = distance_index_dictionary[sorted_distances[i]]\n",
        "    if len(analogues_index_list) == 0:\n",
        "      analogues_index_list.append(index)\n",
        "      euclidian_distance_list.append(sorted_distances[i])\n",
        "      i = i + 1\n",
        "    else:\n",
        "      for item in analogues_index_list:\n",
        "        difference = (index-item)\n",
        "        if difference < (-1*analogue_seperation_range) or difference > analogue_seperation_range:\n",
        "          differences.append(2) #goed\n",
        "        elif difference >= (-1*analogue_seperation_range) and difference <= (analogue_seperation_range):\n",
        "          differences.append(1) #niet goed\n",
        "      if min(differences) == 2:\n",
        "        analogues_index_list.append(index)\n",
        "        euclidian_distance_list.append(sorted_distances[i])\n",
        "        i = i + 1\n",
        "      elif min(differences) == 1:\n",
        "        i = i + 1\n",
        "\n",
        "  return analogues_index_list,euclidian_distance_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "6u48c2zsfZ0B"
      },
      "outputs": [],
      "source": [
        "def persistence(analogue_indexes,data,c_threshold):\n",
        "  \"\"\"\n",
        "Defines the persistence of each analogue based on a correlation threshold (>=). It returns a list with a list for each analogue, -1 is a day before the event, 1 is a day after the event.\n",
        "So if you want the total lenght of the event you still need to add the event itself? (nu wel in lenghts gedaan :) Also returns the correlations in the correct order.\n",
        "(als je dus de analogues bepaald heb met data gesliced op seizoen die ook hier gebruiken?)\n",
        "  \"\"\"\n",
        "  list_for_lists = [] #\n",
        "  list_for_lists_values = []\n",
        "  # nu 500 ook laatste dag dus kan niet vooruit\n",
        "  for index in analogue_indexes:\n",
        "    list_for_length = []\n",
        "    list_for_values = []\n",
        "    analogue_map = data[index]\n",
        "    Ab,Ac = np.shape(analogue_map)\n",
        "    analogue_af = analogue_map.reshape(Ab*Ac)\n",
        "\n",
        "    index_dag1 = index + 1\n",
        "    index_dag2 = index - 1\n",
        "    correlation = 1\n",
        "\n",
        "    while correlation >= c_threshold and index_dag2 >= 0:\n",
        "      day_data = data[index_dag2]\n",
        "      Bb,Bc = np.shape(day_data)\n",
        "      day_map_af = day_data.reshape(Bb*Bc)\n",
        "      correlation = pearsonr(analogue_af, day_map_af)[0]\n",
        "      if correlation >= c_threshold:\n",
        "        list_for_length.insert(0,-1)\n",
        "        list_for_values.insert(0,correlation) # dit is met insert dat is anders, dan doet ie het aan de voorkant dus dan staan de values echt op volgorde van de dag.\n",
        "      index_dag2 = index_dag2 - 1\n",
        "\n",
        "    correlation = 1\n",
        "\n",
        "    while correlation >= c_threshold and index_dag1 <= (len(data)-1):\n",
        "      day_data = data[index_dag1]\n",
        "      Bb,Bc = np.shape(day_data)\n",
        "      day_map_af = day_data.reshape(Bb*Bc)\n",
        "      correlation = pearsonr(analogue_af, day_map_af)[0]\n",
        "      if correlation >= c_threshold:\n",
        "        list_for_length.append(1)\n",
        "        list_for_values.append(correlation)\n",
        "      index_dag1 = index_dag1 + 1\n",
        "\n",
        "\n",
        "    list_for_lists.append(list_for_length) # nu dus niet anaologue zelf meegenomen in lengte dus voor lengte nog lengte lijst +1\n",
        "    list_for_lists_values.append(list_for_values)\n",
        "\n",
        "  list_for_lengths = []\n",
        "  for lijst in list_for_lists:\n",
        "    lengte = len(lijst) + 1\n",
        "    list_for_lengths.append(lengte)\n",
        "\n",
        "  return list_for_lists, list_for_lists_values,list_for_lengths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "E-5Cz3KM8dRV"
      },
      "outputs": [],
      "source": [
        "def plot_variable(lat, lon, variable,folder,name):\n",
        "    plt.figure(figsize = (10,10))\n",
        "    ax = plt.axes(projection = ccrs.PlateCarree())\n",
        "    plot = plt.contourf(lon, lat, variable, cmap = \"RdBu_r\", transform = ccrs.PlateCarree(), levels = 15) #levels=np.linspace(-8.2e7, 1e7, 10), extend='both\n",
        "    ax.coastlines()\n",
        "    ax.add_feature(cf.BORDERS)\n",
        "    plt.colorbar(plot, ax=ax, orientation = \"horizontal\", label = \"Pressure (Pa)\", pad = 0.05)\n",
        "    plt.savefig(f\"{folder}/{name}.png\",dpi=300)\n",
        "    #plt.show()\n",
        "    plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def determine_analogues_era5(euclidian_distances, target_number_of_analogues,analogue_seperation_range,list_months,list_years,desired_start_month,desired_end_month,event_is_in_data = True):\n",
        "  \"\"\"\n",
        "  Determine the best analogues. Give an array of all unsorted euclidan distances. The analogue_seperation_range determines how many days have to seperate\n",
        "  the different analogues (if analogue_seperation_range = 5, the fifth day after an analogue can also still not be a new analogue(dus die dag mag ook niet)).\n",
        "  Target_number_of_analogues is how many analogues are selected. Returns the indexes of the best analogues in the original data, and the euclidian distances corresponding to the analogues.\n",
        "  Give semi-filtered data to euclidian_distances: filtered on years but all months, so the seperation range doesn't continue to count in the next season\n",
        "  Should give semi-filtered lists of months and years as well??\n",
        "  \"\"\"\n",
        "  distance_index_dictionary = {value: index for index, value in enumerate(euclidian_distances)} # Gives the index in the original euclidian distances list for a value in the sorted list\n",
        "  sorted_distances = np.sort(euclidian_distances) # sort the distances from low to high euclidian distances\n",
        "  analogues_index_list = [] # create a list to save the indexes of the selected analogues\n",
        "  euclidian_distance_list = [] # create a list to save the euclidian distances of the selected analogues\n",
        "  selected_analogue_years = []\n",
        "  if event_is_in_data == False:\n",
        "    i = 0\n",
        "    if sorted_distances[0] == 0:\n",
        "      print (\"WARNING: event does seem to be in data, while event_is_in_data == False (zelf)\")\n",
        "    while len(analogues_index_list) < target_number_of_analogues and (i < len(euclidian_distances)):\n",
        "      differences= []\n",
        "      index = distance_index_dictionary[sorted_distances[i]]\n",
        "      month = list_months[index]\n",
        "      year = list_years[index]\n",
        "      if len(analogues_index_list) == 0:\n",
        "        if month >= desired_start_month and month <= desired_end_month:\n",
        "          analogues_index_list.append(index)\n",
        "          euclidian_distance_list.append(sorted_distances[i])\n",
        "          selected_analogue_years.append(year)\n",
        "        i = i + 1\n",
        "      else:\n",
        "        if month >= desired_start_month and month <= desired_end_month:\n",
        "          for item in analogues_index_list:\n",
        "            difference = (index-item)\n",
        "            if difference < (-1*analogue_seperation_range) or difference > analogue_seperation_range:\n",
        "              differences.append(2) #goed\n",
        "            elif difference >= (-1*analogue_seperation_range) and difference <= (analogue_seperation_range):\n",
        "              differences.append(1) #niet goed\n",
        "          if min(differences) == 2:\n",
        "            analogues_index_list.append(index)\n",
        "            euclidian_distance_list.append(sorted_distances[i])\n",
        "            selected_analogue_years.append(year)\n",
        "            i = i + 1\n",
        "          elif min(differences) == 1:\n",
        "            i = i + 1\n",
        "        else:\n",
        "          i = i + 1\n",
        "\n",
        "  elif event_is_in_data == True: #Need to make sure the selected analogues are also 5 days away from the event, eventhough it is not a selecgted analogue \n",
        "    event_index = distance_index_dictionary[sorted_distances[0]]\n",
        "    analogues_index_list.append(event_index)\n",
        "    i=1 #index counter\n",
        "    if sorted_distances[0] != 0:\n",
        "      print (\"WARNING: event does not seem to be in data, while event_is_in_data == True (zelf)\")\n",
        "    while len(analogues_index_list) < (target_number_of_analogues+1) and (i < len(euclidian_distances)): #+1 omdat event er nu ook in en die later weghalen\n",
        "      differences= []\n",
        "      index = distance_index_dictionary[sorted_distances[i]]\n",
        "      month = list_months[index]\n",
        "      year = list_years[index]\n",
        "      if len(analogues_index_list) == 0:\n",
        "        if month >= desired_start_month and month <= desired_end_month:\n",
        "          analogues_index_list.append(index)\n",
        "          euclidian_distance_list.append(sorted_distances[i])\n",
        "          selected_analogue_years.append(year)\n",
        "        i = i + 1\n",
        "      else:\n",
        "        if month >= desired_start_month and month <= desired_end_month:\n",
        "          for item in analogues_index_list:\n",
        "            difference = (index-item)\n",
        "            if difference < (-1*analogue_seperation_range) or difference > analogue_seperation_range:\n",
        "              differences.append(2) #goed\n",
        "            elif difference >= (-1*analogue_seperation_range) and difference <= (analogue_seperation_range):\n",
        "              differences.append(1) #niet goed\n",
        "          if min(differences) == 2:\n",
        "            analogues_index_list.append(index)\n",
        "            euclidian_distance_list.append(sorted_distances[i])\n",
        "            selected_analogue_years.append(year)\n",
        "            i = i + 1\n",
        "          elif min(differences) == 1:\n",
        "            i = i + 1\n",
        "        else:\n",
        "          i = i + 1\n",
        "    analogues_index_list.pop(0)\n",
        "\n",
        "  if len(analogues_index_list) < target_number_of_analogues:\n",
        "    print (\"WARNING: not enough data to find the required amount of analogues (zelf)\")\n",
        "  if len(analogues_index_list) != target_number_of_analogues:\n",
        "    print (\"WARNING: not the right amount of analogues has been found\")\n",
        "  return analogues_index_list,euclidian_distance_list,selected_analogue_years"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "7B-1Fx0YATlM"
      },
      "outputs": [],
      "source": [
        "def typicality(analogue_distances, analogue_indexes,data,target_number_of_analogues,analogue_seperation_range,month_list,year_list,season_start,season_end):\n",
        "  \"\"\"\n",
        "Calculates the Tevent en Tanalogue. Distances need to be from the selected analogues only. Data has to be the one used to calculate the original analogues.\n",
        "  \"\"\"\n",
        "  Tevent = 1/(sum(analogue_distances))\n",
        "  list_for_Tanalogue = []\n",
        "  for i in range(len(analogue_indexes)):\n",
        "    index_in_used_data = analogue_indexes[i]\n",
        "    euclidian_distance_for_analogue = euclidian_distance(data,data[index_in_used_data])\n",
        "    index_for_analogues_for_analogue,distance_for_analogues_for_analogue, analogues_years = determine_analogues_era5(euclidian_distance_for_analogue, target_number_of_analogues,analogue_seperation_range,month_list,year_list,season_start,season_end,event_is_in_data=True)\n",
        "    Tanalogue = 1/(sum(distance_for_analogues_for_analogue))\n",
        "    list_for_Tanalogue.append(Tanalogue)\n",
        "  return Tevent, list_for_Tanalogue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "sPTpuiJbyOM1"
      },
      "outputs": [],
      "source": [
        "def violin_plot(Tevent_past,Tevent_present,Tanalogue_past,Tanalogue_present,persistence_past,persistence_present,folder,name):\n",
        "  \"\"\"\n",
        "Doet het? maar stipjes van persistence standaard op 1 want hebt nog niet van event?\n",
        "  \"\"\"\n",
        "  u, p = stats.ttest_ind(Tanalogue_past, Tanalogue_present) #calculate p value\n",
        "  text_to_plot = f\"p = {p:.3f}\"\n",
        "\n",
        "  fig, (ax1,ax2) = plt.subplots(1,2,figsize = (8, 5))\n",
        "  violins = ax1.violinplot([Tanalogue_past, Tanalogue_present], [1, 1.6], showmeans=False, showextrema=False, showmedians=False)\n",
        "  colors = [\"magenta\", \"green\"]\n",
        "  for pc, color in zip(violins[\"bodies\"], colors): #pc zijn de violins dus moet zo als verschillende kleuren wil\n",
        "    pc.set_facecolor(color)\n",
        "  ax1.plot(1,Tevent_past, marker = \"o\", color = \"r\") #plot de events\n",
        "  ax1.plot(1.6,Tevent_present, marker = \"o\", color = \"r\")  #plot de events\n",
        "  ax1.axhline(np.mean(Tanalogue_past), color = colors[0], linewidth = 3)\n",
        "  ax1.axhline(np.mean(Tanalogue_present), color = colors[1], linewidth = 3)\n",
        "  ax1.set_xticks([1, 1.6])\n",
        "  ax1.set_xticklabels([\"Past\", \"Present\"])\n",
        "  ax1.set_ylim(top = (1.035*(max(max(Tanalogue_past),max(Tanalogue_present))))) # to ensure that the plotted p-value and graph do not overlap\n",
        "  ax1.text(0.035,0.95,text_to_plot,transform=ax1.transAxes)\n",
        "\n",
        "  u, p = stats.ttest_ind(persistence_past, persistence_present) #calculate p value\n",
        "  text_to_plot = f\"p = {p:.3f}\"\n",
        "\n",
        "  violins = ax2.violinplot([persistence_past, persistence_present], [1, 1.6], showmeans=False, showextrema=False, showmedians=False)\n",
        "  colors = [\"magenta\", \"green\"]\n",
        "  for pc, color in zip(violins[\"bodies\"], colors): #pc zijn de violins dus moet zo als verschillende kleuren wil\n",
        "    pc.set_facecolor(color)\n",
        "  ax2.plot(1,1, marker = \"o\", color = \"r\") #plot de events\n",
        "  ax2.plot(1.6,1, marker = \"o\", color = \"r\")  #plot de events\n",
        "  ax2.axhline(np.mean(persistence_past), color = colors[0], linewidth = 3)\n",
        "  ax2.axhline(np.mean(persistence_present), color = colors[1], linewidth = 3)\n",
        "  ax2.set_xticks([1, 1.6])\n",
        "  ax2.set_xticklabels([\"Past\", \"Present\"])\n",
        "  ax2.set_ylim(top = (1.07*(max(max(persistence_past),max(persistence_present))))) # to ensure that the plotted p-value and graph do not overlap\n",
        "  ax2.text(0.035,0.95,text_to_plot,transform=ax2.transAxes)\n",
        "\n",
        "  plt.savefig(f\"{folder}/{name}.png\",dpi=300)\n",
        "  plt.close()\n",
        "  #plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Z3anvXpTC95F"
      },
      "outputs": [],
      "source": [
        "def yearly_minimum_euclidian_distance(euclidian_distances, yearlength = 360):\n",
        "  \"\"\"\n",
        "Finds the best analogue (lowest ecleudian distance) for each year to do trend analysis. Returns an array with the distances.\n",
        "  \"\"\"\n",
        "  distances_per_year = euclidian_distances.reshape((euclidian_distances.shape[0] // yearlength, yearlength))\n",
        "  return np.min(distances_per_year, axis = 1) #loopt blijkbaar al automatisch door alle jaren"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "NKYevsV52hj7"
      },
      "outputs": [],
      "source": [
        "def plot_trend(yearly_min_distance,distances, running_mean_window, start_year, end_year, folder, name):\n",
        "  \"\"\"\n",
        "Moet wel goed checken dat length x_ticks and y_data hetzelfde is (dus 100 years, is 1850 tot en met 1949! dus ook end_year = 1949)\n",
        "  \"\"\"\n",
        "  plt.figure(figsize = (10,8))\n",
        "  max_distance = max(distances)\n",
        "  y_data = 1 - (yearly_min_distance/max_distance) # loopt ook somehow al door alle distances\n",
        "  running_mean = np.convolve(y_data, np.ones(running_mean_window) / running_mean_window, mode = \"valid\") #valid is dat rand niet doet (kan wel met 'same' maar dan voegt die nullen toe dus dan average fout,). Kernel (2e input) is weight voor verschillende plekken in window (hier overal gelijk?)\n",
        "  x_ticks = range(start_year, end_year + 1)\n",
        "  plt.ylabel(\"1 - (ED/EDmax)\")\n",
        "  plt.xlabel(\"Years\")\n",
        "\n",
        "  plt.plot(x_ticks,y_data)\n",
        "  plt.plot(range(start_year + (running_mean_window // 2 - 1), end_year - (running_mean_window // 2 - 1)),running_mean) #begin van bram, omdat eerste paar jaar niet window kan doen\n",
        "  plt.savefig(f\"{folder}/{name}.png\",dpi=300)\n",
        "  #plt.show()\n",
        "  plt.close()\n",
        "\n",
        "  return y_data, running_mean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_directories(base_path,name_run,extra_folder1 = \"figures\",extra_folder2 = \"lists\",figure_folders = []):\n",
        "  \"\"\"\n",
        "Create a new folder to save the outputs\n",
        "  \"\"\"\n",
        "  new_directory_path = os.path.join(base_path, name_run)\n",
        "  if not os.path.exists(new_directory_path):\n",
        "    os.makedirs(new_directory_path)\n",
        "\n",
        "  figure_folder_path = os.path.join(new_directory_path, extra_folder1)\n",
        "  if not os.path.exists(figure_folder_path):\n",
        "    os.makedirs(figure_folder_path)\n",
        "\n",
        "  list_folder_path = os.path.join(new_directory_path, extra_folder2)\n",
        "  if not os.path.exists(list_folder_path):\n",
        "    os.makedirs(list_folder_path)\n",
        "  \n",
        "  violin_plot_path = os.path.join(figure_folder_path, \"violin_plots\")\n",
        "  if not os.path.exists(violin_plot_path):\n",
        "    os.makedirs(violin_plot_path)\n",
        "\n",
        "  analogue_plot_path = os.path.join(figure_folder_path, \"analogue_plots\")\n",
        "  if not os.path.exists(analogue_plot_path):\n",
        "    os.makedirs(analogue_plot_path)\n",
        "\n",
        "  difference_plot_path = os.path.join(figure_folder_path, \"difference_plots\")\n",
        "  if not os.path.exists(difference_plot_path):\n",
        "    os.makedirs(difference_plot_path)\n",
        "\n",
        "  trend_plot_path = os.path.join(figure_folder_path, \"trend_plots\")\n",
        "  if not os.path.exists(trend_plot_path):\n",
        "    os.makedirs(trend_plot_path)\n",
        "\n",
        "  AMO_plot_path = os.path.join(figure_folder_path, \"AMO_plots\")\n",
        "  if not os.path.exists(AMO_plot_path):\n",
        "    os.makedirs(AMO_plot_path)\n",
        "\n",
        "  return new_directory_path,figure_folder_path,list_folder_path,violin_plot_path,analogue_plot_path,difference_plot_path,trend_plot_path,AMO_plot_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "-LiOo5CV2ygt"
      },
      "outputs": [],
      "source": [
        "def mean_difference_per_ensemble(analogue_indexes_1850,data_1850,analogue_indexes_1950,data_1950):\n",
        "  list_for_1850 = []\n",
        "  list_for_1950 = []\n",
        "\n",
        "  for index in analogue_indexes_1850:\n",
        "    data_analogue = data_1850[index]\n",
        "    list_for_1850.append(data_analogue)\n",
        "\n",
        "  for index in analogue_indexes_1950:\n",
        "    data_analogue = data_1950[index]\n",
        "    list_for_1950.append(data_analogue)\n",
        "\n",
        "  array_1850 = np.array(list_for_1850) #maak van de list weer een array\n",
        "  mean_array_1850 = np.mean(array_1850, axis = 0)\n",
        "\n",
        "  array_1950 = np.array(list_for_1950) #maak van de list weer een array\n",
        "  mean_array_1950 = np.mean(array_1950, axis = 0)\n",
        "\n",
        "  difference_per_ensemble = mean_array_1950 - mean_array_1850\n",
        "  return difference_per_ensemble, mean_array_1850, mean_array_1950, list_for_1850, list_for_1950"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "C4WASdHBnP-W"
      },
      "outputs": [],
      "source": [
        "def combined_difference_top_analogues(all_analogue_fields,all_analogue_distances,target_number_of_analogues):\n",
        "  \"\"\"\n",
        "all_analogue_fields should be a list with lists, with each inner list containing the fields of all selected analogues for one ensemble member. all_analogue_distances should be the same but with the euclidian distances of the selected analogues.\n",
        "Returns the mean of the top combined selected analogues from all ensemble members combined (so for example the best 30 analogues when taking into account all ensemble members at once) for the difference plot.\n",
        "Also returns the list with all the best analogue fields, for the t-test.\n",
        "  \"\"\"\n",
        "  best_combined_analogue_fields = []\n",
        "  best_combined_distances_list = []\n",
        "\n",
        "  complete_list_fields = []\n",
        "  for analogue_list in all_analogue_fields:\n",
        "    for field in analogue_list:\n",
        "      complete_list_fields.append(field)\n",
        "\n",
        "  complete_list_distances = []\n",
        "  for distance_list in all_analogue_distances:\n",
        "    for distance in distance_list:\n",
        "      complete_list_distances.append(distance)\n",
        "\n",
        "  distance_index_dictionary = {value: index for index, value in enumerate(complete_list_distances)} # Gives the index in the original euclidian distances list for a value in the sorted list\n",
        "  sorted_distances = sorted(complete_list_distances)\n",
        "  for i in range(target_number_of_analogues):\n",
        "    index = distance_index_dictionary[sorted_distances[i]]\n",
        "    data_to_add = complete_list_fields[index]\n",
        "    best_combined_analogue_fields.append(data_to_add)\n",
        "    distance_to_add = sorted_distances[i]\n",
        "    best_combined_distances_list.append(distance_to_add)\n",
        "\n",
        "  array_best_combined_fields = np.array(best_combined_analogue_fields) #maak van de list weer een array\n",
        "  ensembles_combined_mean_array = np.mean(array_best_combined_fields, axis = 0)\n",
        "\n",
        "  return ensembles_combined_mean_array,best_combined_analogue_fields,best_combined_distances_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "dmk0FS_fovLk"
      },
      "outputs": [],
      "source": [
        "def t_test(data1850,data1950,p_value):\n",
        "  \"\"\"\n",
        "Performs a two sides t-test, for a analogue difference map data1850 and data1950 should be lists with arrays for each analogue (so not already the mean!), p_value should be for example 0.05\n",
        "  \"\"\"\n",
        "  number_of_analogues = len(data1850)\n",
        "  significance_mask = data1850[0].copy()\n",
        "  a, b = np.shape(data1850[0])\n",
        "  for i in range(a): # for a x\n",
        "    #print(i)\n",
        "    for j in range(b): # check every y on that x\n",
        "      lijst1850 = []\n",
        "      lijst1950 = []\n",
        "      for R in range(number_of_analogues): # for each combination of x and y (so each cell) check each analogue\n",
        "        lijst1850.append(data1850[R][i,j])\n",
        "        lijst1950.append(data1950[R][i,j])\n",
        "      #u, p = stats.mannwhitneyu(loc_list1,loc_list2)\n",
        "      u, p = stats.ttest_ind(lijst1850, lijst1950)\n",
        "      if p < p_value:\n",
        "        significance_mask[i,j] = 1\n",
        "      else:\n",
        "        significance_mask[i,j] = 0\n",
        "  return significance_mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "bb8dWmzoqaTj"
      },
      "outputs": [],
      "source": [
        "def plot_difference(lat, lon, variable,significance_mask,folder,name):\n",
        "    plt.figure(figsize = (10,10))\n",
        "    ax = plt.axes(projection = ccrs.PlateCarree())\n",
        "    plot = plt.contourf(lon, lat, variable, cmap = \"RdBu_r\", transform = ccrs.PlateCarree(), levels = 15) #levels=np.linspace(-8.2e7, 1e7, 10), extend='both\n",
        "    ax.contourf( lon, lat,significance_mask, levels=[-2,0,2], hatches=[None, '////'], colors='none', transform=ccrs.PlateCarree())\n",
        "    ax.coastlines()\n",
        "    ax.add_feature(cf.BORDERS)\n",
        "    plt.colorbar(plot, ax=ax, orientation = \"horizontal\", label = \"Pressure (Pa)\", pad = 0.05)\n",
        "    plt.savefig(f\"{folder}/{name}.png\",dpi=300)\n",
        "    #plt.show()\n",
        "    plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "def typicality_combined(analogue_distances,analogue_fields,data,target_number_of_analogues,analogue_seperation_range):\n",
        "  \"\"\"\n",
        "Calculates the Tevent en Tanalogue for the top analogues\n",
        "  \"\"\"\n",
        "  Tevent = 1/(sum(analogue_distances))\n",
        "  list_for_Tanalogue = []\n",
        "  for i in range(len(analogue_fields)):\n",
        "    analogue_to_set_as_event = analogue_fields[i]\n",
        "    euclidian_distance_for_analogue = euclidian_distance(data,analogue_to_set_as_event)\n",
        "    index_for_analogues_for_analogue,distance_for_analogues_for_analogue = determine_analogues(euclidian_distance_for_analogue, target_number_of_analogues,analogue_seperation_range)\n",
        "    Tanalogue = 1/(sum(distance_for_analogues_for_analogue))\n",
        "    list_for_Tanalogue.append(Tanalogue)\n",
        "  return Tevent, list_for_Tanalogue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "def typicality_combined_new(analogue_distances, analogue_fields,data,target_number_of_analogues,analogue_seperation_range,month_list,year_list,season_start,season_end):\n",
        "  \"\"\"\n",
        "Calculates the Tevent en Tanalogue. Distances need to be from the selected analogues only. Data has to be the one used to calculate the original analogues.\n",
        "  \"\"\"\n",
        "  Tevent = 1/(sum(analogue_distances))\n",
        "  list_for_Tanalogue = []\n",
        "  for i in range(len(analogue_fields)):\n",
        "    analogue_to_set_as_event = analogue_fields[i]\n",
        "    euclidian_distance_for_analogue = euclidian_distance(data,analogue_to_set_as_event)\n",
        "    index_for_analogues_for_analogue,distance_for_analogues_for_analogue, analogues_years = determine_analogues_era5(euclidian_distance_for_analogue, target_number_of_analogues,analogue_seperation_range,month_list,year_list,season_start,season_end,event_is_in_data=True)\n",
        "    Tanalogue = 1/(sum(distance_for_analogues_for_analogue))\n",
        "    list_for_Tanalogue.append(Tanalogue)\n",
        "  return Tevent, list_for_Tanalogue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "def persistence_combined(analogue_fields,data,c_threshold,yearlength_filtered):\n",
        "  \"\"\"\n",
        "Defines the persistence of each analogue based on a correlation threshold (>=). It returns a list with a list for each analogue, -1 is a day before the event, 1 is a day after the event.\n",
        "So if you want the total lenght of the event you still need to add the event itself? (nu wel in lenghts gedaan :) Also returns the correlations in the correct order.\n",
        "(als je dus de analogues bepaald heb met data gesliced op seizoen die ook hier gebruiken?)\n",
        "  \"\"\"\n",
        "\n",
        "  list_for_lists = [] #\n",
        "  list_for_lists_values = []\n",
        "  analogue_indexes = []\n",
        "\n",
        "  for analogue_field in analogue_fields:\n",
        "    index_counter = 0\n",
        "    match_condition = False\n",
        "    while match_condition == False and index_counter < (data.shape[0]):\n",
        "      data_field = data[index_counter]\n",
        "      if np.array_equal(data_field, analogue_field) == True:\n",
        "        analogue_indexes.append(index_counter)\n",
        "        match_condition = True\n",
        "      else:\n",
        "        index_counter = index_counter + 1\n",
        "\n",
        "  # nu 500 ook laatste dag dus kan niet vooruit\n",
        "  for index in analogue_indexes:\n",
        "    list_for_length = []\n",
        "    list_for_values = []\n",
        "    analogue_map = data[index]\n",
        "    Ab,Ac = np.shape(analogue_map)\n",
        "    analogue_af = analogue_map.reshape(Ab*Ac)\n",
        "\n",
        "    index_dag1 = index + 1\n",
        "    index_dag2 = index - 1\n",
        "    correlation = 1\n",
        "    amount_of_years_passed = index//yearlength_filtered # To ensure that an analogue cant continue to persist in data from a different ensemble member\n",
        "    max_index_threshold_forward = ((amount_of_years_passed+1)*yearlength_filtered) # moet kleiner zijn dan dat (want index 0-89 is 1 jaar)\n",
        "    min_index_threshold_forward = amount_of_years_passed*yearlength_filtered #die mag nog wel\n",
        "    while correlation >= c_threshold and index_dag2 >= min_index_threshold_forward:\n",
        "      day_data = data[index_dag2]\n",
        "      Bb,Bc = np.shape(day_data)\n",
        "      day_map_af = day_data.reshape(Bb*Bc)\n",
        "      correlation = pearsonr(analogue_af, day_map_af)[0]\n",
        "      if correlation >= c_threshold:\n",
        "        list_for_length.insert(0,-1)\n",
        "        list_for_values.insert(0,correlation) # dit is met insert dat is anders, dan doet ie het aan de voorkant dus dan staan de values echt op volgorde van de dag.\n",
        "      index_dag2 = index_dag2 - 1\n",
        "\n",
        "    correlation = 1\n",
        "\n",
        "    while correlation >= c_threshold and index_dag1 < max_index_threshold_forward:\n",
        "      day_data = data[index_dag1]\n",
        "      Bb,Bc = np.shape(day_data)\n",
        "      day_map_af = day_data.reshape(Bb*Bc)\n",
        "      correlation = pearsonr(analogue_af, day_map_af)[0]\n",
        "      if correlation >= c_threshold:\n",
        "        list_for_length.append(1)\n",
        "        list_for_values.append(correlation)\n",
        "      index_dag1 = index_dag1 + 1\n",
        "\n",
        "\n",
        "    list_for_lists.append(list_for_length) # nu dus niet anaologue zelf meegenomen in lengte dus voor lengte nog lengte lijst +1\n",
        "    list_for_lists_values.append(list_for_values)\n",
        "\n",
        "  list_for_lengths = []\n",
        "  for lijst in list_for_lists:\n",
        "    lengte = len(lijst) + 1\n",
        "    list_for_lengths.append(lengte)\n",
        "\n",
        "  return list_for_lists, list_for_lists_values,list_for_lengths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "def persistence_combined_new(analogue_fields,data,c_threshold):\n",
        "  \"\"\"\n",
        "Defines the persistence of each analogue based on a correlation threshold (>=). It returns a list with a list for each analogue, -1 is a day before the event, 1 is a day after the event.\n",
        "So if you want the total lenght of the event you still need to add the event itself? (nu wel in lenghts gedaan :) Also returns the correlations in the correct order.\n",
        "(als je dus de analogues bepaald heb met data gesliced op seizoen die ook hier gebruiken?)\n",
        "New omdat vorige met yearlenght deed, maar nu doet al semi data dus dan hoeft dat niet (en dan filtered yearlenght niet meer 90)\n",
        "  \"\"\"\n",
        "  list_for_lists = [] #\n",
        "  list_for_lists_values = []\n",
        "  analogue_indexes = []\n",
        "\n",
        "  for analogue_field in analogue_fields:\n",
        "    index_counter = 0\n",
        "    match_condition = False\n",
        "    while match_condition == False and index_counter < (data.shape[0]):\n",
        "      data_field = data[index_counter]\n",
        "      if np.array_equal(data_field, analogue_field) == True:\n",
        "        analogue_indexes.append(index_counter)\n",
        "        match_condition = True\n",
        "      else:\n",
        "        index_counter = index_counter + 1\n",
        "        if index_counter == (data.shape[0]):\n",
        "          print (\"WARNING: for persistence combined the analogue field was not found in the data (zelf)\")\n",
        "\n",
        "\n",
        "  # nu 500 ook laatste dag dus kan niet vooruit\n",
        "  for index in analogue_indexes:\n",
        "    list_for_length = []\n",
        "    list_for_values = []\n",
        "    analogue_map = data[index]\n",
        "    Ab,Ac = np.shape(analogue_map)\n",
        "    analogue_af = analogue_map.reshape(Ab*Ac)\n",
        "\n",
        "    index_dag1 = index + 1\n",
        "    index_dag2 = index - 1\n",
        "    correlation = 1\n",
        "\n",
        "    while correlation >= c_threshold and index_dag2 >= 0:\n",
        "      day_data = data[index_dag2]\n",
        "      Bb,Bc = np.shape(day_data)\n",
        "      day_map_af = day_data.reshape(Bb*Bc)\n",
        "      correlation = pearsonr(analogue_af, day_map_af)[0]\n",
        "      if correlation >= c_threshold:\n",
        "        list_for_length.insert(0,-1)\n",
        "        list_for_values.insert(0,correlation) # dit is met insert dat is anders, dan doet ie het aan de voorkant dus dan staan de values echt op volgorde van de dag.\n",
        "      index_dag2 = index_dag2 - 1\n",
        "\n",
        "    correlation = 1\n",
        "\n",
        "    while correlation >= c_threshold and index_dag1 <= (len(data)-1):\n",
        "      day_data = data[index_dag1]\n",
        "      Bb,Bc = np.shape(day_data)\n",
        "      day_map_af = day_data.reshape(Bb*Bc)\n",
        "      correlation = pearsonr(analogue_af, day_map_af)[0]\n",
        "      if correlation >= c_threshold:\n",
        "        list_for_length.append(1)\n",
        "        list_for_values.append(correlation)\n",
        "      index_dag1 = index_dag1 + 1\n",
        "\n",
        "\n",
        "    list_for_lists.append(list_for_length) # nu dus niet anaologue zelf meegenomen in lengte dus voor lengte nog lengte lijst +1\n",
        "    list_for_lists_values.append(list_for_values)\n",
        "\n",
        "  list_for_lengths = []\n",
        "  for lijst in list_for_lists:\n",
        "    lengte = len(lijst) + 1\n",
        "    list_for_lengths.append(lengte)\n",
        "\n",
        "  return list_for_lists, list_for_lists_values,list_for_lengths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_trend_combined(y_data_combined,running_mean_data_combined, running_mean_window, start_year, end_year, folder, name):\n",
        "  \"\"\"\n",
        "Moet wel goed checken dat length x_ticks and y_data hetzelfde is (dus 100 years, is 1850 tot en met 1949! dus ook end_year = 1949)\n",
        "  \"\"\"\n",
        "  fig, ax1 = plt.subplots(figsize = (10,8))\n",
        "  x_ticks = range(start_year, end_year + 1)\n",
        "  ax1.set_ylabel(\"1 - (ED/EDmax)\")\n",
        "  ax1.set_xlabel(\"Years\")\n",
        "\n",
        "  array_for_mean_y_data = np.array(y_data_combined)\n",
        "  mean_trend_y_data = np.mean(array_for_mean_y_data, axis=0)\n",
        "  \n",
        "  for i in range(len(y_data_combined)):\n",
        "    ax1.plot(x_ticks,y_data_combined[i], alpha = 0.3, c = \"darkturquoise\")\n",
        "    #ax1.plot(range(start_year + (running_mean_window // 2 - 1), end_year - (running_mean_window // 2 - 1)),running_mean_data_combined[i], alpha = 0.3) #begin van bram, omdat eerste paar jaar niet window kan doen\n",
        "  ax1.plot(x_ticks,mean_trend_y_data, c = \"black\",linewidth = 2)\n",
        "  plt.savefig(f\"{folder}/{name}.png\",dpi=300)\n",
        "  #plt.show()\n",
        "  plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_trend_combined_subplots(y_data_combined1850,y_data_combined1950, running_mean_window, start_year1850,start_year1950, end_year1850,end_year1950, folder, name):\n",
        "  \"\"\"\n",
        "Moet wel goed checken dat length x_ticks and y_data hetzelfde is (dus 100 years, is 1850 tot en met 1949! dus ook end_year = 1949)\n",
        "  \"\"\"\n",
        "  fig, (ax1,ax2) = plt.subplots(nrows = 1, ncols = 2,figsize = (10,4), sharey = True)\n",
        "  x_ticks1 = range(start_year1850, end_year1850 + 1)\n",
        "  x_ticks2 = range(start_year1950, end_year1950 + 1)\n",
        "  ax1.set_ylabel(\"1 - (ED/EDmax)\")\n",
        "  ax1.set_xlabel(\"Years\")\n",
        "  ax2.set_xlabel(\"Years\")\n",
        "  \n",
        "  array_for_mean_y_data1850 = np.array(y_data_combined1850)\n",
        "  mean_trend_y_data1850 = np.mean(array_for_mean_y_data1850, axis=0)\n",
        "\n",
        "  array_for_mean_y_data1950 = np.array(y_data_combined1950)\n",
        "  mean_trend_y_data1950 = np.mean(array_for_mean_y_data1950, axis=0)\n",
        "\n",
        "  for i in range(len(y_data_combined1850)):\n",
        "    ax1.plot(x_ticks1,y_data_combined1850[i], alpha = 0.3, c = \"darkturquoise\")\n",
        "    #ax1.plot(range(start_year + (running_mean_window // 2 - 1), end_year - (running_mean_window // 2 - 1)),running_mean_data_combined[i], alpha = 0.3) #begin van bram, omdat eerste paar jaar niet window kan doen\n",
        "  ax1.plot(x_ticks1,mean_trend_y_data1850, c = \"black\",linewidth = 2)\n",
        "\n",
        "  for i in range(len(y_data_combined1950)):\n",
        "    ax2.plot(x_ticks2,y_data_combined1950[i], alpha = 0.3, c = \"darkturquoise\")\n",
        "    #ax2.plot(range(start_year + (running_mean_window // 2 - 1), end_year - (running_mean_window // 2 - 1)),running_mean_data_combined[i], alpha = 0.3) #begin van bram, omdat eerste paar jaar niet window kan doen\n",
        "  ax2.plot(x_ticks2,mean_trend_y_data1950, c = \"black\",linewidth = 2)\n",
        "\n",
        "  plt.subplots_adjust(wspace=0.1) # Adjust the horizontal space between the two plot (in percentage compared to the average axis-length?)\n",
        "  plt.savefig(f\"{folder}/{name}.png\",dpi=300)\n",
        "  #plt.show()\n",
        "  plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "def violin_plot_overlay(Tevent_past,Tevent_present,Tanalogue_past,Tanalogue_present,persistence_past,persistence_present,all_t_analogue_list1850,all_t_analogue_list1950,all_lengths_list1850,all_lengths_list1950,folder,name):\n",
        "  \"\"\"\n",
        "Doet het? maar stipjes van persistence standaard op 1 want hebt nog niet van event?\n",
        "  \"\"\"\n",
        "  fig, (ax1,ax2) = plt.subplots(1,2,figsize = (8, 5))\n",
        "  for i in range(len(all_t_analogue_list1850)):\n",
        "    violins_background = ax1.violinplot([all_t_analogue_list1850[i], all_t_analogue_list1950[i]], [1, 1.6], showmeans=False, showextrema=False, showmedians=False)\n",
        "    #color_background = \"darkturquoise\" \n",
        "    colors = [\"magenta\", \"green\"]\n",
        "\n",
        "    #alpha_background = 0.3 \n",
        "    violins_background[\"bodies\"][0].set_facecolor(colors[0])\n",
        "    #violins_background[\"bodies\"][0].set_alpha(alpha_background)\n",
        "    violins_background[\"bodies\"][1].set_facecolor(colors[1])\n",
        "    #violins_background[\"bodies\"][1].set_alpha(alpha_background)\n",
        "  \n",
        "  u, p = stats.ttest_ind(Tanalogue_past, Tanalogue_present) #calculate p value\n",
        "  text_to_plot = f\"p = {p:.3f}\"\n",
        "\n",
        "  violins = ax1.violinplot([Tanalogue_past, Tanalogue_present], [1, 1.6], showmeans=False, showextrema=False, showmedians=False)\n",
        "  colors = [\"magenta\", \"green\"]\n",
        "  for pc, color in zip(violins[\"bodies\"], colors): #pc zijn de violins dus moet zo als verschillende kleuren wil\n",
        "    pc.set_facecolor(color)\n",
        "    pc.set_edgecolor('black')\n",
        "\n",
        "\n",
        "  ax1.plot(1,Tevent_past, marker = \"o\", color = \"r\") #plot de events\n",
        "  ax1.plot(1.6,Tevent_present, marker = \"o\", color = \"r\")  #plot de events\n",
        "  ax1.axhline(np.mean(Tanalogue_past), color = colors[0], linewidth = 3)\n",
        "  ax1.axhline(np.mean(Tanalogue_present), color = colors[1], linewidth = 3)\n",
        "  ax1.set_xticks([1, 1.6])\n",
        "  ax1.set_xticklabels([\"Past\", \"Present\"])\n",
        "  ax1.set_ylim(top = (1.035*(max(max(Tanalogue_past),max(Tanalogue_present))))) # to ensure that the plotted p-value and graph do not overlap\n",
        "  ax1.text(0.035,0.95,text_to_plot,transform=ax1.transAxes)\n",
        "\n",
        "  for i in range(len(all_lengths_list1850)):\n",
        "    violins_background2 = ax2.violinplot([all_lengths_list1850[i], all_lengths_list1950[i]], [1, 1.6], showmeans=False, showextrema=False, showmedians=False)\n",
        "    #color_background2 = \"darkturquoise\" \n",
        "    #alpha_background2 = 0.3 \n",
        "    violins_background2[\"bodies\"][0].set_facecolor(colors[0])\n",
        "    #violins_background2[\"bodies\"][0].set_alpha(alpha_background2)\n",
        "    violins_background2[\"bodies\"][1].set_facecolor(colors[1])\n",
        "    #violins_background2[\"bodies\"][1].set_alpha(alpha_background2)\n",
        "\n",
        "\n",
        "  u, p = stats.ttest_ind(persistence_past, persistence_present) #calculate p value\n",
        "  text_to_plot = f\"p = {p:.3f}\"\n",
        "\n",
        "  violins = ax2.violinplot([persistence_past, persistence_present], [1, 1.6], showmeans=False, showextrema=False, showmedians=False)\n",
        "  colors = [\"magenta\", \"green\"]\n",
        "  for pc, color in zip(violins[\"bodies\"], colors): #pc zijn de violins dus moet zo als verschillende kleuren wil\n",
        "    pc.set_facecolor(color)\n",
        "    pc.set_edgecolor('black')\n",
        "    #pc.set_alpha(1)\n",
        "  \n",
        "  ax2.plot(1,1, marker = \"o\", color = \"r\") #plot de events\n",
        "  ax2.plot(1.6,1, marker = \"o\", color = \"r\")  #plot de events\n",
        "  ax2.axhline(np.mean(persistence_past), color = colors[0], linewidth = 3)\n",
        "  ax2.axhline(np.mean(persistence_present), color = colors[1], linewidth = 3)\n",
        "  ax2.set_xticks([1, 1.6])\n",
        "  ax2.set_xticklabels([\"Past\", \"Present\"])\n",
        "  ax2.set_ylim(top = (1.07*(max(max(persistence_past),max(persistence_present))))) # to ensure that the plotted p-value and graph do not overlap\n",
        "  ax2.text(0.035,0.95,text_to_plot,transform=ax2.transAxes)\n",
        "\n",
        "  plt.savefig(f\"{folder}/{name}.png\",dpi=300)\n",
        "  plt.close()\n",
        "  #plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "def lists_for_model_dates(amount_of_years):\n",
        "    \"\"\"\n",
        "    Creates lists with all months and years in the complete model data (for 1950-2014) (amount_of_years 65 of 100)\n",
        "    \"\"\"\n",
        "    month_list = []\n",
        "    year_list = []\n",
        "    \n",
        "    if amount_of_years == 65:\n",
        "        year = 1950\n",
        "    elif amount_of_years == 100:\n",
        "        year = 1850\n",
        "    else:\n",
        "        print(\"WARNING: not a proper amount of years is given (zelf)\")\n",
        "    for j in range(amount_of_years):\n",
        "        month = 1\n",
        "        for m in range(12):\n",
        "            for d in range(30):\n",
        "                year_list.append(year)\n",
        "                month_list.append(month)\n",
        "            month = month + 1     \n",
        "        year = year + 1\n",
        "\n",
        "    return month_list, year_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_years_and_months_era5(era5_data,desired_start_month,desired_end_month,desired_start_year,desired_end_year,list_with_all_months,list_with_all_years):\n",
        "    \"\"\"\n",
        "    Slice era5 data based on the months and years, months and years that are used as input variable will be included as well\n",
        "    \"\"\"\n",
        "    if era5_data.shape[0] != len(list_with_all_months):\n",
        "        print (\"Error: Amount of days in the data and list with all dates are not the same\")\n",
        "    \n",
        "    list_for_filtered_era5_data = []\n",
        "    list_for_filtered_years = []\n",
        "    list_for_filtered_months = []\n",
        "    for i in range(era5_data.shape[0]):\n",
        "        month_at_index = list_with_all_months[i]\n",
        "        year_at_index = list_with_all_years[i]\n",
        "        if month_at_index >= desired_start_month and month_at_index <= desired_end_month and year_at_index >= desired_start_year and year_at_index <= desired_end_year:\n",
        "            data_to_select = era5_data[i,:,:]\n",
        "            list_for_filtered_era5_data.append(data_to_select)\n",
        "            list_for_filtered_years.append(year_at_index)\n",
        "            list_for_filtered_months.append(month_at_index)\n",
        "    array_selected_era5_data = np.array(list_for_filtered_era5_data)\n",
        "\n",
        "    return array_selected_era5_data, list_for_filtered_years, list_for_filtered_months"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "def prep_streamfunction(data):\n",
        "    \"\"\"\n",
        "    doen nadat bijgesneden\n",
        "    \"\"\"\n",
        "    list_for_new_data = []\n",
        "    for i in range(data.shape[0]):\n",
        "        data_day = data[i,:,:]\n",
        "        day_mean = np.mean(data_day)\n",
        "        new_data = data_day - day_mean\n",
        "        list_for_new_data.append(new_data)\n",
        "    new_array = np.array(list_for_new_data)\n",
        "    \n",
        "    return new_array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "def violin_plot2014(Tevent_past,Tevent_present,Tanalogue_past,Tanalogue_present,persistence_past,persistence_present,persistence_event,e):\n",
        "  \"\"\"\n",
        "Doet het? maar stipjes van persistence standaard op 1 want hebt nog niet van event?\n",
        "  \"\"\"\n",
        "  u, p = stats.ttest_ind(Tanalogue_past, Tanalogue_present) #calculate p value\n",
        "  text_to_plot = f\"p = {p:.3f}\"\n",
        "\n",
        "  fig, (ax1,ax2) = plt.subplots(1,2,figsize = (7, 4))\n",
        "  violins = ax1.violinplot([Tanalogue_past, Tanalogue_present], [1, 1.6], showmeans=False, showextrema=False, showmedians=False)\n",
        "  colors = [\"magenta\", \"green\"]\n",
        "  for pc, color in zip(violins[\"bodies\"], colors): #pc zijn de violins dus moet zo als verschillende kleuren wil\n",
        "    pc.set_facecolor(color)\n",
        "  ax1.axhline(np.mean(Tanalogue_past), color = colors[0], linewidth = 3)\n",
        "  ax1.axhline(np.mean(Tanalogue_present), color = colors[1], linewidth = 3)\n",
        "  ax1.plot(1,Tevent_past, marker = \"o\", color = \"r\") #plot de events\n",
        "  ax1.plot(1.6,Tevent_present, marker = \"o\", color = \"r\")  #plot de events\n",
        "  ax1.set_xticks([1, 1.6])\n",
        "  ax1.set_xticklabels([\"Past\", \"Present\"])\n",
        "  ax1.set_ylim(bottom=1.5e-10,top = 3.0e-10) # to ensure that the plotted p-value and graph do not overlap\n",
        "  ax1.text(0.035,0.95,text_to_plot,transform=ax1.transAxes)\n",
        "  ax1.set_ylabel(\"Typicality (x$10^{-10}$)\")\n",
        "  ax1.set_title(\"(A)\",loc= \"center\",fontsize = 11)\n",
        "  u, p = stats.ttest_ind(persistence_past, persistence_present) #calculate p value\n",
        "  text_to_plot = f\"p = {p:.3f}\"\n",
        "\n",
        "\n",
        "  violins = ax2.violinplot([persistence_past, persistence_present], [1, 1.6], showmeans=False, showextrema=False, showmedians=False)\n",
        "  colors = [\"magenta\", \"green\"]\n",
        "  for pc, color in zip(violins[\"bodies\"], colors): #pc zijn de violins dus moet zo als verschillende kleuren wil\n",
        "    pc.set_facecolor(color)\n",
        "  ax2.axhline(np.mean(persistence_past), color = colors[0], linewidth = 3)\n",
        "  ax2.axhline(np.mean(persistence_present), color = colors[1], linewidth = 3)\n",
        "  ax2.plot(1,persistence_event, marker = \"o\", color = \"r\") #plot de events\n",
        "  ax2.plot(1.6,persistence_event, marker = \"o\", color = \"r\")  #plot de events\n",
        "  ax2.set_xticks([1, 1.6])\n",
        "  ax2.set_xticklabels([\"Past\", \"Present\"])\n",
        "  ax2.set_ylim(top = 11) # to ensure that the plotted p-value and graph do not overlap\n",
        "  ax2.text(0.035,0.95,text_to_plot,transform=ax2.transAxes)\n",
        "  ax2.set_ylabel(\"Persistence (days)\")\n",
        "  ax2.set_title(\"(B)\",loc= \"center\",fontsize = 11)\n",
        "\n",
        "  fig.suptitle(f\"HadGEM3 (r{e})\")\n",
        "  plt.subplots_adjust(wspace=0.25)  # Change the value as needed\n",
        "  #plt.savefig(f\"/usr/people/noest/stage_folders/outputs/net/serious_run2/violin/violin_had{e}.png\",dpi=300)\n",
        "  plt.show()\n",
        "  plt.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "def intensity_plot(lat,lon,past,present,difference,sigmask,naam):\n",
        "    fig = plt.figure(figsize = (14,3))\n",
        "    gs = GridSpec(1, 6, figure=fig, height_ratios=[1], width_ratios=[1, 1,1,1,1,1])\n",
        "    #ax = plt.axes(projection = ccrs.PlateCarree())\n",
        "\n",
        "    levels1 = np.linspace(-1.75*1e7,1.75*1e7,15)\n",
        "    levels2 = np.linspace(-3*1e6,3*1e6,13)\n",
        "    # ax1 = fig.add_subplot(gs[0, 1:3],projection=ccrs.PlateCarree())\n",
        "    # contour1 = ax1.contourf(lon, lat, event_box[0], cmap = \"RdBu_r\", transform = ccrs.PlateCarree(), levels = levels1,extend = \"both\") #levels=np.linspace(-8.2e7, 1e7, 10), extend='both\n",
        "    # ax1.coastlines()\n",
        "    # ax1.add_feature(cf.BORDERS)\n",
        "    # ax1.set_title(\"A) Event\",fontsize = 10)\n",
        "    #plt.colorbar(ax1, ax=ax1, orientation = \"horizontal\", label = \"Pressure (Pa)\", pad = 0.05)\n",
        "\n",
        "    ax2 = fig.add_subplot(gs[0, 0:2],projection=ccrs.PlateCarree())\n",
        "    contour1 = ax2.contourf(lon, lat, past, cmap = \"RdBu_r\", transform = ccrs.PlateCarree(), levels = levels1,extend = \"both\") #levels=np.linspace(-8.2e7, 1e7, 10), extend='both\n",
        "    ax2.coastlines()\n",
        "    ax2.add_feature(cf.BORDERS)\n",
        "    ax2.set_title(\"B) Past\",fontsize = 10)\n",
        "\n",
        "    ax3 = fig.add_subplot(gs[0, 2:4],projection=ccrs.PlateCarree())\n",
        "    contour1 = ax3.contourf(lon, lat, present, cmap = \"RdBu_r\", transform = ccrs.PlateCarree(), levels = levels1,extend = \"both\") #levels=np.linspace(-8.2e7, 1e7, 10), extend='both\n",
        "    ax3.coastlines()\n",
        "    ax3.add_feature(cf.BORDERS)\n",
        "    ax3.set_title(\"C) Present\",fontsize = 10)\n",
        "\n",
        "    ax4 = fig.add_subplot(gs[0, 4:],projection=ccrs.PlateCarree())\n",
        "    contour2 = ax4.contourf(lon, lat, difference, cmap = \"PiYG\", transform = ccrs.PlateCarree(), levels = levels2, extend = \"both\") #levels=np.linspace(-8.2e7, 1e7, 10), extend='both\n",
        "    ax4.contourf( lon, lat,sigmask, levels=[-0.5,0.5,1.5], hatches=[None, '////'], colors='none', transform=ccrs.PlateCarree())\n",
        "    ax4.coastlines()\n",
        "    ax4.add_feature(cf.BORDERS)\n",
        "    ax4.set_title(\"D) Difference\",fontsize = 10)\n",
        "\n",
        "    title = f\"HadGEM3 (r{naam})\"\n",
        "    fig.text(0.1, 0.5, title, va='center', ha='center', rotation='vertical', fontsize=12)\n",
        "\n",
        "\n",
        "    #cax1 = fig.add_subplot(gs[2,1:3])\n",
        "    #plt.colorbar(contour1, cax=cax1, orientation=\"horizontal\", label=\"500 hPa Streamfunction\\n(x$10^7$ m$^2$/s)\")\n",
        "    # formatter = ScalarFormatter(useMathText=True)\n",
        "    # formatter.set_powerlimits((0, 0))  # Set the exponent limits\n",
        "    # cax1.yaxis.set_major_formatter(formatter)\n",
        "\n",
        "    #cax2 = fig.add_subplot(gs[2,4:])\n",
        "    #plt.colorbar(contour2, cax=cax2, orientation=\"horizontal\", label=\"Difference in 500 hPa streamfunction\\n(x$10^6$ m$^2$/s)\")\n",
        "    #plt.savefig(f\"/usr/people/noest/stage_folders/outputs/net/serious_run2/intensity/int_had{naam}.png\",dpi=300)\n",
        "    #plt.subplots_adjust(hspace=0.9)  # Change the value as needed\n",
        "    plt.show()\n",
        "    plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **AMO**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_amo_region_and_time_and_mean(amo_lat,amo_lon,amo_var, bbox,start_month_amo,end_month_amo):\n",
        "    \"\"\"\n",
        "    Extract mean of a sub-set of a cube inside a lon, lat bounding box\n",
        "    bbox=[lon_min lon_max lat_min lat_max].\n",
        "    NOTE: This is a work around too subset an iris cube that has\n",
        "    2D lon, lat coords. zelf aangepast zonder iris.\n",
        "    gehardcode dat jaar 12 dingen duurt en dat alle jaren wilt\n",
        "    \"\"\"\n",
        "    #vikki en zelf aangepast dat geen iris:\n",
        "    minmax = lambda x: (np.min(x), np.max(x))\n",
        "    inregion = np.logical_and(np.logical_and(amo_lon > bbox[0],\n",
        "                                             amo_lon < bbox[1]),\n",
        "                              np.logical_and(amo_lat > bbox[2],\n",
        "                                             amo_lat < bbox[3]))\n",
        "    region_inds = np.where(inregion)\n",
        "    imin, imax = minmax(region_inds[0])\n",
        "    jmin, jmax = minmax(region_inds[1])\n",
        "    subcube_var = amo_var[..., imin:imax+1, jmin:jmax+1]\n",
        "    subcube_lat = amo_lat[..., imin:imax+1, jmin:jmax+1]\n",
        "    subcube_lon = amo_lon[..., imin:imax+1, jmin:jmax+1]\n",
        "    \n",
        "    #bram:\n",
        "    years_amo = subcube_var.shape[0] // 12\n",
        "    var_time_filtered = subcube_var.reshape(years_amo, 12, subcube_var.shape[1], subcube_var.shape[2])[:, start_month_amo - 1: end_month_amo, :, :]\n",
        "    amo_mean_state = np.mean(var_time_filtered, axis = (1, 2, 3))\n",
        "    #x = subcube.collapsed(['cell index along second dimension','cell index along first dimension'],iris.analysis.MEAN)\n",
        "    #return var_time_filtered,subcube_lat,subcube_lon\n",
        "    return amo_mean_state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "def AMO_analysis(AMO_data_path_past,AMO_data_path_present,start_month_amo,end_month_amo):\n",
        "    \"\"\"\n",
        "    Analysis the AMO data, for now it is hardcoded that you load the tos/lat/lon variables.\n",
        "    bbox=[lon_min lon_max lat_min lat_max].\n",
        "    nog gehardcode extract_amo_region_and_time dat jaar 12 maanden duurt en dat alle jaren wilt\n",
        "\n",
        "    \"\"\"\n",
        "    amo_lat_past,amo_lon_past,amo_tos_past = load_data(AMO_data_path_past,\"latitude\",\"longitude\",\"tos\")\n",
        "    amo_lat_present,amo_lon_present,amo_tos_present = load_data(AMO_data_path_present,\"latitude\",\"longitude\",\"tos\")\n",
        "\n",
        "    NA_past_mean = extract_amo_region_and_time_and_mean(amo_lat_past,amo_lon_past,amo_tos_past, [-80, 0, 0, 60],start_month_amo,end_month_amo)\n",
        "    NA_present_mean = extract_amo_region_and_time_and_mean(amo_lat_present,amo_lon_present,amo_tos_present, [-80, 0, 0, 60],start_month_amo,end_month_amo)\n",
        "\n",
        "    global_past_mean = extract_amo_region_and_time_and_mean(amo_lat_past,amo_lon_past,amo_tos_past, [-180, 180, -60, 60],start_month_amo,end_month_amo)\n",
        "    global_present_mean = extract_amo_region_and_time_and_mean(amo_lat_present,amo_lon_present,amo_tos_present, [-180, 180, -60, 60],start_month_amo,end_month_amo)\n",
        "\n",
        "    AMO_value_past = NA_past_mean - global_past_mean #should be an array\n",
        "    AMO_value_present = NA_present_mean - global_present_mean\n",
        "    #AMO_value_present =  global_present_mean - NA_present_mean #test\n",
        "\n",
        "    return AMO_value_past, AMO_value_present"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "def amo_bram_plot_test(yearlyEuclidianDistance, runningMean, amoData,running_mean_window,folder,name):\n",
        "    fig, (ax1, ax2) = plt.subplots(nrows = 1, ncols = 2, figsize = (14, 6))\n",
        "    slope, intercept, r2, _, _ = scipy.stats.linregress(yearlyEuclidianDistance, amoData)\n",
        "    xData = np.linspace(np.min(yearlyEuclidianDistance), np.max(yearlyEuclidianDistance), yearlyEuclidianDistance.shape[0])\n",
        "    yData = [slope * x + intercept for x in xData]\n",
        "    ax1.scatter(yearlyEuclidianDistance, amoData)\n",
        "    ax1.plot(xData, yData)\n",
        "    ax1.text(0.05, 0.95, f\"r-value = {r2:.3f}\", transform = ax1.transAxes, bbox = dict(facecolor = \"None\", edgecolor = \"k\"))\n",
        "    ax1.set_xlabel(\"Euclidian distance\")\n",
        "    ax1.set_ylabel(\"AMO\")\n",
        "    ax1.set_title(\"AMO\")\n",
        "\n",
        "    running_mean_amo = np.convolve(amoData, np.ones(running_mean_window) / running_mean_window, mode = \"valid\")    \n",
        "    slope, intercept, r2, _, _ = scipy.stats.linregress(runningMean, running_mean_amo)\n",
        "    xData = np.linspace(np.min(runningMean), np.max(runningMean), runningMean.shape[0])\n",
        "    yData = [slope * x + intercept for x in xData]\n",
        "    ax2.scatter(runningMean, running_mean_amo)\n",
        "    ax2.plot(xData, yData)\n",
        "    ax2.text(0.05, 0.95, f\"r-value = {r2:.3f}\", transform = ax2.transAxes, bbox = dict(facecolor = \"None\", edgecolor = \"k\"))\n",
        "    ax2.set_xlabel(\"Euclidian distance\")\n",
        "    ax2.set_ylabel(\"AMO\")\n",
        "    ax2.set_title(\"10-yr AMO\")\n",
        "    plt.savefig(f\"{folder}/{name}.png\",dpi=300)\n",
        "    plt.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xgVoC3HhzAeW"
      },
      "source": [
        "# **Combined**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "2v1j6B2gzFRN"
      },
      "outputs": [],
      "source": [
        "def analogues_combined(base_path,name_run,data_path,event_path,S,N,W,E,yearlength_original,start_month,end_month,start_year1850,end_year1850,start_year1950,end_year1950,target_number_of_analogues,analogue_seperation_range,c_threshold,yearlength_filtered,running_mean_window,start_x_axis_trend1850,end_x_axis_trend1850,start_x_axis_trend1950,end_x_axis_trend1950,amount_of_analogues_to_plot,p_value_difference_maps,amo_data_folder,start_month_amo,end_month_amo,save_violin_plot = False,save_trend_plot = False, amount_of_ensembles_to_run = 55,save_analogue_plot = False,save_differences_maps = False,save_all_data = False,save_amo_plot = False):\n",
        "  \"\"\"\n",
        "Combine all functions into a single function. For now, assumed that the number of analogues and analogue seperation range are the same for the original analogue and the analogue analogues in the typicallity.\n",
        "Function is also made for the model data. If ERA5 data is used, the event itself still needs to be excluded from the analogues.\n",
        "  \"\"\"\n",
        "  #Create paths\n",
        "  new_base_path,output_folder_figures,output_folder_lists,violin_plot_path,analogue_plot_path,difference_plot_path,trend_plot_path,AMO_plot_path = create_directories(base_path,name_run,extra_folder1 = \"figures\",extra_folder2 = \"lists\")\n",
        "\n",
        "  #Make a list to save all the sliced data for the combined violin plot (so you can look for analogues in ALL data)\n",
        "  list_for_all_selected_data_1850 = []\n",
        "  list_for_all_selected_data_1950 = []\n",
        "\n",
        "  #Make lists so save the past and present analogues for the difference ensembles as well as their euclidian distances\n",
        "  list_for_combined_difference_1850 = [] # Is a list of lists, with in each inner list all the analogue fields for an ensemble member\n",
        "  list_for_combined_difference_1950 = []\n",
        "  list_for_all_analogues_distances_1850 = []\n",
        "  list_for_all_analogues_distances_1950 = []\n",
        "\n",
        "  #Make lists for the OLD difference plot using the mean of ALL analogues (rather than the combined best ones) for all ensemble members\n",
        "  list_for_old_difference_plot_1850 = []\n",
        "  list_for_old_difference_plot_1950 = []\n",
        "\n",
        "  #Make lists for the combined trend plot\n",
        "  list_for_y_data_combined_trend_plot1850 = []\n",
        "  list_for_running_mean_combined_trend_plot1850 = []\n",
        "  list_for_y_data_combined_trend_plot1950 = []\n",
        "  list_for_running_mean_combined_trend_plot1950 = []\n",
        "\n",
        "  #Make lists for the combined violin plot\n",
        "  list_for_violin_plot_t_analogues1850 = []\n",
        "  list_for_violin_plot_t_analogues1950 = []\n",
        "  list_for_violin_plot_lengths1850 = []\n",
        "  list_for_violin_plot_lengths1950 = []\n",
        "\n",
        "  #Make lists for combined months and years list for the new combined violin plot\n",
        "  list_with_combined_years_p1 = []\n",
        "  list_with_combined_months_p1 = []\n",
        "  list_with_combined_years_p2 = []\n",
        "  list_with_combined_months_p2 = []\n",
        "\n",
        "\n",
        "  #Make lists with all months and years for both periods\n",
        "  #month_list_p1,year_list_p1 = lists_for_model_dates(100)\n",
        "  month_list_p2,year_list_p2 = lists_for_model_dates(65)\n",
        "\n",
        "  #Prepare ensemble members\n",
        "  ensemble_member_paths = sorted(os.listdir(data_path))\n",
        "  index_1950 = 0\n",
        "  max_index = amount_of_ensembles_to_run-1\n",
        "\n",
        "  # Loop through ensemble members and determine paths\n",
        "  while index_1950 <= max_index:\n",
        "    file1950 = ensemble_member_paths[index_1950]\n",
        "    data_path1950 = os.path.join(data_path, file1950)\n",
        "    print (file1950)\n",
        "    #Load data\n",
        "    lat1950,lon1950,psl1950_levels = load_data(data_path1950,\"lat\",\"lon\",\"stream\")\n",
        "    psl1950 = np.squeeze(psl1950_levels)\n",
        "    #Load and extract area for event\n",
        "    msl_event = np.load(event_path)\n",
        "    lat_event_bounds,lon_event_bounds,msl_event_bounds_og = extract_area(S, N, W, E,lat1950,lon1950,msl_event,event = True)\n",
        "    msl_event_bounds = prep_streamfunction(msl_event_bounds_og)\n",
        "\n",
        "    #Extract area\n",
        "    lat1950_bounds,lon1950_bounds,psl1950_bounds_og = extract_area(S, N, W, E,lat1950,lon1950,psl1950,event = False)\n",
        "    psl1950_bounds = prep_streamfunction(psl1950_bounds_og)\n",
        "\n",
        "    #Extract season nieuw #nog kijken waar season voor gebruikt wordt\n",
        "    psl1850_semi, list_semi_filterd_years_p1,list_semi_filterd_months_p1 = extract_years_and_months_era5(psl1950_bounds,1,12,start_year1850,end_year1850,month_list_p2,year_list_p2)\n",
        "    psl1850_season, list_season_filterd_years_p1,list_season_filterd_months_p1 = extract_years_and_months_era5(psl1950_bounds,start_month,end_month,start_year1850,end_year1850,month_list_p2,year_list_p2)\n",
        "\n",
        "    psl1950_semi, list_semi_filterd_years_p2,list_semi_filterd_months_p2 = extract_years_and_months_era5(psl1950_bounds,1,12,start_year1950,end_year1950,month_list_p2,year_list_p2)\n",
        "    psl1950_season, list_season_filterd_years_p2,list_season_filterd_months_p2 = extract_years_and_months_era5(psl1950_bounds,start_month,end_month,start_year1950,end_year1950,month_list_p2,year_list_p2)\n",
        "\n",
        "    if save_all_data == True: #nog kijken\n",
        "      list_for_all_selected_data_1850.extend(psl1850_semi)\n",
        "      list_for_all_selected_data_1950.extend(psl1950_semi)\n",
        "      list_with_combined_years_p1.extend(list_semi_filterd_years_p1)\n",
        "      list_with_combined_months_p1.extend(list_semi_filterd_months_p1)\n",
        "      list_with_combined_years_p2.extend(list_semi_filterd_years_p2)\n",
        "      list_with_combined_months_p2.extend(list_semi_filterd_months_p2)\n",
        "\n",
        "    #Calculate euclidian distance\n",
        "    euclidian_distance1850 = euclidian_distance(psl1850_semi, msl_event_bounds)\n",
        "    euclidian_distance1950 = euclidian_distance(psl1950_semi, msl_event_bounds)\n",
        "\n",
        "    #Find analogues *nog kijken voor zekerheid waar index gebruikt word en dat met semi gebruikt word\n",
        "    analogues_indexes1850, analogues_distances1850,analogues_years1850 = determine_analogues_era5(euclidian_distance1850, target_number_of_analogues,analogue_seperation_range,list_semi_filterd_months_p1,list_semi_filterd_years_p1,start_month,end_month,event_is_in_data=False)\n",
        "    analogues_indexes1950, analogues_distances1950,analogues_years1950 = determine_analogues_era5(euclidian_distance1950, target_number_of_analogues,analogue_seperation_range,list_semi_filterd_months_p2,list_semi_filterd_years_p2,start_month,end_month,event_is_in_data=False)\n",
        "\n",
        "    #nog kijken\n",
        "    list_for_all_analogues_distances_1850.append(analogues_distances1850)\n",
        "    list_for_all_analogues_distances_1950.append(analogues_distances1950)\n",
        "\n",
        "    #Determine persistence\n",
        "    persistence_days1850, persistence_correlations1850,lengths1850 = persistence(analogues_indexes1850,psl1850_semi,c_threshold)\n",
        "    persistence_days1950, persistence_correlations1950,lengths1950 = persistence(analogues_indexes1950,psl1950_semi,c_threshold)\n",
        "\n",
        "    #Calculate the mean difference per ensemble and the mean for all ensembles (goed)\n",
        "    difference_per_ensemble, mean_array_1850, mean_array_1950,list_for_1850, list_for_1950 = mean_difference_per_ensemble(analogues_indexes1850,psl1850_semi,analogues_indexes1950,psl1950_semi)\n",
        "    list_for_combined_difference_1850.append(list_for_1850)\n",
        "    list_for_combined_difference_1950.append(list_for_1950)\n",
        "    list_for_old_difference_plot_1850.append(mean_array_1850)\n",
        "    list_for_old_difference_plot_1950.append(mean_array_1950)\n",
        "\n",
        "    #Define the name to identify ensemble members in the saved output\n",
        "    ensemble_number_name = (index_1950) + 1\n",
        "\n",
        "    #Plot violin plot\n",
        "    if save_violin_plot == True:\n",
        "\n",
        "      #Calculate typicallity (nog kijken naar wat append)\n",
        "      Tevent1850, Tanalogues1850 = typicality(analogues_distances1850, analogues_indexes1850,psl1850_semi,target_number_of_analogues,analogue_seperation_range,list_semi_filterd_months_p1,list_semi_filterd_years_p1,start_month,end_month)\n",
        "      Tevent1950, Tanalogues1950 = typicality(analogues_distances1950, analogues_indexes1950,psl1950_semi,target_number_of_analogues,analogue_seperation_range,list_semi_filterd_months_p2,list_semi_filterd_years_p2,start_month,end_month)\n",
        "\n",
        "      list_for_violin_plot_t_analogues1850.append(Tanalogues1850)\n",
        "      list_for_violin_plot_t_analogues1950.append(Tanalogues1950)\n",
        "      list_for_violin_plot_lengths1850.append(lengths1850)\n",
        "      list_for_violin_plot_lengths1950.append(lengths1950)\n",
        "\n",
        "      #Save plot\n",
        "      violin_name = f\"violin_plot_ensemble_number_{ensemble_number_name}\"\n",
        "      violin_plot(Tevent1850,Tevent1950,Tanalogues1850,Tanalogues1950,lengths1850,lengths1950,violin_plot_path,violin_name)\n",
        "      violin_plot2014(Tevent1850,Tevent1950,Tanalogues1850,Tanalogues1950,lengths1850,lengths1950,1,ensemble_number_name)\n",
        "    #Plot trends\n",
        "    if save_trend_plot == True:\n",
        "\n",
        "      #Calculate yearly minimum distance for trend analysis (needs seasonal euclidian distances)\n",
        "      euclidian_distance1850_season = euclidian_distance(psl1850_season, msl_event_bounds)\n",
        "      euclidian_distance1950_season = euclidian_distance(psl1950_season, msl_event_bounds)\n",
        "      yearly_min_eucldidian_distance1850 = yearly_minimum_euclidian_distance(euclidian_distance1850_season, yearlength = yearlength_filtered)\n",
        "      yearly_min_eucldidian_distance1950 = yearly_minimum_euclidian_distance(euclidian_distance1950_season, yearlength = yearlength_filtered)\n",
        "\n",
        "      #Save plot\n",
        "      trend_name_1850 = f\"trend_plot_1850_ensemble_number_{ensemble_number_name}\"\n",
        "      trend_name_1950 = f\"trend_plot_1950_ensemble_number_{ensemble_number_name}\"\n",
        "      y_data_to_save1850, running_mean_to_save1850 = plot_trend(yearly_min_eucldidian_distance1850,euclidian_distance1850, running_mean_window, start_x_axis_trend1850, end_x_axis_trend1850,trend_plot_path,trend_name_1850)\n",
        "      y_data_to_save1950, running_mean_to_save1950 = plot_trend(yearly_min_eucldidian_distance1950,euclidian_distance1950, running_mean_window, start_x_axis_trend1950, end_x_axis_trend1950,trend_plot_path,trend_name_1950)\n",
        "\n",
        "      list_for_y_data_combined_trend_plot1850.append(y_data_to_save1850)\n",
        "      list_for_running_mean_combined_trend_plot1850.append(running_mean_to_save1850)\n",
        "      list_for_y_data_combined_trend_plot1950.append(y_data_to_save1950)\n",
        "      list_for_running_mean_combined_trend_plot1950.append(running_mean_to_save1950)\n",
        "\n",
        "    if save_analogue_plot == True:\n",
        "      for i in range(amount_of_analogues_to_plot):\n",
        "        index_plot_1850 = analogues_indexes1850[i]\n",
        "        index_plot_1950 = analogues_indexes1950[i]\n",
        "        analogue_plot_name_1850 = f\"analogue_plot_1850_ensemble_number_{ensemble_number_name}_analogue_rank_{i+1}\" # So rank 1 is the best analogue\n",
        "        analogue_plot_name_1950 = f\"analogue_plot_1950_ensemble_number_{ensemble_number_name}_analogue_rank_{i+1}\" # So rank 1 is the best analogue\n",
        "        plot_variable(lat1950_bounds,lon1950_bounds,psl1850_semi[index_plot_1850],analogue_plot_path,analogue_plot_name_1850)\n",
        "        plot_variable(lat1950_bounds,lon1950_bounds,psl1950_semi[index_plot_1950],analogue_plot_path,analogue_plot_name_1950)\n",
        "\n",
        "    if save_differences_maps == True:\n",
        "      significance_mask = t_test(list_for_1850,list_for_1950,p_value_difference_maps)\n",
        "      difference_map_name = f\"difference_plot_ensemble_number_{ensemble_number_name}\"\n",
        "      plot_difference(lat1950_bounds,lon1950_bounds,difference_per_ensemble,significance_mask,difference_plot_path,difference_map_name)\n",
        "      intensity_plot(lat1950_bounds,lon1950_bounds,mean_array_1850,mean_array_1950,difference_per_ensemble,significance_mask,ensemble_number_name)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    if save_amo_plot == True:\n",
        "      amo_data_paths = sorted(os.listdir(amo_data_folder))\n",
        "      amo_file1850 = amo_data_paths[index_1950]\n",
        "      amo_file1950 = amo_data_paths[index_1950]\n",
        "      amo_data_path1850 = os.path.join(amo_data_folder, amo_file1850)\n",
        "      amo_data_path1950 = os.path.join(amo_data_folder, amo_file1950)\n",
        "      amo_values_1850, amo_values_1950 = amo_data_test1, amo_data_test2= AMO_analysis(amo_data_path1850,amo_data_path1950,start_month_amo,end_month_amo)\n",
        "      amo_1850_name = f\"amo_plot_1850_ensemble_number_{ensemble_number_name}\"\n",
        "      amo_1950_name = f\"amo_plot_1950_ensemble_number_{ensemble_number_name}\"\n",
        "\n",
        "      amo_bram_plot_test(y_data_to_save1850, running_mean_to_save1850, amo_values_1850,running_mean_window,AMO_plot_path,amo_1850_name)\n",
        "      amo_bram_plot_test(y_data_to_save1950, running_mean_to_save1950, amo_values_1950,running_mean_window,AMO_plot_path,amo_1950_name)\n",
        "\n",
        "    \n",
        "    #Increase index:\n",
        "    index_1950 = index_1950 + 1\n",
        "\n",
        "  #Outside of loop again:\n",
        "  \n",
        "\n",
        "\n",
        "  #This is plotting the difference of all ensembles combined by using the mean from all analogues from all ensemble members and avereging those again, does not contain significance yet so therefore also uses plot_variable instead of plot_difference\n",
        "  array_means_1850 = np.array(list_for_old_difference_plot_1850) #maak van de list weer een array\n",
        "  ensembles_combined_mean_array_1850_old = np.mean(array_means_1850, axis = 0)\n",
        "  array_means_1950 = np.array(list_for_old_difference_plot_1950) #maak van de list weer een array\n",
        "  ensembles_combined_mean_array_1950_old = np.mean(array_means_1950, axis = 0)\n",
        "  combined_difference_old = ensembles_combined_mean_array_1950_old - ensembles_combined_mean_array_1850_old\n",
        "  if save_differences_maps == True:\n",
        "    combined_difference_map_name_old = \"difference_plot_all_ensembles_combined_old\"\n",
        "    plot_variable(lat1950_bounds,lon1950_bounds,combined_difference_old,difference_plot_path,combined_difference_map_name_old)\n",
        "\n",
        "  #This plots the difference maps using only the TOP analogues of all ensemble members combined (goed)\n",
        "  ensembles_combined_mean_array1850_new, best_combined_analogue_fields1850, best_combined_distances_list1850 = combined_difference_top_analogues(list_for_combined_difference_1850,list_for_all_analogues_distances_1850,target_number_of_analogues)\n",
        "  ensembles_combined_mean_array1950_new, best_combined_analogue_fields1950, best_combined_distances_list1950 = combined_difference_top_analogues(list_for_combined_difference_1950,list_for_all_analogues_distances_1950,target_number_of_analogues)\n",
        "  combined_difference_new = ensembles_combined_mean_array1950_new - ensembles_combined_mean_array1850_new\n",
        "  significance_mask_combined = t_test(best_combined_analogue_fields1850,best_combined_analogue_fields1950,p_value_difference_maps)\n",
        "\n",
        "  if save_differences_maps == True:\n",
        "    combined_difference_map_name_new = \"difference_plot_all_ensembles_combined_new\"\n",
        "    plot_difference(lat1950_bounds,lon1950_bounds,combined_difference_new,significance_mask_combined,difference_plot_path,combined_difference_map_name_new)\n",
        "  \n",
        "  #Create the combined violin plot and the necessary combined data\n",
        "  if save_all_data == True:\n",
        "    all_selected_data_1850_array = np.array(list_for_all_selected_data_1850)\n",
        "    all_selected_data_1950_array = np.array(list_for_all_selected_data_1950)\n",
        "    Tevent1850_combined, Tanalogues1850_combined = typicality_combined_new(best_combined_distances_list1850,best_combined_analogue_fields1850,all_selected_data_1850_array,target_number_of_analogues,analogue_seperation_range,list_with_combined_months_p1,list_with_combined_years_p1,start_month,end_month)\n",
        "    Tevent1950_combined, Tanalogues1950_combined = typicality_combined_new(best_combined_distances_list1950,best_combined_analogue_fields1950,all_selected_data_1950_array,target_number_of_analogues,analogue_seperation_range,list_with_combined_months_p2,list_with_combined_years_p2,start_month,end_month)\n",
        "    persistence_days1850_combined, persistence_correlations1850_combined,lengths1850_combined = persistence_combined_new(best_combined_analogue_fields1850,all_selected_data_1850_array,c_threshold)\n",
        "    persistence_days1950_combined, persistence_correlations1950_combined,lengths1950_combined = persistence_combined_new(best_combined_analogue_fields1950,all_selected_data_1950_array,c_threshold)\n",
        "    violin_name_combined = f\"violin_plot_combined\"\n",
        "    violin_name_combined_overlay = f\"violin_plot_combined_overlay\"\n",
        "    violin_plot(Tevent1850_combined,Tevent1950_combined,Tanalogues1850_combined,Tanalogues1950_combined,lengths1850_combined,lengths1950_combined,violin_plot_path,violin_name_combined)\n",
        "    violin_plot_overlay(Tevent1850_combined,Tevent1950_combined,Tanalogues1850_combined,Tanalogues1950_combined,lengths1850_combined,lengths1950_combined,list_for_violin_plot_t_analogues1850,list_for_violin_plot_t_analogues1950,list_for_violin_plot_lengths1850,list_for_violin_plot_lengths1950,violin_plot_path,violin_name_combined_overlay)\n",
        "\n",
        "  #Create the combined trend plot\n",
        "  if save_trend_plot == True:\n",
        "    trend_name_1850_combined = f\"combined_trend_plot_1850\"\n",
        "    trend_name_1950_combined = f\"combined_trend_plot_1950\"\n",
        "    trend_name_combined_subplots = f\"combined_trend_plot_subplots\"\n",
        "\n",
        "    plot_trend_combined(list_for_y_data_combined_trend_plot1850,list_for_running_mean_combined_trend_plot1850, running_mean_window, start_x_axis_trend1850, end_x_axis_trend1850,trend_plot_path,trend_name_1850_combined)\n",
        "    plot_trend_combined(list_for_y_data_combined_trend_plot1950,list_for_running_mean_combined_trend_plot1950, running_mean_window, start_x_axis_trend1950, end_x_axis_trend1950,trend_plot_path,trend_name_1950_combined)\n",
        "    plot_trend_combined_subplots(list_for_y_data_combined_trend_plot1850,list_for_y_data_combined_trend_plot1950, running_mean_window, start_x_axis_trend1850,start_x_axis_trend1950, end_x_axis_trend1850,end_x_axis_trend1950, trend_plot_path, trend_name_combined_subplots)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "itJtGkMX289I"
      },
      "source": [
        "# **Uitvoeren**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "x0Exp_MxrVMf"
      },
      "outputs": [],
      "source": [
        "base_path = \"/usr/people/noest/stage_folders/outputs\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "O5fvPRT8s33o"
      },
      "outputs": [],
      "source": [
        "# Define the name of the map where all the output will be saved\n",
        "name_run = \"2014_violin_test\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "t2yD1JNK2-2Q"
      },
      "outputs": [],
      "source": [
        "# Define the path where the data is located (path to folder)\n",
        "data_path = \"/net/pc200246/nobackup/users/noest/streamfunction_hadgem/regridded_merged\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "WD0zSV1slG7s"
      },
      "outputs": [],
      "source": [
        "# Define the path where the event is located (path to actual file)\n",
        "event_path = \"/usr/people/noest/stage_folders/event_data/Vautard_southerlyflow_2019-06-29_regridded_streamfunction_data_at_index_25381.npy\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "y6IAJJJenEwQ"
      },
      "outputs": [],
      "source": [
        "# Define area of interest\n",
        "S, N, W, E = 30,60,-30,20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "PXhn5yYHo7-J"
      },
      "outputs": [],
      "source": [
        "# Define period (season and years (yearsfor past and present))\n",
        "yearlength_original = 360 # the year length in the original data\n",
        "start_monthV = 3\n",
        "end_monthV = 5\n",
        "start_year1850 = 1950 # als alle jaren moet False zijn = niet meer, gwn jaar invullen\n",
        "end_year1850 = 1979 # als alle jaren moet False zijn\n",
        "start_year1950 = 1985 # als alle jaren moet False zijn\n",
        "end_year1950 = 2014 # als alle jaren moet False zijn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "eWrF4Y4-sFqN"
      },
      "outputs": [],
      "source": [
        "# How many analogues to find and how many days they have to be seperated by (5 is not 5 days before AND after)\n",
        "target_number_of_analogues = 30\n",
        "analogue_seperation_range = 6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "jx92_ZHQveyI"
      },
      "outputs": [],
      "source": [
        "# Determine the minimum correlation coefficient it needs to be taken into account as the same event\n",
        "c_threshold = 0.9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "YDbwRZlA1me6"
      },
      "outputs": [],
      "source": [
        "# Yearly minimum distance for trend analysis\n",
        "yearlength_filtered = 90 # For the cut down data, so if 3 months actually used for analogues = 90"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "jjd1EzPrXgAo"
      },
      "outputs": [],
      "source": [
        "# Save violin plot?:\n",
        "save_violin_plot = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "RdIVmm45oijv"
      },
      "outputs": [],
      "source": [
        "# Save trend plot? (als False toch nog iets invullen voor de andere!)\n",
        "save_trend_plot = False\n",
        "running_mean_window = 10\n",
        "start_x_axis_trend1850 = 1850 # voor alles is 1850\n",
        "end_x_axis_trend1850 = 1949 # voor alles is 1949\n",
        "start_x_axis_trend1950 = 1950 # voor alles is 1950\n",
        "end_x_axis_trend1950 = 2014 # voor alles is 2014"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "qUklvUBg5iat"
      },
      "outputs": [],
      "source": [
        "# Save a plot of the best analogues for each ensemble?\n",
        "save_analogue_plot = False\n",
        "amount_of_analogues_to_plot = 30 # It will save the best x analogues as seperate plots (for 1850 and 1950)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "gYlnkjOE6SDH"
      },
      "outputs": [],
      "source": [
        "# Save the differences map for all analogues combined and each analogue seperate\n",
        "save_differences_maps = False\n",
        "p_value_difference_maps = 0.05 # the p value for the significance in the difference plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "yFOCKjZ5ARPR"
      },
      "outputs": [],
      "source": [
        "# How many ensemble members to run (#5 is everything)\n",
        "amount_of_ensembles_to_run = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save all the selected days for the selected region to look for analogues in all data combined for the combined violin plot\n",
        "save_all_data = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Determine whether to load the AMO data and save the plots\n",
        "save_amo_plot = False\n",
        "amo_data_folder = '/net/pc200023/nobackup/users/thompson/LESFMIP/HadGEM/hist/tos'\n",
        "start_month_amo = 9\n",
        "end_month_amo = 11\n",
        "#start year and end year nog doen "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 213
        },
        "id": "rL1upAjX5zJt",
        "outputId": "6181ba8a-1f1c-4e67-933f-8601717a73ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "psi500_day_HadGEM3-GC31-LL_historical_r1i1p1f3_gn_1950_2014.nc\n"
          ]
        }
      ],
      "source": [
        "# Run the function\n",
        "analogues_combined(base_path,name_run,data_path,event_path,S,N,W,E,yearlength_original,start_monthV,end_monthV,start_year1850,end_year1850,start_year1950,end_year1950,target_number_of_analogues,analogue_seperation_range,c_threshold,yearlength_filtered,running_mean_window,start_x_axis_trend1850,end_x_axis_trend1850,start_x_axis_trend1950,end_x_axis_trend1950,amount_of_analogues_to_plot,p_value_difference_maps,amo_data_folder,start_month_amo,end_month_amo,save_violin_plot = save_violin_plot,save_trend_plot = save_trend_plot,amount_of_ensembles_to_run = amount_of_ensembles_to_run,save_analogue_plot = save_analogue_plot,save_differences_maps = save_differences_maps,save_all_data=save_all_data,save_amo_plot=save_amo_plot)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Proberen**"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
