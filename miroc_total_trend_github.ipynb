{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Notes** \n",
    "### Purpose: Calculates the total trend in TXx and TXm for MIROC6 model data\n",
    "##### Difference compared to HadGEM3: 365 dagen per jaar (en schrikkeldagen) en alleen files voor 1950-2014 in folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Preparation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # for storing vector and matrix data\n",
    "import matplotlib.pyplot as plt # to plot figures\n",
    "import netCDF4 as nc #to read netCDF files\n",
    "import cartopy.crs as ccrs # to plot maps\n",
    "# (ergens in test ook: import cartopy as cart)\n",
    "import cartopy.feature as cf\n",
    "# from matplotlib import ticker\n",
    "import scipy.io\n",
    "from scipy.stats import pearsonr # voor persistence\n",
    "import scipy.stats as stats\n",
    "# from cartopy.util import add_cyclic_point\n",
    "import os\n",
    "import xarray as xr\n",
    "import pickle\n",
    "from datetime import datetime, timedelta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path,*variables_to_add):\n",
    "  \"\"\"\n",
    "Provide the path to a file and the variables you want to extract\n",
    "  \"\"\"\n",
    "  data = nc.Dataset(path, mode='r')\n",
    "  variable_list = []\n",
    "  for variable in variables_to_add:\n",
    "    var =data.variables[variable][:]\n",
    "    variable_list.append(var)\n",
    "  return variable_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_variable(lat, lon, variable,folder,name):\n",
    "    plt.figure(figsize = (10,10))\n",
    "    ax = plt.axes(projection = ccrs.PlateCarree())\n",
    "    plot = plt.contourf(lon, lat, variable, cmap = \"RdBu_r\", transform = ccrs.PlateCarree(), levels = 15) #levels=np.linspace(-8.2e7, 1e7, 10), extend='both\n",
    "    ax.coastlines()\n",
    "    ax.add_feature(cf.BORDERS)\n",
    "    plt.colorbar(plot, ax=ax, orientation = \"horizontal\", label = \"degrees celcius/GWD\", pad = 0.05)\n",
    "    #plt.savefig(f\"{folder}/{name}.png\",dpi=300)\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_area(S, N, W, E, lat, lon, variable,event = False):\n",
    "    \"\"\"\n",
    "    This function slices the data given the S, N, W, E bounds. Use event = True if there are only two dimensions (since then there is no time dimension), this means after using this\n",
    "    function you need to use event[0] to get the data\n",
    "    \"\"\"\n",
    "    # Change longitude data to go from -180 to 180\n",
    "    for i in range(len(lon)):\n",
    "        if lon[i] > 180:\n",
    "          lon[i] = lon[i] - 360\n",
    "        else:\n",
    "          lon[i] = lon[i]\n",
    "\n",
    "    # Calculate the index of the bounds\n",
    "    sIndex = np.argmin(np.abs(lat - S))\n",
    "    nIndex = np.argmin(np.abs(lat - N))\n",
    "    wIndex = np.argmin(np.abs(lon - W))\n",
    "    eIndex = np.argmin(np.abs(lon - E))\n",
    "\n",
    "    if event:\n",
    "        variable = np.expand_dims(variable, axis = 0)\n",
    "\n",
    "    if wIndex > eIndex: # If the west index is higher than the east index, think of the right side of the world map as left boundary and vice versa\n",
    "        latSlice = lat[sIndex: nIndex + 1]\n",
    "        lonSlice = np.concatenate((lon[wIndex:], lon[:eIndex + 1]))\n",
    "        variableSlice = np.concatenate((variable[:, sIndex: nIndex + 1, wIndex:], variable[:, sIndex: nIndex + 1, :eIndex + 1]), axis = 2)\n",
    "\n",
    "    else:\n",
    "        latSlice = lat[sIndex: nIndex + 1]\n",
    "        lonSlice = lon[wIndex: eIndex + 1]\n",
    "        variableSlice = variable[:, sIndex: nIndex + 1, wIndex: eIndex + 1]\n",
    "\n",
    "    return latSlice, lonSlice, variableSlice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_season_year(variable,yearlength,start_day,end_day, start_year = False, end_year = False):\n",
    "  \"\"\"\n",
    "  Start day and end day should be the actuall day, so if you want the second day, third and fourth day, do 2,4 (151,240 would be JJA?) jaren ook: 1,5 is 1 tot en met 5\n",
    "  nadenken dat als gaat checken met al gesneden data dat yearlenght 90 is als op 3 maanden gesneden\n",
    "  \"\"\"\n",
    "  start_index = start_day-1\n",
    "  end_index = end_day-1\n",
    "  if start_year == False and end_year == False:\n",
    "    years = variable.shape[0]//yearlength\n",
    "    for year in range(years):\n",
    "      if year == 0:\n",
    "        selected_data = variable[(year*yearlength)+start_index:(end_index+1),:,:] # +1 omdat tot is ipv tot en met voor de laatste\n",
    "      elif year != 0:\n",
    "        add_data = variable[(year*yearlength)+start_index:(year*yearlength)+(end_index+1),:,:] # stel is 10, na 1 jaar dan 370 is TOT 370 dus index 369 en dan is dag 10\n",
    "        selected_data = np.concatenate((selected_data, add_data), axis = 0)\n",
    "    return selected_data\n",
    "  else:\n",
    "    years = (end_year-start_year) + 1\n",
    "    for year in range(years):\n",
    "      year_multiplier = (year + start_year) - 1\n",
    "      if year == 0:\n",
    "        selected_data = variable[(year_multiplier*yearlength)+start_index:(year_multiplier*yearlength)+(end_index+1),:,:] # +1 omdat tot is ipv tot en met voor de laatste\n",
    "      elif year != 0:\n",
    "        add_data = variable[(year_multiplier*yearlength)+start_index:(year_multiplier*yearlength)+(end_index+1),:,:] # stel is 10, na 1 jaar dan 370 is TOT 370 dus index 369 en dan is dag 10\n",
    "        selected_data = np.concatenate((selected_data, add_data), axis = 0)\n",
    "    return selected_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def global_warming_degree_function(temp_data,adjusted_yearlength, start_year, end_year):\n",
    "    \"\"\"\n",
    "    Hardcodes that its a 5 year centred running average, moet temp_data van seizoen of hele jaar?\n",
    "    Start en end year alleen voor hoeveel jaar, niet ook echt die eruit filteren! Dan moet extract_season_year gebruiken!\n",
    "    \"\"\"\n",
    "    amount_of_years = (end_year - start_year) + 1\n",
    "    list_for_finalized_averages = []\n",
    "    list_for_GWD = []\n",
    "    reshaped_data = temp_data.reshape(amount_of_years,adjusted_yearlength,temp_data.shape[1],temp_data.shape[2])\n",
    "    for i in range(amount_of_years):\n",
    "        if i == 0:\n",
    "            filtered_data = reshaped_data[0:3,:,:,:]\n",
    "        elif i == 1:\n",
    "            filtered_data = reshaped_data[0:4,:,:,:]\n",
    "        elif i == (amount_of_years-1): #laatste\n",
    "            filtered_data = reshaped_data[(i-2):,:,:,:]\n",
    "        elif i == (amount_of_years-2): \n",
    "            filtered_data = reshaped_data[(i-2):,:,:,:]\n",
    "        else:\n",
    "            filtered_data = reshaped_data[(i-2):(i+3),:,:,:]\n",
    "        mean_to_add = np.mean(filtered_data) # zou alle axis moeten meanen\n",
    "        list_for_finalized_averages.append(mean_to_add)\n",
    "\n",
    "    for average in list_for_finalized_averages:\n",
    "        GWD_value = average - list_for_finalized_averages[-1]\n",
    "        list_for_GWD.append(GWD_value)\n",
    "    \n",
    "    return list_for_GWD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temperature_trends__TXx_TXm(max_temp_data,adjusted_yearlength, start_year, end_year):\n",
    "    \"\"\"\n",
    "    Needs to use the maximum daily temperature data (TASMAX),note that instead of looping through the years, you could also just do np.mean(...,axis = 1)\n",
    "    because the o.g. reshaped data is still (years,months,lat,lon) (or lon/lat), so than you would get (years,lat,lon) \n",
    "    Nog doen dat trend averaged over blauwe box\n",
    "    \"\"\"\n",
    "    amount_of_years = (end_year - start_year) + 1\n",
    "    reshaped_data = max_temp_data.reshape(amount_of_years,adjusted_yearlength,max_temp_data.shape[1],max_temp_data.shape[2])\n",
    "    list_for_Txx_values = []\n",
    "    list_for_Txm_values = []\n",
    "\n",
    "    for i in range(amount_of_years):\n",
    "        filtered_data = reshaped_data[i,:,:,:]\n",
    "        max_per_season = np.max(filtered_data,axis = 0)\n",
    "        list_for_Txx_values.append(max_per_season)\n",
    "        mean_per_season = np.mean(filtered_data, axis = 0)\n",
    "        list_for_Txm_values.append(mean_per_season)\n",
    "\n",
    "    array_for_TXx = np.array(list_for_Txx_values)\n",
    "    array_for_TXm = np.array(list_for_Txm_values)\n",
    "    \n",
    "    return array_for_TXx, array_for_TXm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_temperature_trend(GWD_list,temperature_trend_array):\n",
    "\n",
    "    # Create empty arrays for the regression outputs\n",
    "    shape_1 = temperature_trend_array.shape[1]\n",
    "    shape_2 = temperature_trend_array.shape[2]\n",
    "\n",
    "    slope_array = np.zeros((shape_1,shape_2))\n",
    "    intercept_array = np.zeros((shape_1,shape_2))\n",
    "    rvalue_array = np.zeros((shape_1,shape_2))\n",
    "    pvalue_array = np.zeros((shape_1,shape_2))\n",
    "    stderr_array = np.zeros((shape_1,shape_2))\n",
    "\n",
    "    for i in range(shape_1):\n",
    "        for j in range(shape_2):\n",
    "            values_at_specific_coordinates = temperature_trend_array[:, i, j] # Find the values for all years in 1 grid cell\n",
    "            slope, intercept, rvalue, pvalue, stderr = stats.linregress(GWD_list, values_at_specific_coordinates)\n",
    "            \n",
    "            # Store the regression outputs in the empty arrays\n",
    "            slope_array[i, j] = slope\n",
    "            intercept_array[i, j] = intercept\n",
    "            rvalue_array[i, j] = rvalue\n",
    "            pvalue_array[i, j] = pvalue\n",
    "            stderr_array[i, j] = stderr\n",
    "\n",
    "    return slope_array,intercept_array,rvalue_array,pvalue_array,stderr_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def significance_standard_deviation(mean_array,standard_deviation_array):\n",
    "    mask = np.zeros((mean_array.shape[0],mean_array.shape[1]))\n",
    "    for i in range(mean_array.shape[0]):\n",
    "        for j in range(mean_array.shape[1]):\n",
    "            mean_value = mean_array[i,j]\n",
    "            absolute_mean = np.abs(mean_value)\n",
    "            std_value = standard_deviation_array[i,j]\n",
    "            threshold = std_value*2\n",
    "            if absolute_mean > threshold:\n",
    "                mask[i,j] = 0\n",
    "            else:\n",
    "                mask[i,j] = 1\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_differencet(lat, lon, variable,significance_mask,folder,name):\n",
    "    plt.figure(figsize = (10,10))\n",
    "    ax = plt.axes(projection = ccrs.PlateCarree())\n",
    "    plot = plt.contourf(lon, lat, variable, cmap = \"RdBu_r\", transform = ccrs.PlateCarree(), levels = 15) #levels=np.linspace(-8.2e7, 1e7, 10), extend='both\n",
    "    ax.contourf( lon, lat,significance_mask, levels=[-2,0,2], hatches=[None, '////'], colors='none', transform=ccrs.PlateCarree())\n",
    "    ax.coastlines()\n",
    "    ax.add_feature(cf.BORDERS)\n",
    "    plt.colorbar(plot, ax=ax, orientation = \"horizontal\", label = \"Pressure (Pa)\", pad = 0.05)\n",
    "    #plt.savefig(f\"{folder}/{name}.png\",dpi=300)\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_weighted(data):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : data\n",
    "         data.\n",
    "    \"\"\"\n",
    "    ## Calculate global-mean surface temperature (GMST)\n",
    "    cos_lat_2d = np.cos(np.deg2rad(data['lat'])) * xr.ones_like(data['lon']) # effective area weights\n",
    "    mean_ = ((data * cos_lat_2d).sum(dim=['lat','lon']) /\n",
    "                 cos_lat_2d.sum(dim=['lat','lon']))\n",
    "    return mean_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def global_warming_degree_function_area_weighted(temp_data_path,adjusted_yearlength, start_year, end_year, variable = \"t2m\"):\n",
    "    \"\"\"\n",
    "    Hardcodes that its a 5 year centred running average, moet temp_data van seizoen of hele jaar?\n",
    "    Start en end year alleen voor hoeveel jaar, niet ook echt die eruit filteren! Dan moet extract_season_year gebruiken!\n",
    "    doet weighted mean van de gegeven variable, hardcodes dat lat en lon heet (dus als variables in data lattitude en longitude hebt = aanpasses)\n",
    "    door for i in range(amount_of_years) hardcodes ook dat in 1950 begint!\n",
    "    end year kan hardcode 2014 voor reshape want is model data dus tot 2014 is alles\n",
    "    \"\"\"\n",
    "\n",
    "    #Open the NetCDF file with xarray\n",
    "    dataset = xr.open_dataset(temp_data_path)\n",
    "\n",
    "    #Calculate the area weighted mean\n",
    "    weighted_mean = mean_weighted(dataset[variable])\n",
    "\n",
    "    #Transform the xarray DataArray into an array\n",
    "    daily_mean_list = weighted_mean.values.tolist()\n",
    "    daily_mean_array = np.array(daily_mean_list)\n",
    "    \n",
    "    amount_of_years = (end_year - start_year) + 1\n",
    "    list_for_finalized_averages = []\n",
    "    list_for_GWD = []\n",
    "    reshaped_data = daily_mean_array.reshape(amount_of_years,adjusted_yearlength)\n",
    "    for i in range(amount_of_years):\n",
    "        if i == 0:\n",
    "            filtered_data = reshaped_data[0:3,:]\n",
    "        elif i == 1:\n",
    "            filtered_data = reshaped_data[0:4,:]\n",
    "        elif i == (amount_of_years-1): #laatste\n",
    "            filtered_data = reshaped_data[(i-2):,:]\n",
    "        elif i == (amount_of_years-2): \n",
    "            filtered_data = reshaped_data[(i-2):,:]\n",
    "        else:\n",
    "            filtered_data = reshaped_data[(i-2):(i+3),:]\n",
    "        mean_to_add = np.mean(filtered_data) # zou alle axis moeten meanen\n",
    "        list_for_finalized_averages.append(mean_to_add)\n",
    "\n",
    "    for average in list_for_finalized_averages:\n",
    "        GWD_value = average - list_for_finalized_averages[-1]\n",
    "        list_for_GWD.append(GWD_value)\n",
    "    \n",
    "    return list_for_GWD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_weighted_masked(data,mask):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : data\n",
    "         data.\n",
    "    \"\"\"\n",
    "    ## Calculate global-mean surface temperature (GMST)\n",
    "    mask_af = np.squeeze(mask)\n",
    "    cos_lat_2d = np.cos(np.deg2rad(data['lat'])) * xr.ones_like(data['lon']) # effective area weights\n",
    "    if cos_lat_2d.shape != mask_af.shape:\n",
    "        print (f\"WARNING: shapes of mask ({mask_af}) and cos_lat2d  ({cos_lat_2d.shape}) do no match (in mean_weigted2 (zelf))\")\n",
    "    cos_lat_2d_masked = cos_lat_2d*mask_af\n",
    "    mean_ = ((data * cos_lat_2d).sum(dim=['lat','lon']) /\n",
    "                 cos_lat_2d_masked.sum(dim=['lat','lon']))\n",
    "    return mean_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def area_averaged_trend(data,lat,lon,landmask_path,S,N,W,E):\n",
    "    \"\"\"\n",
    "    Calculates the area weighted mean land area trend of a region using a land mask\n",
    "    \"\"\"\n",
    "\n",
    "    #Load the mask and extract the selected region for the mask and actual data\n",
    "    latm,lonm,mask_data = load_data(landmask_path,\"lat\",\"lon\",\"tx\")\n",
    "    latm_box,lonm_box,mask_box = extract_area(S,N,W,E,latm,lonm,mask_data,event = False)\n",
    "    latd_box,lond_box,data_box = extract_area(S,N,W,E,lat,lon,data,event = True)\n",
    "\n",
    "    #Prepare mask\n",
    "    min_data = np.min(mask_box)\n",
    "    if min_data == -9999.0:\n",
    "        mask_box = np.where(mask_box == -9999.0, np.nan, mask_box)\n",
    "    mask = mask_box/mask_box\n",
    "    if mask.shape != data_box.shape:\n",
    "        print (\"WARNING: shape of mask and data do not match (zelf)\")\n",
    "    \n",
    "    #Perform analyses and plot to check\n",
    "    masked_data = data_box*mask\n",
    "    #plot_variable(latd_box,lond_box,masked_data[0],\"test\",\"test\")\n",
    "\n",
    "    #Turn into xarray and take area weighted average\n",
    "    time_indices = np.arange(masked_data.shape[0])  # Because you used event = True in extract area both now have three dimensions\n",
    "    data_xr_array = xr.DataArray(masked_data, dims=['time', 'lat', 'lon'], coords={'time': time_indices, 'lat': latd_box, 'lon': lond_box})\n",
    "    mean_trend = mean_weighted_masked(data_xr_array,mask)\n",
    "    mean_list = mean_trend.values.tolist()\n",
    "    #print (f\"weighted mean = {mean_list}, normal mean = {np.mean(masked_data)}\")\n",
    "\n",
    "    return mean_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lists_for_miroc_dates(final_year = 2015,final_month = 12 ,final_day = 31):\n",
    "    \"\"\"\n",
    "    Creates lists with all months and years in the complete ERA5 data, taking into account leap days (schrikkeldagen)\n",
    "    \"\"\"\n",
    "    start_date_all_era5_data = datetime(1950, 1, 1) # Is included\n",
    "    end_date_all_era5_data = datetime(final_year,final_month,final_day) # Is included\n",
    "    delta_time = timedelta(days=1)\n",
    "\n",
    "    date_list_basic = []\n",
    "    current_date = start_date_all_era5_data\n",
    "    while current_date <= end_date_all_era5_data:\n",
    "        date_list_basic.append(current_date)\n",
    "        current_date += delta_time\n",
    "    date_strings = [date.strftime('%Y-%m-%d') for date in date_list_basic]\n",
    "    month_list = [date.month for date in date_list_basic]\n",
    "    year_list = [date.year for date in date_list_basic]\n",
    "\n",
    "    return month_list, year_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_years_and_months_era5(era5_data,desired_start_month,desired_end_month,desired_start_year,desired_end_year,list_with_all_months,list_with_all_years):\n",
    "    \"\"\"\n",
    "    Slice era5 data based on the months and years, months and years that are used as input variable will be included as well\n",
    "    \"\"\"\n",
    "    if era5_data.shape[0] != len(list_with_all_months):\n",
    "        print (\"Error: Amount of days in the data and list with all dates are not the same\")\n",
    "    \n",
    "    list_for_filtered_era5_data = []\n",
    "    list_for_filtered_years = []\n",
    "    list_for_filtered_months = []\n",
    "    for i in range(era5_data.shape[0]):\n",
    "        month_at_index = list_with_all_months[i]\n",
    "        year_at_index = list_with_all_years[i]\n",
    "        if month_at_index >= desired_start_month and month_at_index <= desired_end_month and year_at_index >= desired_start_year and year_at_index <= desired_end_year:\n",
    "            data_to_select = era5_data[i,:,:]\n",
    "            list_for_filtered_era5_data.append(data_to_select)\n",
    "            list_for_filtered_years.append(year_at_index)\n",
    "            list_for_filtered_months.append(month_at_index)\n",
    "    array_selected_era5_data = np.array(list_for_filtered_era5_data)\n",
    "\n",
    "    return array_selected_era5_data, list_for_filtered_years, list_for_filtered_months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def global_warming_degree_function_miroc_area_weighted(temp_data_path,list_of_years, start_year, end_year, variable = \"tas\"):\n",
    "    \"\"\"\n",
    "    Hardcodes that its a 5 year centred running average, moet temp_data van het hele jaar\n",
    "    laatste if niet nodig want zegt dat maar 65 jaar doet (tot 2014) maar era5 data en year list lopen tot 2024 dus de year list loopt gwn naar 2015\n",
    "    doet weighted mean van de gegeven variable, hardcodes dat lat en lon heet (dus als variables in data lattitude en longitude hebt = aanpasses)\n",
    "    door for i in range(amount_of_years) hardcodes ook dat in 1950 begint!\n",
    "    \"\"\"\n",
    "    #Open the NetCDF file with xarray\n",
    "    dataset = xr.open_dataset(temp_data_path)\n",
    "\n",
    "    #Calculate the area weighted mean\n",
    "    weighted_mean = mean_weighted(dataset[variable])\n",
    "\n",
    "    #Transform the xarray DataArray into a list\n",
    "    daily_mean_list = weighted_mean.values.tolist()\n",
    "\n",
    "\n",
    "    amount_of_years = (end_year - start_year) + 1\n",
    "    list_for_finalized_averages = []\n",
    "    list_for_GWD = []\n",
    "\n",
    "    list_for_reshaped_data = []\n",
    "    day = 0\n",
    "    for i in range(amount_of_years):\n",
    "        list_for_data_per_year = []\n",
    "        condition_variable = True\n",
    "        while condition_variable == True:\n",
    "            data_to_add = daily_mean_list[day]\n",
    "            list_for_data_per_year.append(data_to_add)\n",
    "            day = day + 1\n",
    "            if list_of_years[day] != list_of_years[day-1]: #deze vergelijken met vorige omdat al day+1 hebt gedaan\n",
    "                list_for_reshaped_data.append(list_for_data_per_year)\n",
    "                condition_variable = False\n",
    "            #if day == (temp_data.shape[0]): #niet -1 omdat hierboven al day + 1 hebt gedaan\n",
    "                #condition_variable = False\n",
    "\n",
    "    for i in range(amount_of_years):\n",
    "        if i == 0:\n",
    "            indexes = [0,1,2]\n",
    "        elif i == 1:\n",
    "            indexes = [0,1,2,3]\n",
    "        elif i == (amount_of_years-1): #laatste\n",
    "            indexes = [i-2,i-1,i]\n",
    "        elif i == (amount_of_years-2): #(ook i nog minder hoog dus drm zelfde als vorige)\n",
    "            indexes = [i-2,i-1,i,i+1]\n",
    "        else:\n",
    "            indexes = [i-2,i-1,i,i+1,i+2]\n",
    "        \n",
    "        list_for_means_per_year = []\n",
    "        for index in indexes:\n",
    "            list_to_analyse = list_for_reshaped_data[index]\n",
    "            array_to_analyse = np.array(list_to_analyse)\n",
    "            mean_for_year = np.mean(array_to_analyse)\n",
    "            list_for_means_per_year.append(mean_for_year)\n",
    "        combined_mean = (sum(list_for_means_per_year))/(len(list_for_means_per_year))\n",
    "        list_for_finalized_averages.append(combined_mean)\n",
    "\n",
    "    for average in list_for_finalized_averages:\n",
    "        GWD_value = average - list_for_finalized_averages[-1]\n",
    "        list_for_GWD.append(GWD_value)\n",
    "    \n",
    "    return list_for_GWD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Combined**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temperature_trend_combined(data_path_tas,data_path_tasmax,amount_of_ensembles_to_run,S,N,W,E,start_day,end_day,save_path,season_name,land_mask_path,Sa,Na,Wa,Ea,save_output = False, plot_output = False):\n",
    "  \"\"\"\n",
    "  For now assumes we analyze 1950-2014 so you do not need the 1850-1949 file\n",
    "  Start en end year in GWD en TXx/TXm functie alleen voor hoeveel jaar, niet ook echt die eruit filteren! Dan moet extract_season_year gebruiken!\n",
    "  \"\"\"\n",
    "  #Create empty lists\n",
    "  list_for_mean_model_slope_TXx = []\n",
    "  list_for_mean_model_slope_TXm = []\n",
    "\n",
    "  list_for_TXx_area_average = []\n",
    "  list_for_TXm_area_average = []\n",
    "\n",
    "  #Prepare ensemble members\n",
    "  index_1950 = 0\n",
    "  max_index = (amount_of_ensembles_to_run)-1\n",
    "\n",
    "  folder_path_tas = sorted(os.listdir(data_path_tas)) # mean T model\n",
    "  folder_path_tasmax = sorted(os.listdir(data_path_tasmax)) # max T model\n",
    "  # Loop through ensemble members and determine paths\n",
    "  while index_1950 <= max_index:\n",
    "    file1950_tas = folder_path_tas[index_1950]\n",
    "    data_path1950_tas = os.path.join(data_path_tas, file1950_tas)\n",
    "\n",
    "    file1950_tasmax = folder_path_tasmax[index_1950]\n",
    "    data_path1950_tasmax = os.path.join(data_path_tasmax, file1950_tasmax)\n",
    "    #Load data (als 1950-2014 doet is geen past nodig)\n",
    "    #lat_model_global_tas,lon_model_global_tas,tas_model_global = load_data(data_path1950_tas,\"lat\",\"lon\",\"tas\")\n",
    "    lat_model_global_tasmax,lon_model_global_tasmax,tasmax_model_global = load_data(data_path1950_tasmax,\"lat\",\"lon\",\"tasmax\")\n",
    "\n",
    "    #Create lists with months and years\n",
    "    month_list, year_list = lists_for_miroc_dates()\n",
    "\n",
    "    #Calculate GWD (do this before slicing the data)\n",
    "    #GWD = global_warming_degree_function_area_weighted(data_path1950_tas,360, 1950, 2014,variable = \"tas\") #1950-2014 niet om te selecten, alleen hoeveelheid jaar\n",
    "    GWD = global_warming_degree_function_miroc_area_weighted(data_path1950_tas,year_list,1950,2014,variable=\"tas\")\n",
    "\n",
    "\n",
    "\n",
    "    #Extract area and season for max T en calculte trends\n",
    "    lat_model_extracted,lon_model_extracted,tasmax_model_extracted = extract_area(S, N, W, E,lat_model_global_tasmax,lon_model_global_tasmax,tasmax_model_global,event = False)\n",
    "    #tasmax_model_filtered = extract_season_year(tasmax_model_extracted,360,start_day,end_day)\n",
    "    tasmax_model_filtered, fyears, fmonths = extract_years_and_months_era5(tasmax_model_extracted,start_day,end_day,1950,2014,month_list,year_list)\n",
    "    Txx_array,Txm_array = temperature_trends__TXx_TXm(tasmax_model_filtered,92, 1950, 2014) #90 hardcoded, als longer warm period is moet anders! Ook 1950-2014 niet om te selecten, alleen hoeveelheid jaar\n",
    "    \n",
    "    #Perform regressions\n",
    "    slope_Txx_model,intercept_Txx_model,rvalue_Txx_model,pvalue_Txx_model,stderr_Txx_model = regression_temperature_trend(GWD,Txx_array)\n",
    "    slope_Txm_model,intercept_Txm_model,rvalue_Txm_model,pvalue_Txm_model,stderr_Txm_model = regression_temperature_trend(GWD,Txm_array)\n",
    "    \n",
    "    list_for_mean_model_slope_TXx.append(slope_Txx_model)\n",
    "    list_for_mean_model_slope_TXm.append(slope_Txm_model)\n",
    "    \n",
    "    mean_Txx = area_averaged_trend(slope_Txx_model,lat_model_extracted,lon_model_extracted,land_mask_path,Sa,Na,Wa,Ea)\n",
    "    mean_Txm = area_averaged_trend(slope_Txm_model,lat_model_extracted,lon_model_extracted,land_mask_path,Sa,Na,Wa,Ea)\n",
    "\n",
    "    mean_Txx_value = mean_Txx[0]\n",
    "    list_for_TXx_area_average.append(mean_Txx_value)\n",
    "    mean_Txm_value = mean_Txm[0]\n",
    "    list_for_TXm_area_average.append(mean_Txm_value)\n",
    "\n",
    "    #Make plots\n",
    "    #plot_variable(lat_model_extracted,lon_model_extracted,slope_Txx_model,\"test1name\",\"test2path\")\n",
    "    #plot_variable(lat_model_extracted,lon_model_extracted,slope_Txm_model,\"test1name\",\"test2path\")\n",
    "\n",
    "    #Increase indexes:\n",
    "    index_1950 = index_1950 + 1\n",
    "\n",
    "  #Outside of loop again\n",
    "    \n",
    "  #Calculate and plot the mean for the model ensemble members\n",
    "  array_for_mean_model_slope_TXx = np.array(list_for_mean_model_slope_TXx)\n",
    "  array_for_mean_model_slope_TXm = np.array(list_for_mean_model_slope_TXm)\n",
    "  \n",
    "  if save_output == True:\n",
    "    np.save(f\"{save_path}/{season_name}_TXx_all_ensembles.npy\",array_for_mean_model_slope_TXx) \n",
    "    np.save(f\"{save_path}/{season_name}_TXm_all_ensembles.npy\",array_for_mean_model_slope_TXm) \n",
    "    lat_to_save = np.array(lat_model_extracted)\n",
    "    lon_to_save = np.array(lon_model_extracted)\n",
    "    np.save(f\"{save_path}/{season_name}_lat_extracted.npy\",lat_to_save) \n",
    "    np.save(f\"{save_path}/{season_name}_lon_extracted.npy\",lon_to_save)\n",
    "\n",
    "  mean_array_TXx = np.mean(array_for_mean_model_slope_TXx, axis = 0)\n",
    "  mean_array_TXm = np.mean(array_for_mean_model_slope_TXm, axis = 0)\n",
    "  standard_deviation_TXx = np.std(array_for_mean_model_slope_TXx, axis = 0)\n",
    "  standard_deviation_TXm = np.std(array_for_mean_model_slope_TXm, axis = 0)\n",
    "\n",
    "  if save_output == True:\n",
    "    np.save(f\"{save_path}/{season_name}_TXx_mean.npy\",mean_array_TXx) \n",
    "    np.save(f\"{save_path}/{season_name}_TXm_mean.npy\",mean_array_TXm)\n",
    "    np.save(f\"{save_path}/{season_name}_TXx_std.npy\",standard_deviation_TXx) \n",
    "    np.save(f\"{save_path}/{season_name}_TXm_std.npy\",standard_deviation_TXm)\n",
    "\n",
    "  sig_mask_Txx = significance_standard_deviation(mean_array_TXx,standard_deviation_TXx)\n",
    "  sig_mask_Txm = significance_standard_deviation(mean_array_TXm,standard_deviation_TXm)\n",
    "\n",
    "  if save_output == True:\n",
    "    np.save(f\"{save_path}/{season_name}_TXx_significance.npy\",sig_mask_Txx) \n",
    "    np.save(f\"{save_path}/{season_name}_TXm_significance.npy\",sig_mask_Txm)\n",
    "    \n",
    "  if save_output == True:\n",
    "    filenametxx = f\"{save_path}/{season_name}_model_meanTxxlist.pkl\"\n",
    "    with open(filenametxx, 'wb') as f:\n",
    "        pickle.dump(list_for_TXx_area_average, f)\n",
    "    filenametxm = f\"{save_path}/{season_name}_model_meanTxmlist.pkl\"\n",
    "    with open(filenametxm, 'wb') as f:\n",
    "        pickle.dump(list_for_TXm_area_average, f)\n",
    "\n",
    "  if plot_output == True:\n",
    "    plot_variable(lat_model_extracted,lon_model_extracted,mean_array_TXx,\"test1name\",\"test2path\")\n",
    "    plot_variable(lat_model_extracted,lon_model_extracted,standard_deviation_TXx,\"test1name\",\"test2path\")\n",
    "    plot_differencet(lat_model_extracted,lon_model_extracted,mean_array_TXx,sig_mask_Txx,\"test1name\",\"test2path\")\n",
    "\n",
    "    plot_variable(lat_model_extracted,lon_model_extracted,mean_array_TXm,\"test1name\",\"test2path\")\n",
    "    plot_variable(lat_model_extracted,lon_model_extracted,standard_deviation_TXm,\"test1name\",\"test2path\")\n",
    "    plot_differencet(lat_model_extracted,lon_model_extracted,mean_array_TXm,sig_mask_Txm,\"test1name\",\"test2path\")\n",
    "\n",
    "  print (f\"average trend Txx: {np.mean(np.array(list_for_TXx_area_average))}\")\n",
    "  print (f\"average trend Txm: {np.mean(np.array(list_for_TXm_area_average))}\")\n",
    "\n",
    "  return lat_model_extracted,lon_model_extracted,mean_array_TXx,standard_deviation_TXx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Uitvoeren**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to folders with data\n",
    "data_path_tasV = \"/net/pc200246/nobackup/users/noest/LESFMIP/MIROC6/hist/tas_merged_regridded\"\n",
    "data_path_tasmaxV = \"/net/pc200246/nobackup/users/noest/LESFMIP/MIROC6/hist/tasmax_merged_regridded\"\n",
    "land_mask_pathV = \"/net/pc200246/nobackup/users/noest/landmask/landmask_day_regridded.nc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many ensemble members to run (50 is alles)\n",
    "amount_of_ensembles_to_runV = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract area and season\n",
    "S1 = 30\n",
    "N1 = 70\n",
    "W1 = -30\n",
    "E1 = 30\n",
    "start_dayV = 3 #For JJA = 151, for MAM = 61 NU START MAAND\n",
    "end_dayV = 5 #For JJA = 240 for MAM = 150 NU EIND MAAND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determine the area over which the trend is averaged\n",
    "S_A = 45\n",
    "N_A = 55\n",
    "W_A = -5\n",
    "E_A = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where and how to save (zelf mapje al maken! (in prompt mkdir ...))\n",
    "save_outputV = True\n",
    "plot_outputV = True\n",
    "save_pathV = '/usr/people/noest/stage_folders/outputs/net/serious_run2/total_trend/model_MIROC'\n",
    "season_nameV = \"MAM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lattest,lontest,testxxm_mean,test_std_txx = temperature_trend_combined(data_path_tasV,data_path_tasmaxV,amount_of_ensembles_to_runV,S1,N1,W1,E1,start_dayV,end_dayV,save_pathV,season_nameV,land_mask_pathV,S_A,N_A,W_A,E_A,save_output = save_outputV,plot_output = plot_outputV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Proberen**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "knmi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
