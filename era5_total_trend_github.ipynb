{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Notes** \n",
    "### Purpose: Calculate the total trend in TXx and TXm for ERA5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Preparation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # for storing vector and matrix data\n",
    "import matplotlib.pyplot as plt # to plot figures\n",
    "import netCDF4 as nc #to read netCDF files\n",
    "import cartopy.crs as ccrs # to plot maps\n",
    "# (ergens in test ook: import cartopy as cart)\n",
    "import cartopy.feature as cf\n",
    "# from matplotlib import ticker\n",
    "import scipy.io\n",
    "from scipy.stats import pearsonr # voor persistence\n",
    "import scipy.stats as stats\n",
    "# from cartopy.util import add_cyclic_point\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import xarray as xr\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path,*variables_to_add):\n",
    "  \"\"\"\n",
    "Provide the path to a file and the variables you want to extract\n",
    "  \"\"\"\n",
    "  data = nc.Dataset(path, mode='r')\n",
    "  variable_list = []\n",
    "  for variable in variables_to_add:\n",
    "    var =data.variables[variable][:]\n",
    "    variable_list.append(var)\n",
    "  return variable_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_variablet(lat, lon, variable,folder,name):\n",
    "    plt.figure(figsize = (10,10))\n",
    "    ax = plt.axes(projection = ccrs.PlateCarree())\n",
    "    plot = plt.contourf(lon, lat, variable, cmap = \"RdBu_r\", transform = ccrs.PlateCarree(), levels = 15) #levels=np.linspace(-8.2e7, 1e7, 10), extend='both\n",
    "    ax.coastlines()\n",
    "    ax.add_feature(cf.BORDERS)\n",
    "    plt.colorbar(plot, ax=ax, orientation = \"horizontal\", label = \"degree celcius/GWD\", pad = 0.05)\n",
    "    #plt.savefig(f\"{folder}/{name}.png\",dpi=300)\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_area(S, N, W, E, lat, lon, variable,event = False):\n",
    "    \"\"\"\n",
    "    This function slices the data given the S, N, W, E bounds. Use event = True if there are only two dimensions (since then there is no time dimension), this means after using this\n",
    "    function you need to use event[0] to get the data\n",
    "    \"\"\"\n",
    "    # Change longitude data to go from -180 to 180\n",
    "    for i in range(len(lon)):\n",
    "        if lon[i] > 180:\n",
    "          lon[i] = lon[i] - 360\n",
    "        else:\n",
    "          lon[i] = lon[i]\n",
    "\n",
    "    # Calculate the index of the bounds\n",
    "    sIndex = np.argmin(np.abs(lat - S))\n",
    "    nIndex = np.argmin(np.abs(lat - N))\n",
    "    wIndex = np.argmin(np.abs(lon - W))\n",
    "    eIndex = np.argmin(np.abs(lon - E))\n",
    "\n",
    "    if event:\n",
    "        variable = np.expand_dims(variable, axis = 0)\n",
    "\n",
    "    if wIndex > eIndex: # If the west index is higher than the east index, think of the right side of the world map as left boundary and vice versa\n",
    "        latSlice = lat[sIndex: nIndex + 1]\n",
    "        lonSlice = np.concatenate((lon[wIndex:], lon[:eIndex + 1]))\n",
    "        variableSlice = np.concatenate((variable[:, sIndex: nIndex + 1, wIndex:], variable[:, sIndex: nIndex + 1, :eIndex + 1]), axis = 2)\n",
    "\n",
    "    else:\n",
    "        latSlice = lat[sIndex: nIndex + 1]\n",
    "        lonSlice = lon[wIndex: eIndex + 1]\n",
    "        variableSlice = variable[:, sIndex: nIndex + 1, wIndex: eIndex + 1]\n",
    "\n",
    "    return latSlice, lonSlice, variableSlice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_season_year(variable,yearlength,start_day,end_day, start_year = False, end_year = False):\n",
    "  \"\"\"\n",
    "  Start day and end day should be the actuall day, so if you want the second day, third and fourth day, do 2,4 (151,240 would be JJA?) jaren ook: 1,5 is 1 tot en met 5\n",
    "  nadenken dat als gaat checken met al gesneden data dat yearlenght 90 is als op 3 maanden gesneden\n",
    "  \"\"\"\n",
    "  start_index = start_day-1\n",
    "  end_index = end_day-1\n",
    "  if start_year == False and end_year == False:\n",
    "    years = variable.shape[0]//yearlength\n",
    "    for year in range(years):\n",
    "      if year == 0:\n",
    "        selected_data = variable[(year*yearlength)+start_index:(end_index+1),:,:] # +1 omdat tot is ipv tot en met voor de laatste\n",
    "      elif year != 0:\n",
    "        add_data = variable[(year*yearlength)+start_index:(year*yearlength)+(end_index+1),:,:] # stel is 10, na 1 jaar dan 370 is TOT 370 dus index 369 en dan is dag 10\n",
    "        selected_data = np.concatenate((selected_data, add_data), axis = 0)\n",
    "    return selected_data\n",
    "  else:\n",
    "    years = (end_year-start_year) + 1\n",
    "    for year in range(years):\n",
    "      year_multiplier = (year + start_year) - 1\n",
    "      if year == 0:\n",
    "        selected_data = variable[(year_multiplier*yearlength)+start_index:(year_multiplier*yearlength)+(end_index+1),:,:] # +1 omdat tot is ipv tot en met voor de laatste\n",
    "      elif year != 0:\n",
    "        add_data = variable[(year_multiplier*yearlength)+start_index:(year_multiplier*yearlength)+(end_index+1),:,:] # stel is 10, na 1 jaar dan 370 is TOT 370 dus index 369 en dan is dag 10\n",
    "        selected_data = np.concatenate((selected_data, add_data), axis = 0)\n",
    "    return selected_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def global_warming_degree_function(temp_data,adjusted_yearlength, start_year, end_year):\n",
    "    \"\"\"\n",
    "    Hardcodes that its a 5 year centred running average, moet temp_data van seizoen of hele jaar?\n",
    "    \"\"\"\n",
    "    amount_of_years = (end_year - start_year) + 1\n",
    "    list_for_finalized_averages = []\n",
    "    list_for_GWD = []\n",
    "    reshaped_data = temp_data.reshape(amount_of_years,adjusted_yearlength,temp_data.shape[1],temp_data.shape[2])\n",
    "    for i in range(amount_of_years):\n",
    "        if i == 0:\n",
    "            filtered_data = reshaped_data[0:3,:,:,:]\n",
    "        elif i == 1:\n",
    "            filtered_data = reshaped_data[0:4,:,:,:]\n",
    "        elif i == (amount_of_years-1): #laatste\n",
    "            filtered_data = reshaped_data[(i-2):,:,:,:]\n",
    "        elif i == (amount_of_years-2): \n",
    "            filtered_data = reshaped_data[(i-2):,:,:,:]\n",
    "        else:\n",
    "            filtered_data = reshaped_data[(i-2):(i+3),:,:,:]\n",
    "        mean_to_add = np.mean(filtered_data) # zou alle axis moeten meanen\n",
    "        list_for_finalized_averages.append(mean_to_add)\n",
    "\n",
    "    for average in list_for_finalized_averages:\n",
    "        GWD_value = average - list_for_finalized_averages[-1]\n",
    "        list_for_GWD.append(GWD_value)\n",
    "    \n",
    "    return list_for_GWD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temperature_trends__TXx_TXm(max_temp_data,adjusted_yearlength, start_year, end_year):\n",
    "    \"\"\"\n",
    "    Needs to use the maximum daily temperature data (TASMAX),note that instead of looping through the years, you could also just do np.mean(...,axis = 1)\n",
    "    because the o.g. reshaped data is still (years,months,lat,lon) (or lon/lat), so than you would get (years,lat,lon) \n",
    "    \"\"\"\n",
    "    amount_of_years = (end_year - start_year) + 1\n",
    "    reshaped_data = max_temp_data.reshape(amount_of_years,adjusted_yearlength,max_temp_data.shape[1],max_temp_data.shape[2])\n",
    "    list_for_Txx_values = []\n",
    "    list_for_Txm_values = []\n",
    "\n",
    "    for i in range(amount_of_years):\n",
    "        filtered_data = reshaped_data[i,:,:,:]\n",
    "        max_per_season = np.max(filtered_data,axis = 0)\n",
    "        list_for_Txx_values.append(max_per_season)\n",
    "        mean_per_season = np.mean(filtered_data, axis = 0)\n",
    "        list_for_Txm_values.append(mean_per_season)\n",
    "\n",
    "    array_for_TXx = np.array(list_for_Txx_values)\n",
    "    array_for_TXm = np.array(list_for_Txm_values)\n",
    "    \n",
    "    return array_for_TXx, array_for_TXm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_temperature_trend(GWD_list,temperature_trend_array):\n",
    "\n",
    "    # Create empty arrays for the regression outputs\n",
    "    shape_1 = temperature_trend_array.shape[1]\n",
    "    shape_2 = temperature_trend_array.shape[2]\n",
    "\n",
    "    slope_array = np.zeros((shape_1,shape_2))\n",
    "    intercept_array = np.zeros((shape_1,shape_2))\n",
    "    rvalue_array = np.zeros((shape_1,shape_2))\n",
    "    pvalue_array = np.zeros((shape_1,shape_2))\n",
    "    stderr_array = np.zeros((shape_1,shape_2))\n",
    "\n",
    "    for i in range(shape_1):\n",
    "        for j in range(shape_2):\n",
    "            values_at_specific_coordinates = temperature_trend_array[:, i, j] # Find the values for all years in 1 grid cell\n",
    "            slope, intercept, rvalue, pvalue, stderr = stats.linregress(GWD_list, values_at_specific_coordinates)\n",
    "            \n",
    "            # Store the regression outputs in the empty arrays\n",
    "            slope_array[i, j] = slope\n",
    "            intercept_array[i, j] = intercept\n",
    "            rvalue_array[i, j] = rvalue\n",
    "            pvalue_array[i, j] = pvalue\n",
    "            stderr_array[i, j] = stderr\n",
    "\n",
    "    return slope_array,intercept_array,rvalue_array,pvalue_array,stderr_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lists_for_era5_dates(final_year,final_month,final_day):\n",
    "    \"\"\"\n",
    "    Creates lists with all months and years in the complete ERA5 data, taking into account leap days (schrikkeldagen)\n",
    "    \"\"\"\n",
    "    start_date_all_era5_data = datetime(1950, 1, 1) # Is included\n",
    "    end_date_all_era5_data = datetime(final_year,final_month,final_day) # Is included\n",
    "    delta_time = timedelta(days=1)\n",
    "\n",
    "    date_list_basic = []\n",
    "    current_date = start_date_all_era5_data\n",
    "    while current_date <= end_date_all_era5_data:\n",
    "        date_list_basic.append(current_date)\n",
    "        current_date += delta_time\n",
    "    #date_strings = [date.strftime('%Y-%m-%d') for date in date_list_basic]\n",
    "    month_list = [date.month for date in date_list_basic]\n",
    "    year_list = [date.year for date in date_list_basic]\n",
    "\n",
    "    return month_list, year_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_years_and_months_era5(era5_data,desired_start_month,desired_end_month,desired_start_year,desired_end_year,list_with_all_months,list_with_all_years):\n",
    "    \"\"\"\n",
    "    Slice era5 data based on the months and years, months and years that are used as input variable will be included as well\n",
    "    \"\"\"\n",
    "    if era5_data.shape[0] != len(list_with_all_months):\n",
    "        print (\"Error: Amount of days in the data and list with all dates are not the same\")\n",
    "    \n",
    "    list_for_filtered_era5_data = []\n",
    "    for i in range(era5_data.shape[0]):\n",
    "        month_at_index = list_with_all_months[i]\n",
    "        year_at_index = list_with_all_years[i]\n",
    "        if month_at_index >= desired_start_month and month_at_index <= desired_end_month and year_at_index >= desired_start_year and year_at_index <= desired_end_year:\n",
    "            data_to_select = era5_data[i,:,:]\n",
    "            list_for_filtered_era5_data.append(data_to_select)\n",
    "            \n",
    "    array_selected_era5_data = np.array(list_for_filtered_era5_data)\n",
    "\n",
    "    return array_selected_era5_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def global_warming_degree_function_era5(temp_data,list_of_years, start_year, end_year):\n",
    "    \"\"\"\n",
    "    Hardcodes that its a 5 year centred running average, moet temp_data van seizoen of hele jaar?\n",
    "    laatste if niet nodig want zegt dat maar 65 jaar doet (tot 2014) maar era5 data en year list lopen tot 2024 dus de year list loopt gwn naar 2015\n",
    "    \"\"\"\n",
    "    amount_of_years = (end_year - start_year) + 1\n",
    "    list_for_finalized_averages = []\n",
    "    list_for_GWD = []\n",
    "\n",
    "    list_for_reshaped_data = []\n",
    "    day = 0\n",
    "    for i in range(amount_of_years):\n",
    "        list_for_data_per_year = []\n",
    "        condition_variable = True\n",
    "        while condition_variable == True:\n",
    "            data_to_add = temp_data[day,:,:]\n",
    "            list_for_data_per_year.append(data_to_add)\n",
    "            day = day + 1\n",
    "            if list_of_years[day] != list_of_years[day-1]:\n",
    "                list_for_reshaped_data.append(list_for_data_per_year)\n",
    "                condition_variable = False\n",
    "            #if day == (temp_data.shape[0]): #niet -1 omdat hierboven al day + 1 hebt gedaan\n",
    "                #condition_variable = False\n",
    "\n",
    "    for i in range(amount_of_years):\n",
    "        if i == 0:\n",
    "            indexes = [0,1,2]\n",
    "        elif i == 1:\n",
    "            indexes = [0,1,2,3]\n",
    "        elif i == (amount_of_years-1): #laatste\n",
    "            indexes = [i-2,i-1,i]\n",
    "        elif i == (amount_of_years-2): #(ook i nog minder hoog dus drm zelfde als vorige)\n",
    "            indexes = [i-2,i-1,i,i+1]\n",
    "        else:\n",
    "            indexes = [i-2,i-1,i,i+1,i+2]\n",
    "        \n",
    "        list_for_means_per_year = []\n",
    "        for index in indexes:\n",
    "            list_to_analyse = list_for_reshaped_data[index]\n",
    "            array_to_analyse = np.array(list_to_analyse)\n",
    "            mean_for_year = np.mean(array_to_analyse)\n",
    "            list_for_means_per_year.append(mean_for_year)\n",
    "        combined_mean = (sum(list_for_means_per_year))/(len(list_for_means_per_year))\n",
    "        list_for_finalized_averages.append(combined_mean)\n",
    "\n",
    "    for average in list_for_finalized_averages:\n",
    "        GWD_value = average - list_for_finalized_averages[-1]\n",
    "        list_for_GWD.append(GWD_value)\n",
    "    \n",
    "    return list_for_GWD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_weighted(data):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : data\n",
    "         data.\n",
    "    \"\"\"\n",
    "    ## Calculate global-mean surface temperature (GMST)\n",
    "    cos_lat_2d = np.cos(np.deg2rad(data['lat'])) * xr.ones_like(data['lon']) # effective area weights\n",
    "    mean_ = ((data * cos_lat_2d).sum(dim=['lat','lon']) /\n",
    "                 cos_lat_2d.sum(dim=['lat','lon']))\n",
    "    return mean_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def global_warming_degree_function_era5_area_weighted(temp_data_path,list_of_years, start_year, end_year, variable = \"t2m\"):\n",
    "    \"\"\"\n",
    "    Hardcodes that its a 5 year centred running average, moet temp_data van het hele jaar\n",
    "    laatste if niet nodig want zegt dat maar 65 jaar doet (tot 2014) maar era5 data en year list lopen tot 2024 dus de year list loopt gwn naar 2015\n",
    "    doet weighted mean van de gegeven variable, hardcodes dat lat en lon heet (dus als variables in data lattitude en longitude hebt = aanpasses)\n",
    "    door for i in range(amount_of_years) hardcodes ook dat in 1950 begint!\n",
    "    \"\"\"\n",
    "    #Open the NetCDF file with xarray\n",
    "    dataset = xr.open_dataset(temp_data_path)\n",
    "\n",
    "    #Calculate the area weighted mean\n",
    "    weighted_mean = mean_weighted(dataset[variable])\n",
    "\n",
    "    #Transform the xarray DataArray into a list\n",
    "    daily_mean_list = weighted_mean.values.tolist()\n",
    "\n",
    "\n",
    "    amount_of_years = (end_year - start_year) + 1\n",
    "    list_for_finalized_averages = []\n",
    "    list_for_GWD = []\n",
    "\n",
    "    list_for_reshaped_data = []\n",
    "    day = 0\n",
    "    for i in range(amount_of_years):\n",
    "        list_for_data_per_year = []\n",
    "        condition_variable = True\n",
    "        while condition_variable == True:\n",
    "            data_to_add = daily_mean_list[day]\n",
    "            list_for_data_per_year.append(data_to_add)\n",
    "            day = day + 1\n",
    "            if list_of_years[day] != list_of_years[day-1]: #deze vergelijken met vorige omdat al day+1 hebt gedaan\n",
    "                list_for_reshaped_data.append(list_for_data_per_year)\n",
    "                condition_variable = False\n",
    "            #if day == (temp_data.shape[0]): #niet -1 omdat hierboven al day + 1 hebt gedaan\n",
    "                #condition_variable = False\n",
    "\n",
    "    for i in range(amount_of_years):\n",
    "        if i == 0:\n",
    "            indexes = [0,1,2]\n",
    "        elif i == 1:\n",
    "            indexes = [0,1,2,3]\n",
    "        elif i == (amount_of_years-1): #laatste\n",
    "            indexes = [i-2,i-1,i]\n",
    "        elif i == (amount_of_years-2): #(ook i nog minder hoog dus drm zelfde als vorige)\n",
    "            indexes = [i-2,i-1,i,i+1]\n",
    "        else:\n",
    "            indexes = [i-2,i-1,i,i+1,i+2]\n",
    "        \n",
    "        list_for_means_per_year = []\n",
    "        for index in indexes:\n",
    "            list_to_analyse = list_for_reshaped_data[index]\n",
    "            array_to_analyse = np.array(list_to_analyse)\n",
    "            mean_for_year = np.mean(array_to_analyse)\n",
    "            list_for_means_per_year.append(mean_for_year)\n",
    "        combined_mean = (sum(list_for_means_per_year))/(len(list_for_means_per_year))\n",
    "        list_for_finalized_averages.append(combined_mean)\n",
    "\n",
    "    for average in list_for_finalized_averages:\n",
    "        GWD_value = average - list_for_finalized_averages[-1]\n",
    "        list_for_GWD.append(GWD_value)\n",
    "    \n",
    "    return list_for_GWD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_weighted_masked(data,mask):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : data\n",
    "         data.\n",
    "    \"\"\"\n",
    "    ## Calculate global-mean surface temperature (GMST)\n",
    "    mask_af = np.squeeze(mask)\n",
    "    cos_lat_2d = np.cos(np.deg2rad(data['lat'])) * xr.ones_like(data['lon']) # effective area weights\n",
    "    if cos_lat_2d.shape != mask_af.shape:\n",
    "        print (f\"WARNING: shapes of mask ({mask_af}) and cos_lat2d  ({cos_lat_2d.shape}) do no match (in mean_weigted2 (zelf))\")\n",
    "    cos_lat_2d_masked = cos_lat_2d*mask_af\n",
    "    mean_ = ((data * cos_lat_2d).sum(dim=['lat','lon']) /\n",
    "                 cos_lat_2d_masked.sum(dim=['lat','lon']))\n",
    "    return mean_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def area_averaged_trend(data,lat,lon,landmask_path,S,N,W,E):\n",
    "    \"\"\"\n",
    "    Calculates the area weighted mean land area trend of a region using a land mask\n",
    "    \"\"\"\n",
    "\n",
    "    #Load the mask and extract the selected region for the mask and actual data\n",
    "    latm,lonm,mask_data = load_data(landmask_path,\"lat\",\"lon\",\"tx\")\n",
    "    latm_box,lonm_box,mask_box = extract_area(S,N,W,E,latm,lonm,mask_data,event = False)\n",
    "    latd_box,lond_box,data_box = extract_area(S,N,W,E,lat,lon,data,event = True)\n",
    "\n",
    "    #Prepare mask\n",
    "    min_data = np.min(mask_box)\n",
    "    if min_data == -9999.0:\n",
    "        mask_box = np.where(mask_box == -9999.0, np.nan, mask_box)\n",
    "    mask = mask_box/mask_box\n",
    "    if mask.shape != data_box.shape:\n",
    "        print (\"WARNING: shape of mask and data do not match (zelf)\")\n",
    "    \n",
    "    #Perform analyses and plot to check\n",
    "    masked_data = data_box*mask\n",
    "    plot_variablet(latd_box,lond_box,masked_data[0],\"test\",\"test\")\n",
    "\n",
    "    #Turn into xarray and take area weighted average\n",
    "    time_indices = np.arange(masked_data.shape[0])  # Because you used event = True in extract area both now have three dimensions\n",
    "    data_xr_array = xr.DataArray(masked_data, dims=['time', 'lat', 'lon'], coords={'time': time_indices, 'lat': latd_box, 'lon': lond_box})\n",
    "    mean_trend = mean_weighted_masked(data_xr_array,mask)\n",
    "    mean_list = mean_trend.values.tolist()\n",
    "    print (f\"weighted mean = {mean_list}, normal mean = {np.mean(masked_data)}\")\n",
    "\n",
    "    return mean_list\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Set-up**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select: \"regridded\" or \"original\", Als original data doet hebt kans dat kleiner gebied moet doen want doet EU data en dat data geupdate is dus dat lijst langer moet zijn met dates\n",
    "data_to_use = \"regridded\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The latest date in the original ERA5 dataset, (for the regridded data its 2024, 2, 29 which is selected automatically if data_to_use = \"regridded\" \n",
    "final_year_og = 2024\n",
    "final_month_og = 3\n",
    "final_day_og = 31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the months and years that will be analysed\n",
    "desired_start_monthV = 6 # 6 voor JJA en 3 voor MAM\n",
    "desired_end_monthV = 8 # 8 voor JJA en 5 voor MAM\n",
    "desired_start_yearV = 1950 # 1950 for all data\n",
    "desired_end_yearV = 2023 # 2014 for model comparison, 2023 is the last complete year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the area to be analysed and the area over which the average trend is calculated\n",
    "bboxT = [30,70,-30,30] #Pick borders like: [S,N,W,E] (T from temperature) for serious_run1 = [30,70,-30,30]\n",
    "bboxA = [45,55,-5,15] #Pick borders like: [S,N,W,E] (A from average) for Vautard = [45,55,-5,15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the adjusted yearlenght\n",
    "adjusted_yearlengthV = 92 # 92 voor JJA en MAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where and how to save (zelf mapje al maken! (in prompt mkdir ...))\n",
    "save_outputV = True\n",
    "plot_outputV = True\n",
    "save_pathV = '/usr/people/noest/stage_folders/outputs/net/serious_run2/total_trend/era5'\n",
    "season_nameV = \"JJA_new\" #JJA or MAM\n",
    "save_statisticsV = True\n",
    "save_path_statisticsV = '/usr/people/noest/stage_folders/outputs/net/serious_run2/total_trend/era5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Uitvoeren**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_to_use == \"regridded\":\n",
    "    data_path_tmax = \"/net/pc200246/nobackup/users/noest/ERA5_regridded/era5_tmax_daily_regridded.nc\"\n",
    "    data_path_t2m = \"/net/pc200246/nobackup/users/noest/ERA5_regridded/era5_t2m_daily_regridded.nc\"\n",
    "    land_mask_path = \"/net/pc200246/nobackup/users/noest/landmask/landmask_day_regridded.nc\"\n",
    "    final_yearV = 2024\n",
    "    final_monthV = 2\n",
    "    final_dayV = 29\n",
    "elif data_to_use == \"original\":\n",
    "    data_path_tmax = \"/net/pc230042/nobackup/users/sager/nobackup_2_old/ERA5-CX-READY/era5_tmax_daily_eu.nc\"\n",
    "    #data_path_t2m = \"/net/pc230042/nobackup/users/sager/nobackup_2_old/ERA5-CX-READY/era5_t2m_daily.nc\"\n",
    "    data_path_t2m = \"/net/pc200246/nobackup/users/noest/ERA5_regridded/era5_t2m_daily_regridded.nc\"\n",
    "    land_mask_path = \"/net/pc200246/nobackup/users/noest/landmask/landmask_day_tmax_highres.nc\"\n",
    "    final_yearV = final_year_og\n",
    "    final_monthV = final_month_og\n",
    "    final_dayV = final_day_og"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determine the boundaries to use for the temperature trend\n",
    "if data_to_use == \"regridded\":\n",
    "    S1 = bboxT[0] # for serious_run1 = 30 en 70 als data_to_use = \"original\"\n",
    "    N1 = bboxT[1] # for serious_run1 = 70 en 30 als data_to_use = \"original\"\n",
    "    W1 = bboxT[2] # for serious_run1 = -30\n",
    "    E1 = bboxT[3] # for serious_run1 = 30\n",
    "elif data_to_use == \"original\":\n",
    "    S1 = bboxT[1] # for serious_run1 = 30 en 70 als data_to_use = \"original\"\n",
    "    N1 = bboxT[0] # for serious_run1 = 70 en 30 als data_to_use = \"original\"\n",
    "    W1 = bboxT[2] # for serious_run1 = -30\n",
    "    E1 = bboxT[3] # for serious_run1 = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determine the boundaries to use for the average trend\n",
    "if data_to_use == \"regridded\":\n",
    "    S_A = bboxA[0] # for serious_run1 = 30 en 70 als data_to_use = \"original\"\n",
    "    N_A = bboxA[1] # for serious_run1 = 70 en 30 als data_to_use = \"original\"\n",
    "    W_A = bboxA[2] # for serious_run1 = -30\n",
    "    E_A = bboxA[3] # for serious_run1 = 30\n",
    "elif data_to_use == \"original\":\n",
    "    S_A = bboxA[1] # for serious_run1 = 30 en 70 als data_to_use = \"original\"\n",
    "    N_A = bboxA[0] # for serious_run1 = 70 en 30 als data_to_use = \"original\"\n",
    "    W_A = bboxA[2] # for serious_run1 = -30\n",
    "    E_A = bboxA[3] # for serious_run1 = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#t2m_lat,t2m_lon,t2m_global = load_data(data_path_t2m,\"lat\",\"lon\",\"t2m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmax_lat,tmax_lon,tmax = load_data(data_path_tmax,\"lat\",\"lon\",\"tmax\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_with_months, list_with_years = lists_for_era5_dates(final_yearV,final_monthV,final_dayV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmax_JJA = extract_years_and_months_era5(tmax,desired_start_monthV,desired_end_monthV,desired_start_yearV,desired_end_yearV,list_with_months,list_with_years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_slice,lon_slice,tmax_extracted = extract_area(S1, N1, W1, E1, tmax_lat,tmax_lon, tmax_JJA,event = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Txx_era5, Txm_era5 = temperature_trends__TXx_TXm(tmax_extracted,adjusted_yearlengthV, desired_start_yearV, desired_end_yearV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gwd_era5 = global_warming_degree_function_era5_area_weighted(data_path_t2m,list_with_years,desired_start_yearV,desired_end_yearV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slope_array_era5_Txx,intercept_array_era5_Txx,rvalue_array_era5_Txx,pvalue_array_era5_Txx,stderr_array_era5_Txx = regression_temperature_trend(gwd_era5,Txx_era5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slope_array_era5_Txm,intercept_array_era5_Txm,rvalue_array_era5_Txm,pvalue_array_era5_Txm,stderr_array_era5_Txm = regression_temperature_trend(gwd_era5,Txm_era5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_outputV == True:\n",
    "    np.save(f\"{save_pathV}/{season_nameV}_era5_TXx_slope_{data_to_use}_data_until_{desired_end_yearV}.npy\",slope_array_era5_Txx) \n",
    "    np.save(f\"{save_pathV}/{season_nameV}_era5_TXm_slope_{data_to_use}_data_until_{desired_end_yearV}.npy\",slope_array_era5_Txm)\n",
    "    lat_to_save = np.array(lat_slice)\n",
    "    lon_to_save = np.array(lon_slice)\n",
    "    np.save(f\"{save_pathV}/{season_nameV}_era5_lat_extracted_{data_to_use}_data_until_{desired_end_yearV}.npy\",lat_to_save) \n",
    "    np.save(f\"{save_pathV}/{season_nameV}_era5_lon_extracted_{data_to_use}_data_until_{desired_end_yearV}.npy\",lon_to_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_statisticsV == True:\n",
    "    np.save(f\"{save_path_statisticsV}/{season_nameV}_era5_TXx_stderr_{data_to_use}_data_until_{desired_end_yearV}.npy\",stderr_array_era5_Txx) \n",
    "    np.save(f\"{save_path_statisticsV}/{season_nameV}_era5_TXx_pvalue_{data_to_use}_data_until_{desired_end_yearV}.npy\",pvalue_array_era5_Txx) \n",
    "    np.save(f\"{save_path_statisticsV}/{season_nameV}_era5_TXm_stderr_{data_to_use}_data_until_{desired_end_yearV}.npy\",stderr_array_era5_Txm) \n",
    "    np.save(f\"{save_path_statisticsV}/{season_nameV}_era5_TXm_pvalue_{data_to_use}_data_until_{desired_end_yearV}.npy\",pvalue_array_era5_Txm) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if plot_outputV == True:\n",
    "    plot_variablet(lat_slice,lon_slice,slope_array_era5_Txx,\"tset\",\"test\")\n",
    "    plot_variablet(lat_slice,lon_slice,slope_array_era5_Txm,\"tset\",\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_Txx = area_averaged_trend(slope_array_era5_Txx,lat_slice,lon_slice,land_mask_path,S_A,N_A,W_A,E_A)\n",
    "mean_Txm = area_averaged_trend(slope_array_era5_Txm,lat_slice,lon_slice,land_mask_path,S_A,N_A,W_A,E_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_outputV == True:\n",
    "    filenametxx = f\"{save_pathV}/{season_nameV}_era5_meanTxxlist_{data_to_use}_data_until_{desired_end_yearV}.pkl\"\n",
    "    with open(filenametxx, 'wb') as f:\n",
    "        pickle.dump(mean_Txx, f)\n",
    "    filenametxm = f\"{save_pathV}/{season_nameV}_era5_meanTxmlist_{data_to_use}_data_until_{desired_end_yearV}.pkl\"\n",
    "    with open(filenametxm, 'wb') as f:\n",
    "        pickle.dump(mean_Txm, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_lists_from_file(measurement_name, stack_name):\n",
    "#     filename = f\"/content/drive/MyDrive/Colab Notebooks/Research Project/plume_output/{measurement_name}_{stack_name}.pkl\"\n",
    "#     with open(filename, 'rb') as f:\n",
    "#         loaded_lists = pickle.load(f)\n",
    "#     return loaded_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.save(f\"/usr/people/noest/stage_folders/outputs/dynamic_test/TOTAL_Txx_weighted2022.npy\",slope_array_era5_Txx) \n",
    "#np.save(f\"/usr/people/noest/stage_folders/outputs/dynamic_test/TOTAL_Txm_weighted2022.npy\",slope_array_era5_Txm) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "knmi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
