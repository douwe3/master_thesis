{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Notes** \n",
    "### Purpose: Calculate the dynamical trend in TXx and TXm for ERA5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Preparation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # for storing vector and matrix data\n",
    "import matplotlib.pyplot as plt # to plot figures\n",
    "import netCDF4 as nc #to read netCDF files\n",
    "import cartopy.crs as ccrs # to plot maps\n",
    "# (ergens in test ook: import cartopy as cart)\n",
    "import cartopy.feature as cf\n",
    "# from matplotlib import ticker\n",
    "import scipy.io\n",
    "from scipy.stats import pearsonr # voor persistence\n",
    "import scipy.stats as stats\n",
    "# from cartopy.util import add_cyclic_point\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import xarray as xr\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path,*variables_to_add):\n",
    "  \"\"\"\n",
    "Provide the path to a file and the variables you want to extract\n",
    "  \"\"\"\n",
    "  data = nc.Dataset(path, mode='r')\n",
    "  variable_list = []\n",
    "  for variable in variables_to_add:\n",
    "    var =data.variables[variable][:]\n",
    "    variable_list.append(var)\n",
    "  return variable_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_variablet(lat, lon, variable,folder,name):\n",
    "    plt.figure(figsize = (10,10))\n",
    "    ax = plt.axes(projection = ccrs.PlateCarree())\n",
    "    plot = plt.contourf(lon, lat, variable, cmap = \"RdBu_r\", transform = ccrs.PlateCarree(), levels = 15) #levels=np.linspace(-8.2e7, 1e7, 10), extend='both\n",
    "    ax.coastlines()\n",
    "    ax.add_feature(cf.BORDERS)\n",
    "    plt.colorbar(plot, ax=ax, orientation = \"horizontal\", label = \"Pressure (Pa)\", pad = 0.05)\n",
    "    #plt.savefig(f\"{folder}/{name}.png\",dpi=300)\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_area(S, N, W, E, lat, lon, variable,event = False):\n",
    "    \"\"\"\n",
    "    This function slices the data given the S, N, W, E bounds. Use event = True if there are only two dimensions (since then there is no time dimension), this means after using this\n",
    "    function you need to use event[0] to get the data\n",
    "    \"\"\"\n",
    "    # Change longitude data to go from -180 to 180\n",
    "    for i in range(len(lon)):\n",
    "        if lon[i] > 180:\n",
    "          lon[i] = lon[i] - 360\n",
    "        else:\n",
    "          lon[i] = lon[i]\n",
    "\n",
    "    # Calculate the index of the bounds\n",
    "    sIndex = np.argmin(np.abs(lat - S))\n",
    "    nIndex = np.argmin(np.abs(lat - N))\n",
    "    wIndex = np.argmin(np.abs(lon - W))\n",
    "    eIndex = np.argmin(np.abs(lon - E))\n",
    "\n",
    "    if event:\n",
    "        variable = np.expand_dims(variable, axis = 0)\n",
    "\n",
    "    if wIndex > eIndex: # If the west index is higher than the east index, think of the right side of the world map as left boundary and vice versa\n",
    "        latSlice = lat[sIndex: nIndex + 1]\n",
    "        lonSlice = np.concatenate((lon[wIndex:], lon[:eIndex + 1]))\n",
    "        variableSlice = np.concatenate((variable[:, sIndex: nIndex + 1, wIndex:], variable[:, sIndex: nIndex + 1, :eIndex + 1]), axis = 2)\n",
    "\n",
    "    else:\n",
    "        latSlice = lat[sIndex: nIndex + 1]\n",
    "        lonSlice = lon[wIndex: eIndex + 1]\n",
    "        variableSlice = variable[:, sIndex: nIndex + 1, wIndex: eIndex + 1]\n",
    "\n",
    "    return latSlice, lonSlice, variableSlice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temperature_trends__TXx_TXm(max_temp_data,adjusted_yearlength, start_year, end_year):\n",
    "    \"\"\"\n",
    "    Needs to use the maximum daily temperature data (TASMAX),note that instead of looping through the years, you could also just do np.mean(...,axis = 1)\n",
    "    because the o.g. reshaped data is still (years,months,lat,lon) (or lon/lat), so than you would get (years,lat,lon) \n",
    "    \"\"\"\n",
    "    amount_of_years = (end_year - start_year) + 1\n",
    "    reshaped_data = max_temp_data.reshape(amount_of_years,adjusted_yearlength,max_temp_data.shape[1],max_temp_data.shape[2])\n",
    "    list_for_Txx_values = []\n",
    "    list_for_Txm_values = []\n",
    "\n",
    "    for i in range(amount_of_years):\n",
    "        filtered_data = reshaped_data[i,:,:,:]\n",
    "        max_per_season = np.max(filtered_data,axis = 0)\n",
    "        list_for_Txx_values.append(max_per_season)\n",
    "        mean_per_season = np.mean(filtered_data, axis = 0)\n",
    "        list_for_Txm_values.append(mean_per_season)\n",
    "\n",
    "    array_for_TXx = np.array(list_for_Txx_values)\n",
    "    array_for_TXm = np.array(list_for_Txm_values)\n",
    "    \n",
    "    return array_for_TXx, array_for_TXm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_temperature_trend(GWD_list,temperature_trend_array):\n",
    "\n",
    "    # Create empty arrays for the regression outputs\n",
    "    shape_1 = temperature_trend_array.shape[1]\n",
    "    shape_2 = temperature_trend_array.shape[2]\n",
    "\n",
    "    slope_array = np.zeros((shape_1,shape_2))\n",
    "    intercept_array = np.zeros((shape_1,shape_2))\n",
    "    rvalue_array = np.zeros((shape_1,shape_2))\n",
    "    pvalue_array = np.zeros((shape_1,shape_2))\n",
    "    stderr_array = np.zeros((shape_1,shape_2))\n",
    "\n",
    "    for i in range(shape_1):\n",
    "        for j in range(shape_2):\n",
    "            values_at_specific_coordinates = temperature_trend_array[:, i, j] # Find the values for all years in 1 grid cell\n",
    "            slope, intercept, rvalue, pvalue, stderr = stats.linregress(GWD_list, values_at_specific_coordinates)\n",
    "            \n",
    "            # Store the regression outputs in the empty arrays\n",
    "            slope_array[i, j] = slope\n",
    "            intercept_array[i, j] = intercept\n",
    "            rvalue_array[i, j] = rvalue\n",
    "            pvalue_array[i, j] = pvalue\n",
    "            stderr_array[i, j] = stderr\n",
    "\n",
    "    return slope_array,intercept_array,rvalue_array,pvalue_array,stderr_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lists_for_era5_dates(final_year,final_month,final_day):\n",
    "    \"\"\"\n",
    "    Creates lists with all months and years in the complete ERA5 data, taking into account leap days (schrikkeldagen)\n",
    "    \"\"\"\n",
    "    start_date_all_era5_data = datetime(1950, 1, 1) # Is included\n",
    "    end_date_all_era5_data = datetime(final_year,final_month,final_day) # Is included\n",
    "    delta_time = timedelta(days=1)\n",
    "\n",
    "    date_list_basic = []\n",
    "    current_date = start_date_all_era5_data\n",
    "    while current_date <= end_date_all_era5_data:\n",
    "        date_list_basic.append(current_date)\n",
    "        current_date += delta_time\n",
    "    #date_strings = [date.strftime('%Y-%m-%d') for date in date_list_basic]\n",
    "    month_list = [date.month for date in date_list_basic]\n",
    "    year_list = [date.year for date in date_list_basic]\n",
    "\n",
    "    return month_list, year_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_years_and_months_era5(era5_data,desired_start_month,desired_end_month,desired_start_year,desired_end_year,list_with_all_months,list_with_all_years):\n",
    "    \"\"\"\n",
    "    Slice era5 data based on the months and years, months and years that are used as input variable will be included as well\n",
    "    \"\"\"\n",
    "    if era5_data.shape[0] != len(list_with_all_months):\n",
    "        print (\"Error: Amount of days in the data and list with all dates are not the same\")\n",
    "    \n",
    "    list_for_filtered_era5_data = []\n",
    "    list_for_filtered_years = []\n",
    "    list_for_filtered_months = []\n",
    "    for i in range(era5_data.shape[0]):\n",
    "        month_at_index = list_with_all_months[i]\n",
    "        year_at_index = list_with_all_years[i]\n",
    "        if month_at_index >= desired_start_month and month_at_index <= desired_end_month and year_at_index >= desired_start_year and year_at_index <= desired_end_year:\n",
    "            data_to_select = era5_data[i,:,:]\n",
    "            list_for_filtered_era5_data.append(data_to_select)\n",
    "            list_for_filtered_years.append(year_at_index)\n",
    "            list_for_filtered_months.append(month_at_index)\n",
    "    array_selected_era5_data = np.array(list_for_filtered_era5_data)\n",
    "\n",
    "    return array_selected_era5_data, list_for_filtered_years, list_for_filtered_months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_weighted(data):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : data\n",
    "         data.\n",
    "    \"\"\"\n",
    "    ## Calculate global-mean surface temperature (GMST)\n",
    "    cos_lat_2d = np.cos(np.deg2rad(data['lat'])) * xr.ones_like(data['lon']) # effective area weights\n",
    "    mean_ = ((data * cos_lat_2d).sum(dim=['lat','lon']) /\n",
    "                 cos_lat_2d.sum(dim=['lat','lon']))\n",
    "    return mean_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def global_warming_degree_function_era5_area_weighted(temp_data_path,list_of_years, start_year, end_year, variable = \"t2m\"):\n",
    "    \"\"\"\n",
    "    Hardcodes that its a 5 year centred running average, moet temp_data van het hele jaar\n",
    "    laatste if niet nodig want zegt dat maar 65 jaar doet (tot 2014) maar era5 data en year list lopen tot 2024 dus de year list loopt gwn naar 2015\n",
    "    doet weighted mean van de gegeven variable, hardcodes dat lat en lon heet (dus als variables in data lattitude en longitude hebt = aanpasses)\n",
    "    door for i in range(amount_of_years) hardcodes ook dat in 1950 begint!\n",
    "    \"\"\"\n",
    "    #Open the NetCDF file with xarray\n",
    "    dataset = xr.open_dataset(temp_data_path)\n",
    "\n",
    "    #Calculate the area weighted mean\n",
    "    weighted_mean = mean_weighted(dataset[variable])\n",
    "\n",
    "    #Transform the xarray DataArray into a list\n",
    "    daily_mean_list = weighted_mean.values.tolist()\n",
    "\n",
    "\n",
    "    amount_of_years = (end_year - start_year) + 1\n",
    "    list_for_finalized_averages = []\n",
    "    list_for_GWD = []\n",
    "\n",
    "    list_for_reshaped_data = []\n",
    "    day = 0\n",
    "    for i in range(amount_of_years):\n",
    "        list_for_data_per_year = []\n",
    "        condition_variable = True\n",
    "        while condition_variable == True:\n",
    "            data_to_add = daily_mean_list[day]\n",
    "            list_for_data_per_year.append(data_to_add)\n",
    "            day = day + 1\n",
    "            if list_of_years[day] != list_of_years[day-1]: #deze vergelijken met vorige omdat al day+1 hebt gedaan\n",
    "                list_for_reshaped_data.append(list_for_data_per_year)\n",
    "                condition_variable = False\n",
    "            #if day == (temp_data.shape[0]): #niet -1 omdat hierboven al day + 1 hebt gedaan\n",
    "                #condition_variable = False\n",
    "\n",
    "    for i in range(amount_of_years):\n",
    "        if i == 0:\n",
    "            indexes = [0,1,2]\n",
    "        elif i == 1:\n",
    "            indexes = [0,1,2,3]\n",
    "        elif i == (amount_of_years-1): #laatste\n",
    "            indexes = [i-2,i-1,i]\n",
    "        elif i == (amount_of_years-2): #(ook i nog minder hoog dus drm zelfde als vorige)\n",
    "            indexes = [i-2,i-1,i,i+1]\n",
    "        else:\n",
    "            indexes = [i-2,i-1,i,i+1,i+2]\n",
    "        \n",
    "        list_for_means_per_year = []\n",
    "        for index in indexes:\n",
    "            list_to_analyse = list_for_reshaped_data[index]\n",
    "            array_to_analyse = np.array(list_to_analyse)\n",
    "            mean_for_year = np.mean(array_to_analyse)\n",
    "            list_for_means_per_year.append(mean_for_year)\n",
    "        combined_mean = (sum(list_for_means_per_year))/(len(list_for_means_per_year))\n",
    "        list_for_finalized_averages.append(combined_mean)\n",
    "\n",
    "    for average in list_for_finalized_averages:\n",
    "        GWD_value = average - list_for_finalized_averages[-1]\n",
    "        list_for_GWD.append(GWD_value)\n",
    "    \n",
    "    return list_for_GWD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidian_distance(data, event):\n",
    "  \"\"\"\n",
    "Calculates the euclidian distance for each day in the data compared to a given single event, gives an array (~list) of all the distances in chronological order\n",
    "  \"\"\"\n",
    "  return np.sqrt(np.sum((data - event)**2, axis = (1, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_analogues_era5(euclidian_distances, target_number_of_analogues,analogue_seperation_range,list_months,list_years,desired_start_month,desired_end_month,event_is_in_data = True):\n",
    "  \"\"\"\n",
    "  Determine the best analogues. Give an array of all unsorted euclidan distances. The analogue_seperation_range determines how many days have to seperate\n",
    "  the different analogues (if analogue_seperation_range = 5, the fifth day after an analogue can also still not be a new analogue(dus die dag mag ook niet)).\n",
    "  Target_number_of_analogues is how many analogues are selected. Returns the indexes of the best analogues in the original data, and the euclidian distances corresponding to the analogues.\n",
    "  Give semi-filtered data to euclidian_distances: filtered on years but all months, so the seperation range doesn't continue to count in the next season\n",
    "  Should give semi-filtered lists of months and years as well??\n",
    "  \"\"\"\n",
    "  distance_index_dictionary = {value: index for index, value in enumerate(euclidian_distances)} # Gives the index in the original euclidian distances list for a value in the sorted list\n",
    "  sorted_distances = np.sort(euclidian_distances) # sort the distances from low to high euclidian distances\n",
    "  analogues_index_list = [] # create a list to save the indexes of the selected analogues\n",
    "  euclidian_distance_list = [] # create a list to save the euclidian distances of the selected analogues\n",
    "  selected_analogue_years = []\n",
    "  if event_is_in_data == False:\n",
    "    i = 0\n",
    "    if sorted_distances[0] == 0:\n",
    "      print (\"WARNING: event does seem to be in data, while event_is_in_data == False (zelf)\")\n",
    "    while len(analogues_index_list) < target_number_of_analogues and (i < len(euclidian_distances)):\n",
    "      differences= []\n",
    "      index = distance_index_dictionary[sorted_distances[i]]\n",
    "      month = list_months[index]\n",
    "      year = list_years[index]\n",
    "      if len(analogues_index_list) == 0:\n",
    "        if month >= desired_start_month and month <= desired_end_month:\n",
    "          analogues_index_list.append(index)\n",
    "          euclidian_distance_list.append(sorted_distances[i])\n",
    "          selected_analogue_years.append(year)\n",
    "        i = i + 1\n",
    "      else:\n",
    "        if month >= desired_start_month and month <= desired_end_month:\n",
    "          for item in analogues_index_list:\n",
    "            difference = (index-item)\n",
    "            if difference < (-1*analogue_seperation_range) or difference > analogue_seperation_range:\n",
    "              differences.append(2) #goed\n",
    "            elif difference >= (-1*analogue_seperation_range) and difference <= (analogue_seperation_range):\n",
    "              differences.append(1) #niet goed\n",
    "          if min(differences) == 2:\n",
    "            analogues_index_list.append(index)\n",
    "            euclidian_distance_list.append(sorted_distances[i])\n",
    "            selected_analogue_years.append(year)\n",
    "            i = i + 1\n",
    "          elif min(differences) == 1:\n",
    "            i = i + 1\n",
    "        else:\n",
    "          i = i + 1\n",
    "\n",
    "  elif event_is_in_data == True: #Need to make sure the selected analogues are also 5 days away from the event, eventhough it is not a selecgted analogue \n",
    "    event_index = distance_index_dictionary[sorted_distances[0]]\n",
    "    analogues_index_list.append(event_index)\n",
    "    i=1 #index counter\n",
    "    if sorted_distances[0] != 0:\n",
    "      print (\"WARNING: event does not seem to be in data, while event_is_in_data == True (zelf)\")\n",
    "    while len(analogues_index_list) < (target_number_of_analogues+1) and (i < len(euclidian_distances)): #+1 omdat event er nu ook in en die later weghalen\n",
    "      differences= []\n",
    "      index = distance_index_dictionary[sorted_distances[i]]\n",
    "      month = list_months[index]\n",
    "      year = list_years[index]\n",
    "      if len(analogues_index_list) == 0:\n",
    "        if month >= desired_start_month and month <= desired_end_month:\n",
    "          analogues_index_list.append(index)\n",
    "          euclidian_distance_list.append(sorted_distances[i])\n",
    "          selected_analogue_years.append(year)\n",
    "        i = i + 1\n",
    "      else:\n",
    "        if month >= desired_start_month and month <= desired_end_month:\n",
    "          for item in analogues_index_list:\n",
    "            difference = (index-item)\n",
    "            if difference < (-1*analogue_seperation_range) or difference > analogue_seperation_range:\n",
    "              differences.append(2) #goed\n",
    "            elif difference >= (-1*analogue_seperation_range) and difference <= (analogue_seperation_range):\n",
    "              differences.append(1) #niet goed\n",
    "          if min(differences) == 2:\n",
    "            analogues_index_list.append(index)\n",
    "            euclidian_distance_list.append(sorted_distances[i])\n",
    "            selected_analogue_years.append(year)\n",
    "            i = i + 1\n",
    "          elif min(differences) == 1:\n",
    "            i = i + 1\n",
    "        else:\n",
    "          i = i + 1\n",
    "    analogues_index_list.pop(0)\n",
    "\n",
    "  if len(analogues_index_list) < target_number_of_analogues:\n",
    "    print (\"WARNING: not enough data to find the required amount of analogues (zelf)\")\n",
    "  if len(analogues_index_list) != target_number_of_analogues:\n",
    "    print (\"WARNING: not the right amount of analogues has been found\")\n",
    "  return analogues_index_list,euclidian_distance_list,selected_analogue_years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def thermodynamical_correction_and_shuffling_function(semi_filtered_p_data,semi_filtered_t_data,season_p_data,season_t_data,analogues_seperation_range,semi_filtered_month_list,semi_filtered_year_list,season_start_month,season_end_month,start_year,gwd_list,season_filtered_year_list):\n",
    "    \"\"\"\n",
    "    test\n",
    "    \"\"\"\n",
    "    if semi_filtered_p_data.shape[0] != semi_filtered_t_data.shape[0]:\n",
    "        print (\"WARNING: the semi_filtered p and T data do not have the same amount of days\")\n",
    "\n",
    "    if season_p_data.shape[0] != season_t_data.shape[0]:\n",
    "        print (\"WARNING: the season_filtered p and T data do not have the same amount of days\")\n",
    "        \n",
    "    one_percent_of_season_days = (season_p_data.shape[0])*0.01\n",
    "    analogue_target_amount = round(one_percent_of_season_days)\n",
    "    print (f\"The amount of days was: {season_p_data.shape[0]}, 1% is: {one_percent_of_season_days}, which was rounded to: {analogue_target_amount}\")\n",
    "    list_for_Ts_arrays = []\n",
    "    list_for_lists_with_indexes_for_shuffling = []\n",
    "    for i in range(season_p_data.shape[0]): #for every day in the season\n",
    "        #Find the top 1% analogues\n",
    "        event_day = season_p_data[i,:,:]\n",
    "        euclidian_distances_array = euclidian_distance(semi_filtered_p_data,event_day)\n",
    "        analogues_indexes,euclidian_distances,selected_analogue_years_list = determine_analogues_era5(euclidian_distances_array,analogue_target_amount,analogues_seperation_range,semi_filtered_month_list,semi_filtered_year_list,season_start_month,season_end_month,event_is_in_data = True) # Event is elke keer zomer dag dus zit in data\n",
    "        list_for_temperature_fields_analogues = []\n",
    "        if len(analogues_indexes) != analogue_target_amount:\n",
    "            print (\"WARNING: not enough analogues where find for calculating the b(X) (zelf)\")\n",
    "\n",
    "        #Find the matching temperature fields\n",
    "        for index in analogues_indexes:\n",
    "            temp_field = semi_filtered_t_data[index,:,:]\n",
    "            list_for_temperature_fields_analogues.append(temp_field)\n",
    "        temp_fields_analogues = np.array(list_for_temperature_fields_analogues) #For a single day, the temperaturefields of the top 1% analogues (shape: anlogue/lat/lon (of lon/lat))\n",
    "\n",
    "        #Find the matching GWD values (the GWD values of the years in which the analogues are found)\n",
    "        list_for_selected_gwd_values = []\n",
    "        for year in selected_analogue_years_list:\n",
    "            index_in_gwd = year - start_year\n",
    "            gwd_value = gwd_list[index_in_gwd]\n",
    "            list_for_selected_gwd_values.append(gwd_value)\n",
    "\n",
    "        #Perform the regression\n",
    "        shape1 = temp_fields_analogues.shape[1]\n",
    "        shape2 = temp_fields_analogues.shape[2]\n",
    "        slope_array = np.zeros((shape1,shape2))\n",
    "        for a in range(shape1):\n",
    "            for b in range(shape2):\n",
    "                values_at_coordinates = temp_fields_analogues[:,a,b] # Find the values of all analogues in 1 specific gridcell\n",
    "                slope, intercept, rvalue, pvalue, stderr = stats.linregress(list_for_selected_gwd_values, values_at_coordinates)\n",
    "                slope_array[a, b] = slope\n",
    "        \n",
    "        #Multiply the created b(X) (the slope_array) with the GWD value of the summer day that is set as the event\n",
    "        year_of_event_day = season_filtered_year_list[i] #checken of niet al iets met i gedaan hebt (tussen nu en begin for loop)\n",
    "        index_in_gwd2 = year_of_event_day - start_year\n",
    "        gwd_value_event = gwd_list[index_in_gwd2]\n",
    "        thermo_dynamical_correction_array = slope_array*gwd_value_event\n",
    "\n",
    "        #Subtract the correction from the original to create Ts (event_day is nog pressure dus kan niet die gebruiken!)\n",
    "        event_day_T = season_t_data[i,:,:]\n",
    "        Ts_array = event_day_T - thermo_dynamical_correction_array\n",
    "        list_for_Ts_arrays.append(Ts_array)\n",
    "\n",
    "        #Find/save the indexes for the best 3 analogues for the shuffling\n",
    "        list_for_shuffling_indexes = []\n",
    "        for number in range(3):\n",
    "            index_shuffle = analogues_indexes[number]\n",
    "            list_for_shuffling_indexes.append(index_shuffle)\n",
    "        list_for_lists_with_indexes_for_shuffling.append(list_for_shuffling_indexes)\n",
    "        \n",
    "    Ts_array_finalized = np.array(list_for_Ts_arrays)\n",
    "\n",
    "    list_for_lists_corrected_indexes = [] # the indexes in list_for_lists_with_indexes_for_shuffling are for semi-filtered but you need season indexes\n",
    "    for lijst in list_for_lists_with_indexes_for_shuffling:\n",
    "        list_for_corrected_indexes = []\n",
    "        for item in lijst:\n",
    "            counter = 0\n",
    "            match_condition = False\n",
    "            while match_condition == False and counter < season_t_data.shape[0]:\n",
    "                indexed_field = semi_filtered_t_data[item]\n",
    "                season_field = season_t_data[counter]\n",
    "                if np.array_equal(indexed_field, season_field) == True:\n",
    "                    list_for_corrected_indexes.append(counter)\n",
    "                    match_condition = True\n",
    "                else:\n",
    "                    counter = counter + 1\n",
    "                    if counter == season_t_data.shape[0]:\n",
    "                        print (\"WARNING: the field in the season data corresponding to the analogue field in semi-filtered data was not found? (ZELF)\")\n",
    "        list_for_lists_corrected_indexes.append(list_for_corrected_indexes)\n",
    "\n",
    "    #Shuffle the Ts timeseries into three new time series\n",
    "    time_series_list1 = []\n",
    "    time_series_list2 = []\n",
    "    time_series_list3 = []\n",
    "    for day in range(Ts_array_finalized.shape[0]):\n",
    "        list_for_day = list_for_lists_corrected_indexes[day]\n",
    "        index_day_1 = list_for_day[0]\n",
    "        data_day_1 = Ts_array_finalized[index_day_1]\n",
    "        time_series_list1.append(data_day_1)\n",
    "        index_day_2 = list_for_day[1]\n",
    "        data_day_2 = Ts_array_finalized[index_day_2]\n",
    "        time_series_list2.append(data_day_2)\n",
    "        index_day_3 = list_for_day[2]\n",
    "        data_day_3 = Ts_array_finalized[index_day_3]\n",
    "        time_series_list3.append(data_day_3)\n",
    "    time_series1 = np.array(time_series_list1)\n",
    "    time_series2 = np.array(time_series_list2)\n",
    "    time_series3 = np.array(time_series_list3)\n",
    "\n",
    "    return time_series1,time_series2,time_series3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_weighted_masked(data,mask):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : data\n",
    "         data.\n",
    "    \"\"\"\n",
    "    ## Calculate global-mean surface temperature (GMST)\n",
    "    mask_af = np.squeeze(mask)\n",
    "    cos_lat_2d = np.cos(np.deg2rad(data['lat'])) * xr.ones_like(data['lon']) # effective area weights\n",
    "    if cos_lat_2d.shape != mask_af.shape:\n",
    "        print (f\"WARNING: shapes of mask ({mask_af}) and cos_lat2d  ({cos_lat_2d.shape}) do no match (in mean_weigted2 (zelf))\")\n",
    "    cos_lat_2d_masked = cos_lat_2d*mask_af\n",
    "    mean_ = ((data * cos_lat_2d).sum(dim=['lat','lon']) /\n",
    "                 cos_lat_2d_masked.sum(dim=['lat','lon']))\n",
    "    return mean_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def area_averaged_trend(data,lat,lon,landmask_path,S,N,W,E):\n",
    "    \"\"\"\n",
    "    Calculates the area weighted mean land area trend of a region using a land mask\n",
    "    \"\"\"\n",
    "\n",
    "    #Load the mask and extract the selected region for the mask and actual data\n",
    "    latm,lonm,mask_data = load_data(landmask_path,\"lat\",\"lon\",\"tx\")\n",
    "    latm_box,lonm_box,mask_box = extract_area(S,N,W,E,latm,lonm,mask_data,event = False)\n",
    "    latd_box,lond_box,data_box = extract_area(S,N,W,E,lat,lon,data,event = True)\n",
    "\n",
    "    #Prepare mask\n",
    "    min_data = np.min(mask_box)\n",
    "    if min_data == -9999.0:\n",
    "        mask_box = np.where(mask_box == -9999.0, np.nan, mask_box)\n",
    "    mask = mask_box/mask_box\n",
    "    if mask.shape != data_box.shape:\n",
    "        print (\"WARNING: shape of mask and data do not match (zelf)\")\n",
    "    \n",
    "    #Perform analyses and plot to check\n",
    "    masked_data = data_box*mask\n",
    "    plot_variablet(latd_box,lond_box,masked_data[0],\"test\",\"test\")\n",
    "\n",
    "    #Turn into xarray and take area weighted average\n",
    "    time_indices = np.arange(masked_data.shape[0])  # Because you used event = True in extract area both now have three dimensions\n",
    "    data_xr_array = xr.DataArray(masked_data, dims=['time', 'lat', 'lon'], coords={'time': time_indices, 'lat': latd_box, 'lon': lond_box})\n",
    "    mean_trend = mean_weighted_masked(data_xr_array,mask)\n",
    "    mean_list = mean_trend.values.tolist()\n",
    "    print (f\"weighted mean = {mean_list}, normal mean = {np.mean(masked_data)}\")\n",
    "\n",
    "    return mean_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_streamfunction(data):\n",
    "    \"\"\"\n",
    "    doen nadat bijgesneden\n",
    "    \"\"\"\n",
    "    list_for_new_data = []\n",
    "    for i in range(data.shape[0]):\n",
    "        data_day = data[i,:,:]\n",
    "        day_mean = np.mean(data_day)\n",
    "        new_data = data_day - day_mean\n",
    "        list_for_new_data.append(new_data)\n",
    "    new_array = np.array(list_for_new_data)\n",
    "    \n",
    "    return new_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Set-up**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select: \"regridded\" or \"original\", Als original data doet hebt kans dat kleiner gebied moet doen want doet EU data en dat data geupdate is dus dat lijst langer moet zijn met dates\n",
    "data_to_use = \"regridded\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The latest date in the ERA5 dataset, for the regridded data its 2024, 2, 29 (which is automatically selected if data_to_use = \"regridded\")\n",
    "final_year_og = 2024\n",
    "final_month_og = 3\n",
    "final_day_og = 31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the months and years that will be analysed\n",
    "desired_start_monthV = 6 # 6 voor JJA en 3 voor MAM\n",
    "desired_end_monthV = 8 # 8 voor JJA en 5 voor MAM\n",
    "desired_start_yearV = 1950 # 1950 for all data\n",
    "desired_end_yearV = 2023 # 2014 for model comparison, 2023 is the last complete year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the area for which to calculate the temperature trend (for original data zit een limit omdat EU data doet dus kan niet alles kiezen)\n",
    "bboxT = [30,70,-30,30] #Pick borders like: [S,N,W,E] (T from temperature) for serious_run1 = [30,70,-30,30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the area in which to look for analogues!\n",
    "bboxP = [30,60,-30,20] #Pick borders like: [S,N,W,E] (P from pressure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the area over which to average the trend\n",
    "bboxA = [45,55,-5,15] #Pick borders like: [S,N,W,E] (A from average) for Vautard = [45,55,-5,15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the adjusted yearlenght\n",
    "adjusted_yearlengthV = 92 # 92 voor JJA en MAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set how to look for analogues (for the b(X) and shuffling)\n",
    "analogues_seperation_rangeV = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where and how to save (zelf mapje al maken! (in prompt mkdir ...))\n",
    "save_outputV = True\n",
    "plot_outputV = True\n",
    "save_pathV = '/usr/people/noest/stage_folders/outputs/net/serious_run2/dynamical_trend/era5_prepared_psi'\n",
    "season_nameV = \"JJA_new\" #JJA or MAM\n",
    "save_statisticsV = True\n",
    "save_path_statisticsV = '/usr/people/noest/stage_folders/outputs/net/serious_run2/dynamical_trend/era5_prepared_psi'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Uitvoeren**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_to_use == \"regridded\":\n",
    "    data_path_tmax = \"/net/pc200246/nobackup/users/noest/ERA5_regridded/era5_tmax_daily_regridded.nc\"\n",
    "    data_path_t2m = \"/net/pc200246/nobackup/users/noest/ERA5_regridded/era5_t2m_daily_regridded.nc\"\n",
    "    #data_path_msl = \"/net/pc200246/nobackup/users/noest/ERA5_regridded/era5_psi500_daily_regridded.nc\"\n",
    "    data_path_msl = \"/net/pc200246/nobackup/users/noest/ERA5_regridded/era5_psi500_daily_regridded_1950_2023.nc\"\n",
    "    land_mask_path = \"/net/pc200246/nobackup/users/noest/landmask/landmask_day_regridded.nc\"\n",
    "    final_yearV = 2024\n",
    "    final_monthV = 2\n",
    "    final_dayV = 29\n",
    "elif data_to_use == \"original\":\n",
    "    data_path_tmax = \"/net/pc230042/nobackup/users/sager/nobackup_2_old/ERA5-CX-READY/era5_tmax_daily_eu.nc\"\n",
    "    #data_path_t2m = \"/net/pc230042/nobackup/users/sager/nobackup_2_old/ERA5-CX-READY/era5_t2m_daily.nc\"\n",
    "    data_path_t2m = \"/net/pc200246/nobackup/users/noest/ERA5_regridded/era5_t2m_daily_regridded.nc\"\n",
    "    data_path_msl = \"/net/pc200265/nobackup/users/pinto/phi500/era5_phi500.fixed.1degE.nc\"\n",
    "    land_mask_path = \"/net/pc200246/nobackup/users/noest/landmask/landmask_day_tmax_highres.nc\"\n",
    "    final_yearV = final_year_og\n",
    "    final_monthV = final_month_og\n",
    "    final_dayV = final_day_og"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determine the boundaries to use for the temperature trend\n",
    "if data_to_use == \"regridded\":\n",
    "    S1 = bboxT[0] # for serious_run1 = 30 en 70 als data_to_use = \"original\"\n",
    "    N1 = bboxT[1] # for serious_run1 = 70 en 30 als data_to_use = \"original\"\n",
    "    W1 = bboxT[2] # for serious_run1 = -30\n",
    "    E1 = bboxT[3] # for serious_run1 = 30\n",
    "elif data_to_use == \"original\":\n",
    "    S1 = bboxT[1] # for serious_run1 = 30 en 70 als data_to_use = \"original\"\n",
    "    N1 = bboxT[0] # for serious_run1 = 70 en 30 als data_to_use = \"original\"\n",
    "    W1 = bboxT[2] # for serious_run1 = -30\n",
    "    E1 = bboxT[3] # for serious_run1 = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determine the boundaries to use to look for analogues\n",
    "if data_to_use == \"regridded\":\n",
    "    S_P = bboxP[0] # for serious_run1 = 30 en 70 als data_to_use = \"original\"\n",
    "    N_P = bboxP[1] # for serious_run1 = 70 en 30 als data_to_use = \"original\"\n",
    "    W_P = bboxP[2] # for serious_run1 = -30\n",
    "    E_P = bboxP[3] # for serious_run1 = 30\n",
    "elif data_to_use == \"original\":\n",
    "    S_P = bboxP[1] # for serious_run1 = 30 en 70 als data_to_use = \"original\"\n",
    "    N_P = bboxP[0] # for serious_run1 = 70 en 30 als data_to_use = \"original\"\n",
    "    W_P = bboxP[2] # for serious_run1 = -30\n",
    "    E_P = bboxP[3] # for serious_run1 = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determine the boundaries to use for the average trend\n",
    "if data_to_use == \"regridded\":\n",
    "    S_A = bboxA[0] # for serious_run1 = 30 en 70 als data_to_use = \"original\"\n",
    "    N_A = bboxA[1] # for serious_run1 = 70 en 30 als data_to_use = \"original\"\n",
    "    W_A = bboxA[2] # for serious_run1 = -30\n",
    "    E_A = bboxA[3] # for serious_run1 = 30\n",
    "elif data_to_use == \"original\":\n",
    "    S_A = bboxA[1] # for serious_run1 = 30 en 70 als data_to_use = \"original\"\n",
    "    N_A = bboxA[0] # for serious_run1 = 70 en 30 als data_to_use = \"original\"\n",
    "    W_A = bboxA[2] # for serious_run1 = -30\n",
    "    E_A = bboxA[3] # for serious_run1 = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create lists with for every day, the month and the year it is in (taking into account leap days :))\n",
    "list_with_months, list_with_years = lists_for_era5_dates(final_yearV,final_monthV,final_dayV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the GWD using the area weighted average as a running 5 year mean\n",
    "gwd_era5 = global_warming_degree_function_era5_area_weighted(data_path_t2m,list_with_years,desired_start_yearV,desired_end_yearV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load, slice and extract temperature data\n",
    "tmax_lat,tmax_lon,tmax = load_data(data_path_tmax,\"lat\",\"lon\",\"tmax\")\n",
    "lat_bboxT,lon_bboxT,tmax_all_days_bboxT = extract_area(S1, N1, W1, E1, tmax_lat,tmax_lon, tmax,event = False)\n",
    "tmax_semi_filtered_bboxT, list_semi_filterd_years_t,list_semi_filterd_months_t = extract_years_and_months_era5(tmax_all_days_bboxT,1,12,desired_start_yearV,desired_end_yearV,list_with_months,list_with_years)\n",
    "tmax_season_bboxT, list_season_filterd_years_t,list_season_filterd_months_t = extract_years_and_months_era5(tmax_all_days_bboxT,desired_start_monthV,desired_end_monthV,desired_start_yearV,desired_end_yearV,list_with_months,list_with_years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load, slice and extract pressure data\n",
    "msl_lat,msl_lon,msl_levels = load_data(data_path_msl,\"lat\",\"lon\",\"stream\")\n",
    "msl = np.squeeze(msl_levels)\n",
    "lat_bboxP,lon_bboxP,msl_all_days_bboxP_og = extract_area(S_P, N_P, W_P, E_P, msl_lat,msl_lon,msl,event = False)\n",
    "msl_all_days_bboxP = prep_streamfunction(msl_all_days_bboxP_og)\n",
    "msl_semi_filtered_bboxP, list_semi_filterd_years_p,list_semi_filterd_months_p = extract_years_and_months_era5(msl_all_days_bboxP,1,12,desired_start_yearV,desired_end_yearV,list_with_months,list_with_years)\n",
    "msl_season_bboxP, list_season_filterd_years_p,list_season_filterd_months_p = extract_years_and_months_era5(msl_all_days_bboxP,desired_start_monthV,desired_end_monthV,desired_start_yearV,desired_end_yearV,list_with_months,list_with_years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ts1,Ts2,Ts3 = thermodynamical_correction_and_shuffling_function(msl_semi_filtered_bboxP,tmax_semi_filtered_bboxT,msl_season_bboxP,tmax_season_bboxT,analogues_seperation_rangeV,list_semi_filterd_months_p,list_semi_filterd_years_p,desired_start_monthV,desired_end_monthV,desired_start_yearV,gwd_era5,list_season_filterd_years_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tmax extracted was:#lat_slice,lon_slice,tmax_extracted = extract_area(S1, N1, W1, E1, tmax_lat,tmax_lon, tmax_season,event = False)\n",
    "Txx_era5_1, Txm_era5_1 = temperature_trends__TXx_TXm(Ts1,adjusted_yearlengthV, desired_start_yearV, desired_end_yearV)\n",
    "Txx_era5_2, Txm_era5_2 = temperature_trends__TXx_TXm(Ts2,adjusted_yearlengthV, desired_start_yearV, desired_end_yearV)\n",
    "Txx_era5_3, Txm_era5_3 = temperature_trends__TXx_TXm(Ts3,adjusted_yearlengthV, desired_start_yearV, desired_end_yearV)\n",
    "mean_Txx_era5 = np.mean([Txx_era5_1, Txx_era5_2, Txx_era5_3], axis=0)\n",
    "mean_Txm_era5 = np.mean([Txm_era5_1, Txm_era5_2, Txm_era5_3], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slope_array_era5_Txx,intercept_array_era5_Txx,rvalue_array_era5_Txx,pvalue_array_era5_Txx,stderr_array_era5_Txx = regression_temperature_trend(gwd_era5,mean_Txx_era5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slope_array_era5_Txm,intercept_array_era5_Txm,rvalue_array_era5_Txm,pvalue_array_era5_Txm,stderr_array_era5_Txm = regression_temperature_trend(gwd_era5,mean_Txm_era5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_outputV == True:\n",
    "    np.save(f\"{save_pathV}/{season_nameV}_era5_TXx_slope_{data_to_use}_data_until_{desired_end_yearV}.npy\",slope_array_era5_Txx) \n",
    "    np.save(f\"{save_pathV}/{season_nameV}_era5_TXm_slope_{data_to_use}_data_until_{desired_end_yearV}.npy\",slope_array_era5_Txm)\n",
    "    lat_to_save = np.array(lat_bboxT)\n",
    "    lon_to_save = np.array(lon_bboxT)\n",
    "    np.save(f\"{save_pathV}/{season_nameV}_era5_lat_extracted_{data_to_use}_data_until_{desired_end_yearV}.npy\",lat_to_save) \n",
    "    np.save(f\"{save_pathV}/{season_nameV}_era5_lon_extracted_{data_to_use}_data_until_{desired_end_yearV}.npy\",lon_to_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_statisticsV == True:\n",
    "    np.save(f\"{save_path_statisticsV}/{season_nameV}_era5_TXx_stderr_{data_to_use}_data_until_{desired_end_yearV}.npy\",stderr_array_era5_Txx) \n",
    "    np.save(f\"{save_path_statisticsV}/{season_nameV}_era5_TXx_pvalue_{data_to_use}_data_until_{desired_end_yearV}.npy\",pvalue_array_era5_Txx) \n",
    "    np.save(f\"{save_path_statisticsV}/{season_nameV}_era5_TXm_stderr_{data_to_use}_data_until_{desired_end_yearV}.npy\",stderr_array_era5_Txm) \n",
    "    np.save(f\"{save_path_statisticsV}/{season_nameV}_era5_TXm_pvalue_{data_to_use}_data_until_{desired_end_yearV}.npy\",pvalue_array_era5_Txm) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if plot_outputV == True:\n",
    "    plot_variablet(lat_bboxT,lon_bboxT,slope_array_era5_Txx,\"tset\",\"test\")\n",
    "    plot_variablet(lat_bboxT,lon_bboxT,slope_array_era5_Txm,\"tset\",\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_Txx = area_averaged_trend(slope_array_era5_Txx,lat_bboxT,lon_bboxT,land_mask_path,S_A,N_A,W_A,E_A)\n",
    "mean_Txm = area_averaged_trend(slope_array_era5_Txm,lat_bboxT,lon_bboxT,land_mask_path,S_A,N_A,W_A,E_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_outputV == True:\n",
    "    filenametxx = f\"{save_pathV}/{season_nameV}_era5_meanTxxlist_{data_to_use}_data_until_{desired_end_yearV}.pkl\"\n",
    "    with open(filenametxx, 'wb') as f:\n",
    "        pickle.dump(mean_Txx, f)\n",
    "    filenametxm = f\"{save_pathV}/{season_nameV}_era5_meanTxmlist_{data_to_use}_data_until_{desired_end_yearV}.pkl\"\n",
    "    with open(filenametxm, 'wb') as f:\n",
    "        pickle.dump(mean_Txm, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# filename = f\"{save_pathV}/era5_GWD_{data_to_use}_data_until_{desired_end_yearV}.pkl\"\n",
    "# with open(filename, 'wb') as f:\n",
    "#     pickle.dump(gwd_era5, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_lists_from_file(measurement_name, stack_name):\n",
    "#     filename = f\"/content/drive/MyDrive/Colab Notebooks/Research Project/plume_output/{measurement_name}_{stack_name}.pkl\"\n",
    "#     with open(filename, 'rb') as f:\n",
    "#         loaded_lists = pickle.load(f)\n",
    "#     return loaded_lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Proberen**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "knmi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
